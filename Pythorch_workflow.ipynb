{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzVmLUNwVzsqeFMe9dgK1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArseneGiriteka/PyTorch_Deep_Learning/blob/main/Pythorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "D9LT38CoTakt"
      },
      "outputs": [],
      "source": [
        "What_will_be_covered = {1: \"data (prepare and load)\",\n",
        "                        2: \"build model\",\n",
        "                        3: \"fiting the model to data (training)\",\n",
        "                        4: \"making predictions and evaluating model (inference)\",\n",
        "                        5: \"saving and reloading model\",\n",
        "                        6: \"putting it all together\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn #nn contains all of pytorch's building blocks for neural networks\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gmiy6WB5Ujju",
        "outputId": "d9d0e9cc-4114-4687-e19c-e4e567917df1"
      },
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data (prepare and loading)\n",
        "Types of data in machine learning:\n",
        "* Excel speadsheet\n",
        "* Image of any kind\n",
        "* Videos\n",
        "* Audio\n",
        "* DNA\n",
        "* Text\n",
        "\n",
        "Machine learning is a game of two parts:\n",
        "1. Get data into numerical representation\n",
        "2. Build a model to learn patterns in that numerical representation\n",
        "\n",
        "To showcase this, let's create some kwown data using the linear regression formula.\n",
        "\n",
        "we'll use a linear regression formula to make a straight line with kwown parameters"
      ],
      "metadata": {
        "id": "VFiNmru8WHIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create *kwown* parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "Y = weight * X + bias\n",
        "X[:10], Y[:10], len(X), len(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_02g1tyYuyd",
        "outputId": "33dcf141-3432-44da-c674-0c2fbe6f8c4d"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]),\n",
              " 50,\n",
              " 50)"
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and test sets (one of the most important concept in machine learning in general)\n",
        "\n",
        "Let's Create a training and test set with our data"
      ],
      "metadata": {
        "id": "V7b5ZuOmySUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a train/test split\n",
        "\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, Y_train = X[:train_split], Y[:train_split]\n",
        "X_test, Y_test = X[train_split:], Y[train_split:]\n",
        "\n",
        "len(X_train), len(Y_train), len(X_test), len(Y_test)"
      ],
      "metadata": {
        "id": "LWz2e4k_0NP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a8f0f3-7d7a-4ab2-bf75-8e3a6b850f10"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's vizualize data"
      ],
      "metadata": {
        "id": "a4HbvpJCw7Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train, train_label=Y_train,\n",
        "                     test_data=X_test, test_label=Y_test,\n",
        "                     predictions=None):\n",
        "  \"\"\"Plots training data, test data and compare predictions.\"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  #plot training data in blue\n",
        "  plt.scatter(train_data, train_label, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  #plot test data in green\n",
        "  plt.scatter(test_data, test_label, c=\"g\", s=4, label=\"Test data\")\n",
        "\n",
        "  # Are there predictions?\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14})"
      ],
      "metadata": {
        "id": "InkAwlJwxB8U"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "LmYsz-dm0AtT",
        "outputId": "17e5f96f-b88a-4972-e3cd-9e17c8ed06a2"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building a model\n",
        "\n",
        "first model\n",
        "\n",
        "What out model does:\n",
        "* start with random values (weight & bias)\n",
        "* Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
        "\n",
        "How does it do  so?\n",
        "\n",
        "Through two main algorithms:\n",
        "1. Cradient descent\n",
        "2. Backpropagation"
      ],
      "metadata": {
        "id": "N-zb_myF0wpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Create linear regression model class\n",
        "class LinearRegressionModel(nn.Module): # <- Almost everything in Pytorch inherits from nn.Module\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "\n",
        "  # Forward method to define the computation in the model\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data\n",
        "    return self.weights * x + self.bias # linear regression formula"
      ],
      "metadata": {
        "id": "yy2WvoWG07V6"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch model building essentials\n",
        "\n",
        "* torch.nn - contains all of the building for computational graphs (neural networks).\n",
        "* torch.nn.Parameter - What parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us.\n",
        "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite `forward()`!!!\n",
        "* torch.optim - this is where the optimizer in PyTorch live, they will help with gradient descent.\n",
        "* def forward() - All nn.Module subclasses requires you to overwrite `forward()`, this method defines what happens in the forward computation."
      ],
      "metadata": {
        "id": "Dme5ojQ0kA6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the contents of our PyTorch model\n",
        "\n",
        "Now we've created a model, let's see what's inside...\n",
        "\n",
        "So we can check our model parameters or what's inside our model using `.parameter()`."
      ],
      "metadata": {
        "id": "1iYfE5pKpwXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Let's create an instance of the model (this is a subclass of nn.Module)\n",
        "model_0 = LinearRegressionModel()\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0sx8q1OoYPP",
        "outputId": "6a588470-2251-4a63-d9e3-6a1528a45525"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXnM0_Mksggh",
        "outputId": "ea245a26-82c1-405f-e0b5-bb3f811cce46"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions using `torch.inference_mode()`\n",
        "To check our model's predictive power, let's see how well it predicts `y_test` based on `x_test`.\n",
        "\n",
        "When we pass data through our model, it's going to run it through the `forward()` method."
      ],
      "metadata": {
        "id": "CBIrHWD5u6Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAHgNElzw8P0",
        "outputId": "3ab92f87-f804-4a10-e359-6e254a79b12f"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8000],\n",
              "        [0.8200],\n",
              "        [0.8400],\n",
              "        [0.8600],\n",
              "        [0.8800],\n",
              "        [0.9000],\n",
              "        [0.9200],\n",
              "        [0.9400],\n",
              "        [0.9600],\n",
              "        [0.9800]])"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "  Y_preds = model_0(X_test)\n",
        "\n",
        "Y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XvEmETKxEr6",
        "outputId": "ee1a071f-dfe4-44dd-8373-e9e66d471e72"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also do something similar with torch.no_grad(), however, torch.inference_mode() is prefered\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    y_preds = model_0(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "afuRYKra2KGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mpyDpaJyLf6",
        "outputId": "d3f0a39e-bee2-4ffd-9b76-5f13f6304f73"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=Y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "93VXPQrVyRxQ",
        "outputId": "ba8e8c19-c059-4d53-d5b4-a47642690c0d"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training model\n",
        "The whole idea of training is for a model to move from some unkwown parameters (these may be random) to some known parameters.\n",
        "\n",
        "Or in other words from a poor representation of the data to a better representation of the data.\n",
        "\n",
        "* Note: Loss function may also be called cost function or criterion in different areas.\n",
        "\n",
        "\n",
        "Things we need to train:\n",
        "* **Losss function:** A function to measure how wrong your model's predictions are to the ideal outputs, Lower is better.\n",
        "* **Optimizer:** Takes into account the loss of a moddel and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function.\n",
        "\n",
        "  * Inside the optimizer you'll often have to set two parameters:\n",
        "    * `params` - The model parameter you'd like to optimize, for example `params=model_0.parameters()`.\n",
        "    * `lr` (learning rate) - the learning is the hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a large `lr` results in large changes).\n",
        "\n",
        "\n",
        "And specifically for PyTorch, we need:\n",
        "\n",
        "* Training loop\n",
        "* Testing loop"
      ],
      "metadata": {
        "id": "gYZIfOE_3TLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# setup an optimizer (stocastic gradient descent)\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001) # lr = learning rate, possibly the most important hyperparameter you can set\n"
      ],
      "metadata": {
        "id": "fe4_WysD332e"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a training loop (and a testing loop) in PyTorch\n",
        "\n",
        "A couple of things we need in a training loop:\n",
        "0. Loop through the data.\n",
        "1. Foward pass (this involves data moving through our model's `forward()` functions) to make predictions on data - also called forward propagation.\n",
        "2. Calculate the loss (compare forward pass predictions to ground truth labels).\n",
        "3. Optimizer zero grad.\n",
        "4. Loss backward - move backward through the network to calculate the gradiant of each of the parameters of our model with respect to the loss (**Backpropagation**).\n",
        "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss(**Gradient descent**)."
      ],
      "metadata": {
        "id": "TtH9CIk4hPSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)\n",
        "epochs = 2000\n",
        "\n",
        "epochs_counts = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "### Training\n",
        "for epoch in range(epochs):\n",
        "  # Set the training mode\n",
        "  model_0.train() # Train mode in PyTorch sets all parameters that require gradients\n",
        "\n",
        "  # 1. Forward pass\n",
        "  Y_preds = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(Y_preds, Y_train)\n",
        "\n",
        "  #3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #4. Perform backpropagation on the loss with respect to the parameters of the model\n",
        "  loss.backward()\n",
        "\n",
        "  #5. Step the optimizer (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above in step 3 for the netx iteration of the loop\n",
        "\n",
        "\n",
        "\n",
        "  model_0.eval() # turns off gradient tracking\n",
        "  # eval() turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
        "\n",
        "  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind scenes\n",
        "    # 1. Do the forward pass\n",
        "    test_pred = model_0(X_test)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_pred, Y_test)\n",
        "\n",
        "  # Print out what's happenning\n",
        "  if epoch % 10 == 0:\n",
        "    epochs_counts.append(epoch)\n",
        "    loss_values.append(loss)\n",
        "    test_loss_values.append(test_loss)\n",
        "    print(f\"Epoch: {epoch} | loss: {loss} | Test loss: {test_loss}\")\n",
        "\n",
        "    # Print out model state_dict()\n",
        "    print(model_0.state_dict())"
      ],
      "metadata": {
        "id": "0E8P1dgckgLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78a4378-fff1-4296-eba8-ce13f52d366c"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | loss: 0.31288138031959534 | Test loss: 0.4931890368461609\n",
            "OrderedDict([('weights', tensor([0.3371])), ('bias', tensor([0.1298]))])\n",
            "Epoch: 10 | loss: 0.3013603389263153 | Test loss: 0.4797181189060211\n",
            "OrderedDict([('weights', tensor([0.3410])), ('bias', tensor([0.1398]))])\n",
            "Epoch: 20 | loss: 0.28983938694000244 | Test loss: 0.4662471413612366\n",
            "OrderedDict([('weights', tensor([0.3449])), ('bias', tensor([0.1498]))])\n",
            "Epoch: 30 | loss: 0.2783183455467224 | Test loss: 0.4527761936187744\n",
            "OrderedDict([('weights', tensor([0.3488])), ('bias', tensor([0.1598]))])\n",
            "Epoch: 40 | loss: 0.26679736375808716 | Test loss: 0.43930521607398987\n",
            "OrderedDict([('weights', tensor([0.3527])), ('bias', tensor([0.1698]))])\n",
            "Epoch: 50 | loss: 0.2552763819694519 | Test loss: 0.4258342385292053\n",
            "OrderedDict([('weights', tensor([0.3566])), ('bias', tensor([0.1798]))])\n",
            "Epoch: 60 | loss: 0.24375538527965546 | Test loss: 0.41236335039138794\n",
            "OrderedDict([('weights', tensor([0.3605])), ('bias', tensor([0.1898]))])\n",
            "Epoch: 70 | loss: 0.232234388589859 | Test loss: 0.398892343044281\n",
            "OrderedDict([('weights', tensor([0.3644])), ('bias', tensor([0.1998]))])\n",
            "Epoch: 80 | loss: 0.22071340680122375 | Test loss: 0.38542139530181885\n",
            "OrderedDict([('weights', tensor([0.3683])), ('bias', tensor([0.2098]))])\n",
            "Epoch: 90 | loss: 0.2091923952102661 | Test loss: 0.3719504475593567\n",
            "OrderedDict([('weights', tensor([0.3722])), ('bias', tensor([0.2198]))])\n",
            "Epoch: 100 | loss: 0.19767141342163086 | Test loss: 0.35847947001457214\n",
            "OrderedDict([('weights', tensor([0.3761])), ('bias', tensor([0.2298]))])\n",
            "Epoch: 110 | loss: 0.18615040183067322 | Test loss: 0.34500852227211\n",
            "OrderedDict([('weights', tensor([0.3800])), ('bias', tensor([0.2398]))])\n",
            "Epoch: 120 | loss: 0.17462942004203796 | Test loss: 0.33153754472732544\n",
            "OrderedDict([('weights', tensor([0.3839])), ('bias', tensor([0.2498]))])\n",
            "Epoch: 130 | loss: 0.16310855746269226 | Test loss: 0.3180667459964752\n",
            "OrderedDict([('weights', tensor([0.3878])), ('bias', tensor([0.2598]))])\n",
            "Epoch: 140 | loss: 0.15158770978450775 | Test loss: 0.304595947265625\n",
            "OrderedDict([('weights', tensor([0.3917])), ('bias', tensor([0.2698]))])\n",
            "Epoch: 150 | loss: 0.14006686210632324 | Test loss: 0.2911251187324524\n",
            "OrderedDict([('weights', tensor([0.3956])), ('bias', tensor([0.2798]))])\n",
            "Epoch: 160 | loss: 0.12854602932929993 | Test loss: 0.2776543200016022\n",
            "OrderedDict([('weights', tensor([0.3995])), ('bias', tensor([0.2898]))])\n",
            "Epoch: 170 | loss: 0.11702518165111542 | Test loss: 0.26418352127075195\n",
            "OrderedDict([('weights', tensor([0.4034])), ('bias', tensor([0.2998]))])\n",
            "Epoch: 180 | loss: 0.10654274374246597 | Test loss: 0.251315176486969\n",
            "OrderedDict([('weights', tensor([0.4073])), ('bias', tensor([0.3092]))])\n",
            "Epoch: 190 | loss: 0.09747617691755295 | Test loss: 0.2392154484987259\n",
            "OrderedDict([('weights', tensor([0.4112])), ('bias', tensor([0.3179]))])\n",
            "Epoch: 200 | loss: 0.08973254263401031 | Test loss: 0.22795839607715607\n",
            "OrderedDict([('weights', tensor([0.4150])), ('bias', tensor([0.3257]))])\n",
            "Epoch: 210 | loss: 0.08306284993886948 | Test loss: 0.21740305423736572\n",
            "OrderedDict([('weights', tensor([0.4188])), ('bias', tensor([0.3329]))])\n",
            "Epoch: 220 | loss: 0.07729282230138779 | Test loss: 0.20750825107097626\n",
            "OrderedDict([('weights', tensor([0.4225])), ('bias', tensor([0.3395]))])\n",
            "Epoch: 230 | loss: 0.07242877781391144 | Test loss: 0.19840054214000702\n",
            "OrderedDict([('weights', tensor([0.4261])), ('bias', tensor([0.3454]))])\n",
            "Epoch: 240 | loss: 0.06828799843788147 | Test loss: 0.18992407619953156\n",
            "OrderedDict([('weights', tensor([0.4296])), ('bias', tensor([0.3508]))])\n",
            "Epoch: 250 | loss: 0.06476505100727081 | Test loss: 0.18208928406238556\n",
            "OrderedDict([('weights', tensor([0.4330])), ('bias', tensor([0.3556]))])\n",
            "Epoch: 260 | loss: 0.06174134090542793 | Test loss: 0.17478716373443604\n",
            "OrderedDict([('weights', tensor([0.4363])), ('bias', tensor([0.3599]))])\n",
            "Epoch: 270 | loss: 0.05915876477956772 | Test loss: 0.16802480816841125\n",
            "OrderedDict([('weights', tensor([0.4395])), ('bias', tensor([0.3638]))])\n",
            "Epoch: 280 | loss: 0.0569603331387043 | Test loss: 0.161748006939888\n",
            "OrderedDict([('weights', tensor([0.4426])), ('bias', tensor([0.3673]))])\n",
            "Epoch: 290 | loss: 0.05514555424451828 | Test loss: 0.15608695149421692\n",
            "OrderedDict([('weights', tensor([0.4456])), ('bias', tensor([0.3703]))])\n",
            "Epoch: 300 | loss: 0.05357731133699417 | Test loss: 0.15086300671100616\n",
            "OrderedDict([('weights', tensor([0.4485])), ('bias', tensor([0.3730]))])\n",
            "Epoch: 310 | loss: 0.05218071490526199 | Test loss: 0.14595307409763336\n",
            "OrderedDict([('weights', tensor([0.4513])), ('bias', tensor([0.3754]))])\n",
            "Epoch: 320 | loss: 0.05105169489979744 | Test loss: 0.1415499746799469\n",
            "OrderedDict([('weights', tensor([0.4540])), ('bias', tensor([0.3774]))])\n",
            "Epoch: 330 | loss: 0.050030313432216644 | Test loss: 0.1374681293964386\n",
            "OrderedDict([('weights', tensor([0.4567])), ('bias', tensor([0.3791]))])\n",
            "Epoch: 340 | loss: 0.049160152673721313 | Test loss: 0.13370750844478607\n",
            "OrderedDict([('weights', tensor([0.4592])), ('bias', tensor([0.3806]))])\n",
            "Epoch: 350 | loss: 0.04836020991206169 | Test loss: 0.13020753860473633\n",
            "OrderedDict([('weights', tensor([0.4617])), ('bias', tensor([0.3819]))])\n",
            "Epoch: 360 | loss: 0.04769856110215187 | Test loss: 0.12709848582744598\n",
            "OrderedDict([('weights', tensor([0.4640])), ('bias', tensor([0.3829]))])\n",
            "Epoch: 370 | loss: 0.04703690856695175 | Test loss: 0.12398938834667206\n",
            "OrderedDict([('weights', tensor([0.4664])), ('bias', tensor([0.3839]))])\n",
            "Epoch: 380 | loss: 0.04650093987584114 | Test loss: 0.12147434055805206\n",
            "OrderedDict([('weights', tensor([0.4686])), ('bias', tensor([0.3845]))])\n",
            "Epoch: 390 | loss: 0.045996345579624176 | Test loss: 0.11902527511119843\n",
            "OrderedDict([('weights', tensor([0.4708])), ('bias', tensor([0.3850]))])\n",
            "Epoch: 400 | loss: 0.04549176245927811 | Test loss: 0.11657620966434479\n",
            "OrderedDict([('weights', tensor([0.4730])), ('bias', tensor([0.3855]))])\n",
            "Epoch: 410 | loss: 0.045014552772045135 | Test loss: 0.11432783305644989\n",
            "OrderedDict([('weights', tensor([0.4751])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 420 | loss: 0.04461454600095749 | Test loss: 0.11254779994487762\n",
            "OrderedDict([('weights', tensor([0.4771])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 430 | loss: 0.04421453922986984 | Test loss: 0.11076776683330536\n",
            "OrderedDict([('weights', tensor([0.4791])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 440 | loss: 0.043814532458782196 | Test loss: 0.1089877337217331\n",
            "OrderedDict([('weights', tensor([0.4811])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 450 | loss: 0.04341452196240425 | Test loss: 0.10720770061016083\n",
            "OrderedDict([('weights', tensor([0.4831])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 460 | loss: 0.043014515191316605 | Test loss: 0.10542766749858856\n",
            "OrderedDict([('weights', tensor([0.4851])), ('bias', tensor([0.3858]))])\n",
            "Epoch: 470 | loss: 0.04265119880437851 | Test loss: 0.10419009625911713\n",
            "OrderedDict([('weights', tensor([0.4870])), ('bias', tensor([0.3854]))])\n",
            "Epoch: 480 | loss: 0.04230218380689621 | Test loss: 0.1030881255865097\n",
            "OrderedDict([('weights', tensor([0.4888])), ('bias', tensor([0.3849]))])\n",
            "Epoch: 490 | loss: 0.04195316880941391 | Test loss: 0.10198615491390228\n",
            "OrderedDict([('weights', tensor([0.4906])), ('bias', tensor([0.3844]))])\n",
            "Epoch: 500 | loss: 0.04160415381193161 | Test loss: 0.10088418424129486\n",
            "OrderedDict([('weights', tensor([0.4924])), ('bias', tensor([0.3839]))])\n",
            "Epoch: 510 | loss: 0.04125513881444931 | Test loss: 0.09978221356868744\n",
            "OrderedDict([('weights', tensor([0.4942])), ('bias', tensor([0.3834]))])\n",
            "Epoch: 520 | loss: 0.04090612381696701 | Test loss: 0.09868024289608002\n",
            "OrderedDict([('weights', tensor([0.4960])), ('bias', tensor([0.3829]))])\n",
            "Epoch: 530 | loss: 0.04055710881948471 | Test loss: 0.0975782722234726\n",
            "OrderedDict([('weights', tensor([0.4978])), ('bias', tensor([0.3824]))])\n",
            "Epoch: 540 | loss: 0.04020809382200241 | Test loss: 0.09647630155086517\n",
            "OrderedDict([('weights', tensor([0.4996])), ('bias', tensor([0.3819]))])\n",
            "Epoch: 550 | loss: 0.03985908254981041 | Test loss: 0.09537436813116074\n",
            "OrderedDict([('weights', tensor([0.5014])), ('bias', tensor([0.3814]))])\n",
            "Epoch: 560 | loss: 0.03951007127761841 | Test loss: 0.09427239745855331\n",
            "OrderedDict([('weights', tensor([0.5032])), ('bias', tensor([0.3809]))])\n",
            "Epoch: 570 | loss: 0.03916105255484581 | Test loss: 0.09317042678594589\n",
            "OrderedDict([('weights', tensor([0.5050])), ('bias', tensor([0.3804]))])\n",
            "Epoch: 580 | loss: 0.03881204128265381 | Test loss: 0.09206845611333847\n",
            "OrderedDict([('weights', tensor([0.5068])), ('bias', tensor([0.3799]))])\n",
            "Epoch: 590 | loss: 0.03846302255988121 | Test loss: 0.09096648544073105\n",
            "OrderedDict([('weights', tensor([0.5086])), ('bias', tensor([0.3794]))])\n",
            "Epoch: 600 | loss: 0.03811401128768921 | Test loss: 0.08986451476812363\n",
            "OrderedDict([('weights', tensor([0.5104])), ('bias', tensor([0.3789]))])\n",
            "Epoch: 610 | loss: 0.03776499256491661 | Test loss: 0.0887625440955162\n",
            "OrderedDict([('weights', tensor([0.5122])), ('bias', tensor([0.3784]))])\n",
            "Epoch: 620 | loss: 0.03741598129272461 | Test loss: 0.08766057342290878\n",
            "OrderedDict([('weights', tensor([0.5140])), ('bias', tensor([0.3779]))])\n",
            "Epoch: 630 | loss: 0.037067197263240814 | Test loss: 0.08662726730108261\n",
            "OrderedDict([('weights', tensor([0.5157])), ('bias', tensor([0.3774]))])\n",
            "Epoch: 640 | loss: 0.03672432154417038 | Test loss: 0.08579986542463303\n",
            "OrderedDict([('weights', tensor([0.5175])), ('bias', tensor([0.3767]))])\n",
            "Epoch: 650 | loss: 0.03638043254613876 | Test loss: 0.08497253060340881\n",
            "OrderedDict([('weights', tensor([0.5192])), ('bias', tensor([0.3760]))])\n",
            "Epoch: 660 | loss: 0.036036938428878784 | Test loss: 0.08421380817890167\n",
            "OrderedDict([('weights', tensor([0.5209])), ('bias', tensor([0.3752]))])\n",
            "Epoch: 670 | loss: 0.035693906247615814 | Test loss: 0.08338643610477448\n",
            "OrderedDict([('weights', tensor([0.5226])), ('bias', tensor([0.3745]))])\n",
            "Epoch: 680 | loss: 0.035350002348423004 | Test loss: 0.08255907148122787\n",
            "OrderedDict([('weights', tensor([0.5243])), ('bias', tensor([0.3738]))])\n",
            "Epoch: 690 | loss: 0.035006679594516754 | Test loss: 0.08180035650730133\n",
            "OrderedDict([('weights', tensor([0.5260])), ('bias', tensor([0.3731]))])\n",
            "Epoch: 700 | loss: 0.03466346859931946 | Test loss: 0.08097299933433533\n",
            "OrderedDict([('weights', tensor([0.5277])), ('bias', tensor([0.3724]))])\n",
            "Epoch: 710 | loss: 0.03431956097483635 | Test loss: 0.08014564216136932\n",
            "OrderedDict([('weights', tensor([0.5294])), ('bias', tensor([0.3717]))])\n",
            "Epoch: 720 | loss: 0.03397642448544502 | Test loss: 0.07938691228628159\n",
            "OrderedDict([('weights', tensor([0.5311])), ('bias', tensor([0.3709]))])\n",
            "Epoch: 730 | loss: 0.0336330309510231 | Test loss: 0.07855955511331558\n",
            "OrderedDict([('weights', tensor([0.5328])), ('bias', tensor([0.3702]))])\n",
            "Epoch: 740 | loss: 0.03328912332653999 | Test loss: 0.07773219794034958\n",
            "OrderedDict([('weights', tensor([0.5346])), ('bias', tensor([0.3695]))])\n",
            "Epoch: 750 | loss: 0.032946161925792694 | Test loss: 0.07697348296642303\n",
            "OrderedDict([('weights', tensor([0.5363])), ('bias', tensor([0.3688]))])\n",
            "Epoch: 760 | loss: 0.03260258957743645 | Test loss: 0.07614611089229584\n",
            "OrderedDict([('weights', tensor([0.5380])), ('bias', tensor([0.3681]))])\n",
            "Epoch: 770 | loss: 0.0322587676346302 | Test loss: 0.07538740336894989\n",
            "OrderedDict([('weights', tensor([0.5397])), ('bias', tensor([0.3673]))])\n",
            "Epoch: 780 | loss: 0.031915903091430664 | Test loss: 0.07456003129482269\n",
            "OrderedDict([('weights', tensor([0.5414])), ('bias', tensor([0.3666]))])\n",
            "Epoch: 790 | loss: 0.031572166830301285 | Test loss: 0.07373266667127609\n",
            "OrderedDict([('weights', tensor([0.5431])), ('bias', tensor([0.3659]))])\n",
            "Epoch: 800 | loss: 0.03122851625084877 | Test loss: 0.07297395914793015\n",
            "OrderedDict([('weights', tensor([0.5448])), ('bias', tensor([0.3652]))])\n",
            "Epoch: 810 | loss: 0.03088562563061714 | Test loss: 0.07214658707380295\n",
            "OrderedDict([('weights', tensor([0.5465])), ('bias', tensor([0.3645]))])\n",
            "Epoch: 820 | loss: 0.03054172359406948 | Test loss: 0.07131922990083694\n",
            "OrderedDict([('weights', tensor([0.5482])), ('bias', tensor([0.3638]))])\n",
            "Epoch: 830 | loss: 0.030198251828551292 | Test loss: 0.0705605149269104\n",
            "OrderedDict([('weights', tensor([0.5499])), ('bias', tensor([0.3630]))])\n",
            "Epoch: 840 | loss: 0.029855191707611084 | Test loss: 0.0697331428527832\n",
            "OrderedDict([('weights', tensor([0.5516])), ('bias', tensor([0.3623]))])\n",
            "Epoch: 850 | loss: 0.029511287808418274 | Test loss: 0.0689057856798172\n",
            "OrderedDict([('weights', tensor([0.5534])), ('bias', tensor([0.3616]))])\n",
            "Epoch: 860 | loss: 0.02916799485683441 | Test loss: 0.06814707070589066\n",
            "OrderedDict([('weights', tensor([0.5551])), ('bias', tensor([0.3609]))])\n",
            "Epoch: 870 | loss: 0.028824755921959877 | Test loss: 0.06731969118118286\n",
            "OrderedDict([('weights', tensor([0.5568])), ('bias', tensor([0.3602]))])\n",
            "Epoch: 880 | loss: 0.02848084643483162 | Test loss: 0.06649234890937805\n",
            "OrderedDict([('weights', tensor([0.5585])), ('bias', tensor([0.3595]))])\n",
            "Epoch: 890 | loss: 0.02813773788511753 | Test loss: 0.06573362648487091\n",
            "OrderedDict([('weights', tensor([0.5602])), ('bias', tensor([0.3587]))])\n",
            "Epoch: 900 | loss: 0.027794325724244118 | Test loss: 0.06490625441074371\n",
            "OrderedDict([('weights', tensor([0.5619])), ('bias', tensor([0.3580]))])\n",
            "Epoch: 910 | loss: 0.02745041623711586 | Test loss: 0.06407888978719711\n",
            "OrderedDict([('weights', tensor([0.5636])), ('bias', tensor([0.3573]))])\n",
            "Epoch: 920 | loss: 0.0271074827760458 | Test loss: 0.06332017481327057\n",
            "OrderedDict([('weights', tensor([0.5653])), ('bias', tensor([0.3566]))])\n",
            "Epoch: 930 | loss: 0.026763886213302612 | Test loss: 0.062492817640304565\n",
            "OrderedDict([('weights', tensor([0.5670])), ('bias', tensor([0.3559]))])\n",
            "Epoch: 940 | loss: 0.02642008289694786 | Test loss: 0.06173409894108772\n",
            "OrderedDict([('weights', tensor([0.5687])), ('bias', tensor([0.3551]))])\n",
            "Epoch: 950 | loss: 0.02607722207903862 | Test loss: 0.06090673804283142\n",
            "OrderedDict([('weights', tensor([0.5704])), ('bias', tensor([0.3544]))])\n",
            "Epoch: 960 | loss: 0.025733450427651405 | Test loss: 0.06007937341928482\n",
            "OrderedDict([('weights', tensor([0.5721])), ('bias', tensor([0.3537]))])\n",
            "Epoch: 970 | loss: 0.02538982965052128 | Test loss: 0.05932066589593887\n",
            "OrderedDict([('weights', tensor([0.5738])), ('bias', tensor([0.3530]))])\n",
            "Epoch: 980 | loss: 0.02504691109061241 | Test loss: 0.058493297547101974\n",
            "OrderedDict([('weights', tensor([0.5756])), ('bias', tensor([0.3523]))])\n",
            "Epoch: 990 | loss: 0.0247030109167099 | Test loss: 0.057665932923555374\n",
            "OrderedDict([('weights', tensor([0.5773])), ('bias', tensor([0.3516]))])\n",
            "Epoch: 1000 | loss: 0.02435956709086895 | Test loss: 0.05690721794962883\n",
            "OrderedDict([('weights', tensor([0.5790])), ('bias', tensor([0.3508]))])\n",
            "Epoch: 1010 | loss: 0.024016480892896652 | Test loss: 0.05607985332608223\n",
            "OrderedDict([('weights', tensor([0.5807])), ('bias', tensor([0.3501]))])\n",
            "Epoch: 1020 | loss: 0.02367258258163929 | Test loss: 0.055252473801374435\n",
            "OrderedDict([('weights', tensor([0.5824])), ('bias', tensor([0.3494]))])\n",
            "Epoch: 1030 | loss: 0.023329313844442368 | Test loss: 0.054493773728609085\n",
            "OrderedDict([('weights', tensor([0.5841])), ('bias', tensor([0.3487]))])\n",
            "Epoch: 1040 | loss: 0.022986043244600296 | Test loss: 0.05366641283035278\n",
            "OrderedDict([('weights', tensor([0.5858])), ('bias', tensor([0.3480]))])\n",
            "Epoch: 1050 | loss: 0.022642139345407486 | Test loss: 0.05283904820680618\n",
            "OrderedDict([('weights', tensor([0.5875])), ('bias', tensor([0.3473]))])\n",
            "Epoch: 1060 | loss: 0.02229905314743519 | Test loss: 0.05208033323287964\n",
            "OrderedDict([('weights', tensor([0.5892])), ('bias', tensor([0.3465]))])\n",
            "Epoch: 1070 | loss: 0.02195560745894909 | Test loss: 0.05125296115875244\n",
            "OrderedDict([('weights', tensor([0.5909])), ('bias', tensor([0.3458]))])\n",
            "Epoch: 1080 | loss: 0.021611705422401428 | Test loss: 0.05042559653520584\n",
            "OrderedDict([('weights', tensor([0.5927])), ('bias', tensor([0.3451]))])\n",
            "Epoch: 1090 | loss: 0.021268798038363457 | Test loss: 0.0496668815612793\n",
            "OrderedDict([('weights', tensor([0.5944])), ('bias', tensor([0.3444]))])\n",
            "Epoch: 1100 | loss: 0.02092517353594303 | Test loss: 0.0488395169377327\n",
            "OrderedDict([('weights', tensor([0.5961])), ('bias', tensor([0.3437]))])\n",
            "Epoch: 1110 | loss: 0.020581401884555817 | Test loss: 0.04808080196380615\n",
            "OrderedDict([('weights', tensor([0.5978])), ('bias', tensor([0.3429]))])\n",
            "Epoch: 1120 | loss: 0.020238537341356277 | Test loss: 0.047253452241420746\n",
            "OrderedDict([('weights', tensor([0.5995])), ('bias', tensor([0.3422]))])\n",
            "Epoch: 1130 | loss: 0.019894743338227272 | Test loss: 0.04642607644200325\n",
            "OrderedDict([('weights', tensor([0.6012])), ('bias', tensor([0.3415]))])\n",
            "Epoch: 1140 | loss: 0.019551146775484085 | Test loss: 0.0456673689186573\n",
            "OrderedDict([('weights', tensor([0.6029])), ('bias', tensor([0.3408]))])\n",
            "Epoch: 1150 | loss: 0.019208211451768875 | Test loss: 0.04483998939394951\n",
            "OrderedDict([('weights', tensor([0.6046])), ('bias', tensor([0.3401]))])\n",
            "Epoch: 1160 | loss: 0.018864300101995468 | Test loss: 0.044012635946273804\n",
            "OrderedDict([('weights', tensor([0.6063])), ('bias', tensor([0.3394]))])\n",
            "Epoch: 1170 | loss: 0.018520886078476906 | Test loss: 0.04325391724705696\n",
            "OrderedDict([('weights', tensor([0.6080])), ('bias', tensor([0.3386]))])\n",
            "Epoch: 1180 | loss: 0.01817776821553707 | Test loss: 0.04242655634880066\n",
            "OrderedDict([('weights', tensor([0.6097])), ('bias', tensor([0.3379]))])\n",
            "Epoch: 1190 | loss: 0.01783386431634426 | Test loss: 0.04159919545054436\n",
            "OrderedDict([('weights', tensor([0.6115])), ('bias', tensor([0.3372]))])\n",
            "Epoch: 1200 | loss: 0.017490629106760025 | Test loss: 0.04084048420190811\n",
            "OrderedDict([('weights', tensor([0.6131])), ('bias', tensor([0.3365]))])\n",
            "Epoch: 1210 | loss: 0.017147328704595566 | Test loss: 0.040013112127780914\n",
            "OrderedDict([('weights', tensor([0.6149])), ('bias', tensor([0.3358]))])\n",
            "Epoch: 1220 | loss: 0.016803432255983353 | Test loss: 0.03918575122952461\n",
            "OrderedDict([('weights', tensor([0.6166])), ('bias', tensor([0.3351]))])\n",
            "Epoch: 1230 | loss: 0.016460370272397995 | Test loss: 0.03842703625559807\n",
            "OrderedDict([('weights', tensor([0.6183])), ('bias', tensor([0.3343]))])\n",
            "Epoch: 1240 | loss: 0.016116898506879807 | Test loss: 0.03759966418147087\n",
            "OrderedDict([('weights', tensor([0.6200])), ('bias', tensor([0.3336]))])\n",
            "Epoch: 1250 | loss: 0.015772998332977295 | Test loss: 0.036772288382053375\n",
            "OrderedDict([('weights', tensor([0.6217])), ('bias', tensor([0.3329]))])\n",
            "Epoch: 1260 | loss: 0.01543011236935854 | Test loss: 0.03601359575986862\n",
            "OrderedDict([('weights', tensor([0.6234])), ('bias', tensor([0.3322]))])\n",
            "Epoch: 1270 | loss: 0.015086461789906025 | Test loss: 0.03518623113632202\n",
            "OrderedDict([('weights', tensor([0.6251])), ('bias', tensor([0.3315]))])\n",
            "Epoch: 1280 | loss: 0.014742719009518623 | Test loss: 0.03442750498652458\n",
            "OrderedDict([('weights', tensor([0.6268])), ('bias', tensor([0.3307]))])\n",
            "Epoch: 1290 | loss: 0.014399850741028786 | Test loss: 0.03360014408826828\n",
            "OrderedDict([('weights', tensor([0.6285])), ('bias', tensor([0.3300]))])\n",
            "Epoch: 1300 | loss: 0.014056024141609669 | Test loss: 0.03277278691530228\n",
            "OrderedDict([('weights', tensor([0.6302])), ('bias', tensor([0.3293]))])\n",
            "Epoch: 1310 | loss: 0.013712462969124317 | Test loss: 0.03201407939195633\n",
            "OrderedDict([('weights', tensor([0.6319])), ('bias', tensor([0.3286]))])\n",
            "Epoch: 1320 | loss: 0.013369491323828697 | Test loss: 0.031186699867248535\n",
            "OrderedDict([('weights', tensor([0.6337])), ('bias', tensor([0.3279]))])\n",
            "Epoch: 1330 | loss: 0.01302559394389391 | Test loss: 0.030359338968992233\n",
            "OrderedDict([('weights', tensor([0.6354])), ('bias', tensor([0.3272]))])\n",
            "Epoch: 1340 | loss: 0.012682202272117138 | Test loss: 0.029600614681839943\n",
            "OrderedDict([('weights', tensor([0.6371])), ('bias', tensor([0.3264]))])\n",
            "Epoch: 1350 | loss: 0.012339059263467789 | Test loss: 0.028773266822099686\n",
            "OrderedDict([('weights', tensor([0.6388])), ('bias', tensor([0.3257]))])\n",
            "Epoch: 1360 | loss: 0.011995160952210426 | Test loss: 0.02794589474797249\n",
            "OrderedDict([('weights', tensor([0.6405])), ('bias', tensor([0.3250]))])\n",
            "Epoch: 1370 | loss: 0.011651946231722832 | Test loss: 0.02718718722462654\n",
            "OrderedDict([('weights', tensor([0.6422])), ('bias', tensor([0.3243]))])\n",
            "Epoch: 1380 | loss: 0.011308628134429455 | Test loss: 0.026359815150499344\n",
            "OrderedDict([('weights', tensor([0.6439])), ('bias', tensor([0.3236]))])\n",
            "Epoch: 1390 | loss: 0.010964717715978622 | Test loss: 0.025532448664307594\n",
            "OrderedDict([('weights', tensor([0.6456])), ('bias', tensor([0.3229]))])\n",
            "Epoch: 1400 | loss: 0.010621682740747929 | Test loss: 0.0247737355530262\n",
            "OrderedDict([('weights', tensor([0.6473])), ('bias', tensor([0.3221]))])\n",
            "Epoch: 1410 | loss: 0.010278185829520226 | Test loss: 0.023946374654769897\n",
            "OrderedDict([('weights', tensor([0.6490])), ('bias', tensor([0.3214]))])\n",
            "Epoch: 1420 | loss: 0.009934291243553162 | Test loss: 0.023187648504972458\n",
            "OrderedDict([('weights', tensor([0.6507])), ('bias', tensor([0.3207]))])\n",
            "Epoch: 1430 | loss: 0.009591431356966496 | Test loss: 0.0223603006452322\n",
            "OrderedDict([('weights', tensor([0.6525])), ('bias', tensor([0.3200]))])\n",
            "Epoch: 1440 | loss: 0.00924774818122387 | Test loss: 0.021532922983169556\n",
            "OrderedDict([('weights', tensor([0.6542])), ('bias', tensor([0.3193]))])\n",
            "Epoch: 1450 | loss: 0.008904037065804005 | Test loss: 0.020774226635694504\n",
            "OrderedDict([('weights', tensor([0.6559])), ('bias', tensor([0.3185]))])\n",
            "Epoch: 1460 | loss: 0.008561169728636742 | Test loss: 0.019946854561567307\n",
            "OrderedDict([('weights', tensor([0.6576])), ('bias', tensor([0.3178]))])\n",
            "Epoch: 1470 | loss: 0.008217317052185535 | Test loss: 0.01911948248744011\n",
            "OrderedDict([('weights', tensor([0.6593])), ('bias', tensor([0.3171]))])\n",
            "Epoch: 1480 | loss: 0.007873778231441975 | Test loss: 0.018360769376158714\n",
            "OrderedDict([('weights', tensor([0.6610])), ('bias', tensor([0.3164]))])\n",
            "Epoch: 1490 | loss: 0.0075307851657271385 | Test loss: 0.01753341034054756\n",
            "OrderedDict([('weights', tensor([0.6627])), ('bias', tensor([0.3157]))])\n",
            "Epoch: 1500 | loss: 0.007186878472566605 | Test loss: 0.01670604944229126\n",
            "OrderedDict([('weights', tensor([0.6644])), ('bias', tensor([0.3150]))])\n",
            "Epoch: 1510 | loss: 0.006843519397079945 | Test loss: 0.01594732329249382\n",
            "OrderedDict([('weights', tensor([0.6661])), ('bias', tensor([0.3142]))])\n",
            "Epoch: 1520 | loss: 0.006500349845737219 | Test loss: 0.015119964256882668\n",
            "OrderedDict([('weights', tensor([0.6678])), ('bias', tensor([0.3135]))])\n",
            "Epoch: 1530 | loss: 0.006156443618237972 | Test loss: 0.014292603358626366\n",
            "OrderedDict([('weights', tensor([0.6695])), ('bias', tensor([0.3128]))])\n",
            "Epoch: 1540 | loss: 0.005813261028379202 | Test loss: 0.013533895835280418\n",
            "OrderedDict([('weights', tensor([0.6712])), ('bias', tensor([0.3121]))])\n",
            "Epoch: 1550 | loss: 0.005469909869134426 | Test loss: 0.012706518173217773\n",
            "OrderedDict([('weights', tensor([0.6730])), ('bias', tensor([0.3114]))])\n",
            "Epoch: 1560 | loss: 0.0051260096952319145 | Test loss: 0.011879158206284046\n",
            "OrderedDict([('weights', tensor([0.6747])), ('bias', tensor([0.3107]))])\n",
            "Epoch: 1570 | loss: 0.004782737232744694 | Test loss: 0.011086148209869862\n",
            "OrderedDict([('weights', tensor([0.6764])), ('bias', tensor([0.3099]))])\n",
            "Epoch: 1580 | loss: 0.004439301788806915 | Test loss: 0.010327416472136974\n",
            "OrderedDict([('weights', tensor([0.6781])), ('bias', tensor([0.3092]))])\n",
            "Epoch: 1590 | loss: 0.004096207674592733 | Test loss: 0.0095000509172678\n",
            "OrderedDict([('weights', tensor([0.6798])), ('bias', tensor([0.3085]))])\n",
            "Epoch: 1600 | loss: 0.003752306802198291 | Test loss: 0.008672690019011497\n",
            "OrderedDict([('weights', tensor([0.6815])), ('bias', tensor([0.3078]))])\n",
            "Epoch: 1610 | loss: 0.0034090480767190456 | Test loss: 0.007913952693343163\n",
            "OrderedDict([('weights', tensor([0.6832])), ('bias', tensor([0.3070]))])\n",
            "Epoch: 1620 | loss: 0.0030657730530947447 | Test loss: 0.007086604833602905\n",
            "OrderedDict([('weights', tensor([0.6849])), ('bias', tensor([0.3063]))])\n",
            "Epoch: 1630 | loss: 0.0027218691539019346 | Test loss: 0.006259244866669178\n",
            "OrderedDict([('weights', tensor([0.6866])), ('bias', tensor([0.3056]))])\n",
            "Epoch: 1640 | loss: 0.0023787864483892918 | Test loss: 0.005500525236129761\n",
            "OrderedDict([('weights', tensor([0.6883])), ('bias', tensor([0.3049]))])\n",
            "Epoch: 1650 | loss: 0.002035337733104825 | Test loss: 0.004673170857131481\n",
            "OrderedDict([('weights', tensor([0.6900])), ('bias', tensor([0.3042]))])\n",
            "Epoch: 1660 | loss: 0.0016914367442950606 | Test loss: 0.0038457990158349276\n",
            "OrderedDict([('weights', tensor([0.6918])), ('bias', tensor([0.3035]))])\n",
            "Epoch: 1670 | loss: 0.0013485297095030546 | Test loss: 0.0030870854388922453\n",
            "OrderedDict([('weights', tensor([0.6935])), ('bias', tensor([0.3027]))])\n",
            "Epoch: 1680 | loss: 0.0010049014817923307 | Test loss: 0.002259713364765048\n",
            "OrderedDict([('weights', tensor([0.6952])), ('bias', tensor([0.3020]))])\n",
            "Epoch: 1690 | loss: 0.0006616368773393333 | Test loss: 0.0014667033683508635\n",
            "OrderedDict([('weights', tensor([0.6969])), ('bias', tensor([0.3013]))])\n",
            "Epoch: 1700 | loss: 0.0003177322505507618 | Test loss: 0.0006393313524313271\n",
            "OrderedDict([('weights', tensor([0.6986])), ('bias', tensor([0.3006]))])\n",
            "Epoch: 1710 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1720 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1730 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1740 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1750 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1760 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1770 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1780 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1790 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1800 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1810 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1820 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1830 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1840 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1850 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1860 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1870 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1880 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1890 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1900 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1910 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1920 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1930 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1940 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1950 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1960 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1970 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1980 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 1990 | loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "OrderedDict([('weights', tensor([0.6997])), ('bias', tensor([0.3008]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejUIQpEVRGNS",
        "outputId": "57f5f5e8-176d-429f-d3ca-ca8f9d16427d"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.2998]))])"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Let's visualize loss\n",
        "plt.plot(epochs_counts, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\")\n",
        "plt.plot(epochs_counts, np.array(torch.tensor(test_loss_values).numpy()), label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FMlPwaYvcmHz",
        "outputId": "367ea418-badb-40ff-d4f5-d25764a68c7e"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  Y_preds_new = model_0(X_test)"
      ],
      "metadata": {
        "id": "N6NRcUusTlSJ"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=Y_preds_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "zDXIo5D9UBpp",
        "outputId": "8dfcb82e-9e3e-4cc9-d484-180a67637c06"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a model in PyTorch\n",
        "\n",
        "There are three main methods for saving and loading models in PyTorch.\n",
        "\n",
        "1. `torch.save()` - allows you a PyTorch object in Python's pickle format.\n",
        "2. `torch.load()` - allows you to load a saved PyTorch object.\n",
        "3. `torch.nn.Module.load_state_dict()` - this allows you to load a model's saved state dictionary."
      ],
      "metadata": {
        "id": "mpt6hhzkAXYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving PyTorch model\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create a models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_0.pt\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dictionary\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9N-kWHk6SsM",
        "outputId": "06685102-65a9-42a7-9551-b9bdf70e137f"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/01_pytorch_workflow_0.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a PyTorch model\n",
        "Since we save our model's `state_dict()` rather the entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that."
      ],
      "metadata": {
        "id": "F3s5kp3wADd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ZU80--CFWX",
        "outputId": "db5cb5d1-2b85-418b-a6a4-f1bc68beefe6"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.2998]))])"
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load in a saved state_dict we have to instantiate a new instance of our model class\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)\n",
        "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "loaded_model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT8Uoi4fA2YU",
        "outputId": "9760812d-6a67-4943-8623-5da63a244008"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.2998]))])"
            ]
          },
          "metadata": {},
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a couple prediction with the brand new model\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_0_preds = loaded_model_0(X_test)\n",
        "\n",
        "loaded_model_0_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1vZ5e0sCWZF",
        "outputId": "e77d5489-0b3f-44f6-d095-5eb9a369367e"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8593],\n",
              "        [0.8733],\n",
              "        [0.8873],\n",
              "        [0.9013],\n",
              "        [0.9152],\n",
              "        [0.9292],\n",
              "        [0.9432],\n",
              "        [0.9572],\n",
              "        [0.9712],\n",
              "        [0.9852]])"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare loaded model predictions with original model preds\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  model_0_preds = model_0(X_test)\n",
        "\n",
        "model_0_preds, loaded_model_0_preds, model_0_preds == loaded_model_0_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJW21eHECxnT",
        "outputId": "b0d2369b-ad85-430e-c0bc-419194d28a60"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.8593],\n",
              "         [0.8733],\n",
              "         [0.8873],\n",
              "         [0.9013],\n",
              "         [0.9152],\n",
              "         [0.9292],\n",
              "         [0.9432],\n",
              "         [0.9572],\n",
              "         [0.9712],\n",
              "         [0.9852]]),\n",
              " tensor([[0.8593],\n",
              "         [0.8733],\n",
              "         [0.8873],\n",
              "         [0.9013],\n",
              "         [0.9152],\n",
              "         [0.9292],\n",
              "         [0.9432],\n",
              "         [0.9572],\n",
              "         [0.9712],\n",
              "         [0.9852]]),\n",
              " tensor([[True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True],\n",
              "         [True]]))"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Putting it all togethe"
      ],
      "metadata": {
        "id": "uLul1YBsEmtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's check PyThorch version\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X5Cjf0fAFWwu",
        "outputId": "17442d5d-0e63-4171-e2fb-0e6698690c87"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create device-agnostic code.\n",
        "\n",
        "This means if we've got access to a GPU, our code will use it (for potentially faster computing).\n",
        "\n",
        "If no GPU is available, the code will default to using GPU"
      ],
      "metadata": {
        "id": "BkrWjmGBFze3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQMRuBK5GdQ_",
        "outputId": "29962dcc-1485-43d6-9585-80d96bf386a6"
      },
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Data"
      ],
      "metadata": {
        "id": "9qjv5dnqEySA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Create some data using the linear regression formula of y = weight * x + bias\n",
        "weight = -2\n",
        "bias = 10\n",
        "\n",
        "# Let's create a range values\n",
        "start = -1\n",
        "end = 1\n",
        "step = 0.01\n",
        "\n",
        "# Let's Create X and Y (features and labels)\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up\n",
        "Y = weight * X + bias\n",
        "X[:10], Y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqOxnm-aE75j",
        "outputId": "1c37aa65-e7e0-42ef-8f13-1f8b3e5885b0"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.0000],\n",
              "         [-0.9900],\n",
              "         [-0.9800],\n",
              "         [-0.9700],\n",
              "         [-0.9600],\n",
              "         [-0.9500],\n",
              "         [-0.9400],\n",
              "         [-0.9300],\n",
              "         [-0.9200],\n",
              "         [-0.9100]]),\n",
              " tensor([[12.0000],\n",
              "         [11.9800],\n",
              "         [11.9600],\n",
              "         [11.9400],\n",
              "         [11.9200],\n",
              "         [11.9000],\n",
              "         [11.8800],\n",
              "         [11.8600],\n",
              "         [11.8400],\n",
              "         [11.8200]]))"
            ]
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's split the data\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, Y_train = X[:train_split], Y[:train_split]\n",
        "X_test, Y_test = X[train_split:], Y[train_split:]\n",
        "len(X_train), len(Y_train), len(X_test), len(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SWtUU-5JaeY",
        "outputId": "37061454-b1ac-43a8-da60-ecc2e9e3945c"
      },
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 160, 40, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data\n",
        "plot_predictions(X_train, Y_train, X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "i65PS3T1KUwC",
        "outputId": "73c48387-5198-4bb3-c34c-6c8f768003f9"
      },
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a PyTorch Linear model"
      ],
      "metadata": {
        "id": "O_2Ac-c7LEzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a linear model by subclassing nn.Module\n",
        "class LinearRegressionModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Use nn.Linear() for creating the model parameters / also called: linear transform, probing layer, fully connected layer, dense layer\n",
        "    self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.linear_layer(x)\n",
        "\n",
        "# set the manual seed\n",
        "torch.manual_seed(24)\n",
        "model_1 = LinearRegressionModelV2()\n",
        "model_1, model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVV_WYSoLa7R",
        "outputId": "956a7321-36e5-4553-bd3c-8a12a3f4b460"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(LinearRegressionModelV2(\n",
              "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
              " ),\n",
              " OrderedDict([('linear_layer.weight', tensor([[0.5289]])),\n",
              "              ('linear_layer.bias', tensor([-0.2499]))]))"
            ]
          },
          "metadata": {},
          "execution_count": 350
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the model current device\n",
        "print(next(model_1.parameters()).device)\n",
        "\n",
        "# Let's set the model device to targeted device\n",
        "model_1.to(device)\n",
        "print(next(model_1.parameters()).device)\n",
        "\n",
        "print(next(model_1.parameters()).device)\n",
        "\n",
        "# Put data(training and test ) on target device\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "Y_train = Y_train.to(device)\n",
        "Y_test = Y_test.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_wEnUxGRGdI",
        "outputId": "bbe83e94-0a0b-43c3-a30a-3b4d2cfaf465"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Training\n",
        "For training we need:\n",
        "* Loss function\n",
        "* Optimizer\n",
        "* Training loop\n",
        "* Testing loop"
      ],
      "metadata": {
        "id": "hxNgI0v-Qjqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put data(training and test ) on cpu device\n",
        "device = device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "Y_train = Y_train.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = nn.L1Loss() # same as MAE\n",
        "\n",
        "# Setup our optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.0001)\n",
        "\n",
        "# Write a training loop\n",
        "epochs = 130000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model_1.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  Y_pred = model_1(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(Y_pred, Y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_1(X_test)\n",
        "    test_loss = loss_fn(test_pred, Y_test)\n",
        "\n",
        "  # Print out what's happening\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch {epoch} | Loss: {loss} | Test loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPqblHwrSKCu",
        "outputId": "50803a41-964d-4291-df44-17e661577882"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "Epoch 80000 | Loss: 2.4287984371185303 | Test loss: 1.5401076078414917\n",
            "Epoch 80010 | Loss: 2.427755117416382 | Test loss: 1.5392693281173706\n",
            "Epoch 80020 | Loss: 2.4267117977142334 | Test loss: 1.53843092918396\n",
            "Epoch 80030 | Loss: 2.425668478012085 | Test loss: 1.5375925302505493\n",
            "Epoch 80040 | Loss: 2.4246249198913574 | Test loss: 1.5367542505264282\n",
            "Epoch 80050 | Loss: 2.423581600189209 | Test loss: 1.535915732383728\n",
            "Epoch 80060 | Loss: 2.4225380420684814 | Test loss: 1.535077452659607\n",
            "Epoch 80070 | Loss: 2.421494722366333 | Test loss: 1.5342391729354858\n",
            "Epoch 80080 | Loss: 2.4204511642456055 | Test loss: 1.5334008932113647\n",
            "Epoch 80090 | Loss: 2.419407844543457 | Test loss: 1.532562494277954\n",
            "Epoch 80100 | Loss: 2.4183645248413086 | Test loss: 1.5317240953445435\n",
            "Epoch 80110 | Loss: 2.41732120513916 | Test loss: 1.5308856964111328\n",
            "Epoch 80120 | Loss: 2.4162776470184326 | Test loss: 1.5300472974777222\n",
            "Epoch 80130 | Loss: 2.415234327316284 | Test loss: 1.5292091369628906\n",
            "Epoch 80140 | Loss: 2.4141910076141357 | Test loss: 1.52837073802948\n",
            "Epoch 80150 | Loss: 2.4131476879119873 | Test loss: 1.5275324583053589\n",
            "Epoch 80160 | Loss: 2.412104368209839 | Test loss: 1.5266939401626587\n",
            "Epoch 80170 | Loss: 2.4110608100891113 | Test loss: 1.5258556604385376\n",
            "Epoch 80180 | Loss: 2.410017490386963 | Test loss: 1.525017261505127\n",
            "Epoch 80190 | Loss: 2.4089741706848145 | Test loss: 1.5241789817810059\n",
            "Epoch 80200 | Loss: 2.407930850982666 | Test loss: 1.5233405828475952\n",
            "Epoch 80210 | Loss: 2.4068870544433594 | Test loss: 1.5225021839141846\n",
            "Epoch 80220 | Loss: 2.405843734741211 | Test loss: 1.5216639041900635\n",
            "Epoch 80230 | Loss: 2.4048004150390625 | Test loss: 1.5208256244659424\n",
            "Epoch 80240 | Loss: 2.403757095336914 | Test loss: 1.5199871063232422\n",
            "Epoch 80250 | Loss: 2.4027135372161865 | Test loss: 1.519148826599121\n",
            "Epoch 80260 | Loss: 2.401670217514038 | Test loss: 1.518310546875\n",
            "Epoch 80270 | Loss: 2.4006268978118896 | Test loss: 1.5174721479415894\n",
            "Epoch 80280 | Loss: 2.399583578109741 | Test loss: 1.5166338682174683\n",
            "Epoch 80290 | Loss: 2.3985402584075928 | Test loss: 1.5157954692840576\n",
            "Epoch 80300 | Loss: 2.3974969387054443 | Test loss: 1.514957070350647\n",
            "Epoch 80310 | Loss: 2.396453619003296 | Test loss: 1.5141187906265259\n",
            "Epoch 80320 | Loss: 2.3954100608825684 | Test loss: 1.5132802724838257\n",
            "Epoch 80330 | Loss: 2.394366502761841 | Test loss: 1.5124419927597046\n",
            "Epoch 80340 | Loss: 2.3933231830596924 | Test loss: 1.511603593826294\n",
            "Epoch 80350 | Loss: 2.392279863357544 | Test loss: 1.5107653141021729\n",
            "Epoch 80360 | Loss: 2.3912363052368164 | Test loss: 1.5099270343780518\n",
            "Epoch 80370 | Loss: 2.390192985534668 | Test loss: 1.5090886354446411\n",
            "Epoch 80380 | Loss: 2.3891494274139404 | Test loss: 1.50825035572052\n",
            "Epoch 80390 | Loss: 2.388106107711792 | Test loss: 1.5074118375778198\n",
            "Epoch 80400 | Loss: 2.3870627880096436 | Test loss: 1.5065735578536987\n",
            "Epoch 80410 | Loss: 2.386019468307495 | Test loss: 1.5057352781295776\n",
            "Epoch 80420 | Loss: 2.3849759101867676 | Test loss: 1.504896879196167\n",
            "Epoch 80430 | Loss: 2.383932590484619 | Test loss: 1.5040584802627563\n",
            "Epoch 80440 | Loss: 2.3828892707824707 | Test loss: 1.5032202005386353\n",
            "Epoch 80450 | Loss: 2.3818459510803223 | Test loss: 1.502381682395935\n",
            "Epoch 80460 | Loss: 2.3808023929595947 | Test loss: 1.501543402671814\n",
            "Epoch 80470 | Loss: 2.379758834838867 | Test loss: 1.5007051229476929\n",
            "Epoch 80480 | Loss: 2.3787155151367188 | Test loss: 1.4998668432235718\n",
            "Epoch 80490 | Loss: 2.3776721954345703 | Test loss: 1.4990284442901611\n",
            "Epoch 80500 | Loss: 2.376628875732422 | Test loss: 1.4981900453567505\n",
            "Epoch 80510 | Loss: 2.3755855560302734 | Test loss: 1.4973516464233398\n",
            "Epoch 80520 | Loss: 2.374542236328125 | Test loss: 1.4965132474899292\n",
            "Epoch 80530 | Loss: 2.3734986782073975 | Test loss: 1.4956750869750977\n",
            "Epoch 80540 | Loss: 2.372455358505249 | Test loss: 1.494836688041687\n",
            "Epoch 80550 | Loss: 2.3714120388031006 | Test loss: 1.493998408317566\n",
            "Epoch 80560 | Loss: 2.370368719100952 | Test loss: 1.4931598901748657\n",
            "Epoch 80570 | Loss: 2.3693253993988037 | Test loss: 1.4923216104507446\n",
            "Epoch 80580 | Loss: 2.368281841278076 | Test loss: 1.491483211517334\n",
            "Epoch 80590 | Loss: 2.3672385215759277 | Test loss: 1.490644931793213\n",
            "Epoch 80600 | Loss: 2.3661949634552 | Test loss: 1.4898065328598022\n",
            "Epoch 80610 | Loss: 2.3651516437530518 | Test loss: 1.4889681339263916\n",
            "Epoch 80620 | Loss: 2.3641083240509033 | Test loss: 1.4881298542022705\n",
            "Epoch 80630 | Loss: 2.363065004348755 | Test loss: 1.4872915744781494\n",
            "Epoch 80640 | Loss: 2.3620214462280273 | Test loss: 1.4864530563354492\n",
            "Epoch 80650 | Loss: 2.3609778881073 | Test loss: 1.4856147766113281\n",
            "Epoch 80660 | Loss: 2.3599345684051514 | Test loss: 1.484776496887207\n",
            "Epoch 80670 | Loss: 2.358891248703003 | Test loss: 1.4839380979537964\n",
            "Epoch 80680 | Loss: 2.3578476905822754 | Test loss: 1.4830998182296753\n",
            "Epoch 80690 | Loss: 2.356804370880127 | Test loss: 1.4822614192962646\n",
            "Epoch 80700 | Loss: 2.3557610511779785 | Test loss: 1.481423020362854\n",
            "Epoch 80710 | Loss: 2.35471773147583 | Test loss: 1.480584740638733\n",
            "Epoch 80720 | Loss: 2.3536744117736816 | Test loss: 1.4797462224960327\n",
            "Epoch 80730 | Loss: 2.352631092071533 | Test loss: 1.4789079427719116\n",
            "Epoch 80740 | Loss: 2.3515872955322266 | Test loss: 1.478069543838501\n",
            "Epoch 80750 | Loss: 2.3505442142486572 | Test loss: 1.4772312641143799\n",
            "Epoch 80760 | Loss: 2.3495006561279297 | Test loss: 1.4763929843902588\n",
            "Epoch 80770 | Loss: 2.3484573364257812 | Test loss: 1.4755545854568481\n",
            "Epoch 80780 | Loss: 2.347414016723633 | Test loss: 1.474716305732727\n",
            "Epoch 80790 | Loss: 2.3463706970214844 | Test loss: 1.4738777875900269\n",
            "Epoch 80800 | Loss: 2.345327138900757 | Test loss: 1.4730395078659058\n",
            "Epoch 80810 | Loss: 2.3442838191986084 | Test loss: 1.4722012281417847\n",
            "Epoch 80820 | Loss: 2.34324049949646 | Test loss: 1.471362829208374\n",
            "Epoch 80830 | Loss: 2.3421971797943115 | Test loss: 1.4705244302749634\n",
            "Epoch 80840 | Loss: 2.341153621673584 | Test loss: 1.4696861505508423\n",
            "Epoch 80850 | Loss: 2.3401103019714355 | Test loss: 1.468847632408142\n",
            "Epoch 80860 | Loss: 2.339066743850708 | Test loss: 1.468009352684021\n",
            "Epoch 80870 | Loss: 2.3380234241485596 | Test loss: 1.4671710729599\n",
            "Epoch 80880 | Loss: 2.336979866027832 | Test loss: 1.4663327932357788\n",
            "Epoch 80890 | Loss: 2.3359365463256836 | Test loss: 1.4654943943023682\n",
            "Epoch 80900 | Loss: 2.334893226623535 | Test loss: 1.4646559953689575\n",
            "Epoch 80910 | Loss: 2.3338499069213867 | Test loss: 1.4638175964355469\n",
            "Epoch 80920 | Loss: 2.332806348800659 | Test loss: 1.4629791975021362\n",
            "Epoch 80930 | Loss: 2.3317630290985107 | Test loss: 1.4621410369873047\n",
            "Epoch 80940 | Loss: 2.3307197093963623 | Test loss: 1.461302638053894\n",
            "Epoch 80950 | Loss: 2.329676389694214 | Test loss: 1.460464358329773\n",
            "Epoch 80960 | Loss: 2.3286330699920654 | Test loss: 1.4596258401870728\n",
            "Epoch 80970 | Loss: 2.327589511871338 | Test loss: 1.4587875604629517\n",
            "Epoch 80980 | Loss: 2.3265461921691895 | Test loss: 1.457949161529541\n",
            "Epoch 80990 | Loss: 2.325502872467041 | Test loss: 1.45711088180542\n",
            "Epoch 81000 | Loss: 2.3244593143463135 | Test loss: 1.4562724828720093\n",
            "Epoch 81010 | Loss: 2.323415756225586 | Test loss: 1.4554340839385986\n",
            "Epoch 81020 | Loss: 2.3223724365234375 | Test loss: 1.4545958042144775\n",
            "Epoch 81030 | Loss: 2.321329116821289 | Test loss: 1.4537575244903564\n",
            "Epoch 81040 | Loss: 2.3202857971191406 | Test loss: 1.4529190063476562\n",
            "Epoch 81050 | Loss: 2.319242477416992 | Test loss: 1.4520807266235352\n",
            "Epoch 81060 | Loss: 2.3181989192962646 | Test loss: 1.451242446899414\n",
            "Epoch 81070 | Loss: 2.317155599594116 | Test loss: 1.4504040479660034\n",
            "Epoch 81080 | Loss: 2.3161122798919678 | Test loss: 1.4495657682418823\n",
            "Epoch 81090 | Loss: 2.3150689601898193 | Test loss: 1.4487273693084717\n",
            "Epoch 81100 | Loss: 2.314025640487671 | Test loss: 1.447888970375061\n",
            "Epoch 81110 | Loss: 2.3129823207855225 | Test loss: 1.44705069065094\n",
            "Epoch 81120 | Loss: 2.311938762664795 | Test loss: 1.4462121725082397\n",
            "Epoch 81130 | Loss: 2.3108952045440674 | Test loss: 1.4453738927841187\n",
            "Epoch 81140 | Loss: 2.309851884841919 | Test loss: 1.444535493850708\n",
            "Epoch 81150 | Loss: 2.3088085651397705 | Test loss: 1.443697214126587\n",
            "Epoch 81160 | Loss: 2.307765007019043 | Test loss: 1.4428589344024658\n",
            "Epoch 81170 | Loss: 2.3067216873168945 | Test loss: 1.4420205354690552\n",
            "Epoch 81180 | Loss: 2.305678367614746 | Test loss: 1.441182255744934\n",
            "Epoch 81190 | Loss: 2.3046348094940186 | Test loss: 1.4403437376022339\n",
            "Epoch 81200 | Loss: 2.30359148979187 | Test loss: 1.4395054578781128\n",
            "Epoch 81210 | Loss: 2.3025481700897217 | Test loss: 1.4386671781539917\n",
            "Epoch 81220 | Loss: 2.3015048503875732 | Test loss: 1.437828779220581\n",
            "Epoch 81230 | Loss: 2.3004612922668457 | Test loss: 1.4369903802871704\n",
            "Epoch 81240 | Loss: 2.2994179725646973 | Test loss: 1.4361521005630493\n",
            "Epoch 81250 | Loss: 2.298374652862549 | Test loss: 1.4353135824203491\n",
            "Epoch 81260 | Loss: 2.2973310947418213 | Test loss: 1.434475302696228\n",
            "Epoch 81270 | Loss: 2.296287775039673 | Test loss: 1.433637022972107\n",
            "Epoch 81280 | Loss: 2.2952442169189453 | Test loss: 1.4327987432479858\n",
            "Epoch 81290 | Loss: 2.294200897216797 | Test loss: 1.4319603443145752\n",
            "Epoch 81300 | Loss: 2.2931575775146484 | Test loss: 1.4311219453811646\n",
            "Epoch 81310 | Loss: 2.2921142578125 | Test loss: 1.430283546447754\n",
            "Epoch 81320 | Loss: 2.2910709381103516 | Test loss: 1.4294451475143433\n",
            "Epoch 81330 | Loss: 2.290027379989624 | Test loss: 1.4286069869995117\n",
            "Epoch 81340 | Loss: 2.2889840602874756 | Test loss: 1.427768588066101\n",
            "Epoch 81350 | Loss: 2.287940740585327 | Test loss: 1.42693030834198\n",
            "Epoch 81360 | Loss: 2.2868974208831787 | Test loss: 1.4260917901992798\n",
            "Epoch 81370 | Loss: 2.2858541011810303 | Test loss: 1.4252535104751587\n",
            "Epoch 81380 | Loss: 2.284810781478882 | Test loss: 1.424415111541748\n",
            "Epoch 81390 | Loss: 2.2837672233581543 | Test loss: 1.423576831817627\n",
            "Epoch 81400 | Loss: 2.2827236652374268 | Test loss: 1.4227384328842163\n",
            "Epoch 81410 | Loss: 2.2816803455352783 | Test loss: 1.4219000339508057\n",
            "Epoch 81420 | Loss: 2.28063702583313 | Test loss: 1.4210617542266846\n",
            "Epoch 81430 | Loss: 2.2795937061309814 | Test loss: 1.4202234745025635\n",
            "Epoch 81440 | Loss: 2.278550148010254 | Test loss: 1.4193849563598633\n",
            "Epoch 81450 | Loss: 2.2775065898895264 | Test loss: 1.4185466766357422\n",
            "Epoch 81460 | Loss: 2.276463270187378 | Test loss: 1.417708396911621\n",
            "Epoch 81470 | Loss: 2.2754199504852295 | Test loss: 1.4168699979782104\n",
            "Epoch 81480 | Loss: 2.274376392364502 | Test loss: 1.4160317182540894\n",
            "Epoch 81490 | Loss: 2.2733330726623535 | Test loss: 1.4151933193206787\n",
            "Epoch 81500 | Loss: 2.272289752960205 | Test loss: 1.414354920387268\n",
            "Epoch 81510 | Loss: 2.2712464332580566 | Test loss: 1.413516640663147\n",
            "Epoch 81520 | Loss: 2.270203113555908 | Test loss: 1.4126781225204468\n",
            "Epoch 81530 | Loss: 2.2691597938537598 | Test loss: 1.4118398427963257\n",
            "Epoch 81540 | Loss: 2.2681162357330322 | Test loss: 1.411001443862915\n",
            "Epoch 81550 | Loss: 2.267072916030884 | Test loss: 1.410163164138794\n",
            "Epoch 81560 | Loss: 2.2660293579101562 | Test loss: 1.4093248844146729\n",
            "Epoch 81570 | Loss: 2.264986038208008 | Test loss: 1.4084864854812622\n",
            "Epoch 81580 | Loss: 2.2639427185058594 | Test loss: 1.4076482057571411\n",
            "Epoch 81590 | Loss: 2.262899398803711 | Test loss: 1.406809687614441\n",
            "Epoch 81600 | Loss: 2.2618558406829834 | Test loss: 1.4059714078903198\n",
            "Epoch 81610 | Loss: 2.260812520980835 | Test loss: 1.4051331281661987\n",
            "Epoch 81620 | Loss: 2.2597692012786865 | Test loss: 1.404294729232788\n",
            "Epoch 81630 | Loss: 2.258725881576538 | Test loss: 1.4034563302993774\n",
            "Epoch 81640 | Loss: 2.2576823234558105 | Test loss: 1.4026180505752563\n",
            "Epoch 81650 | Loss: 2.256639003753662 | Test loss: 1.4017795324325562\n",
            "Epoch 81660 | Loss: 2.2555954456329346 | Test loss: 1.400941252708435\n",
            "Epoch 81670 | Loss: 2.254552125930786 | Test loss: 1.400102972984314\n",
            "Epoch 81680 | Loss: 2.2535085678100586 | Test loss: 1.3992646932601929\n",
            "Epoch 81690 | Loss: 2.25246524810791 | Test loss: 1.3984262943267822\n",
            "Epoch 81700 | Loss: 2.2514219284057617 | Test loss: 1.3975878953933716\n",
            "Epoch 81710 | Loss: 2.2503786087036133 | Test loss: 1.396749496459961\n",
            "Epoch 81720 | Loss: 2.2493350505828857 | Test loss: 1.3959110975265503\n",
            "Epoch 81730 | Loss: 2.2482917308807373 | Test loss: 1.3950729370117188\n",
            "Epoch 81740 | Loss: 2.247248411178589 | Test loss: 1.394234538078308\n",
            "Epoch 81750 | Loss: 2.2462050914764404 | Test loss: 1.393396258354187\n",
            "Epoch 81760 | Loss: 2.245161771774292 | Test loss: 1.3925577402114868\n",
            "Epoch 81770 | Loss: 2.2441182136535645 | Test loss: 1.3917194604873657\n",
            "Epoch 81780 | Loss: 2.243074893951416 | Test loss: 1.390881061553955\n",
            "Epoch 81790 | Loss: 2.2420315742492676 | Test loss: 1.390042781829834\n",
            "Epoch 81800 | Loss: 2.240988254547119 | Test loss: 1.3892043828964233\n",
            "Epoch 81810 | Loss: 2.2399444580078125 | Test loss: 1.3883659839630127\n",
            "Epoch 81820 | Loss: 2.238901138305664 | Test loss: 1.3875277042388916\n",
            "Epoch 81830 | Loss: 2.2378578186035156 | Test loss: 1.3866894245147705\n",
            "Epoch 81840 | Loss: 2.236814498901367 | Test loss: 1.3858509063720703\n",
            "Epoch 81850 | Loss: 2.2357711791992188 | Test loss: 1.3850126266479492\n",
            "Epoch 81860 | Loss: 2.234727621078491 | Test loss: 1.3841743469238281\n",
            "Epoch 81870 | Loss: 2.2336843013763428 | Test loss: 1.3833359479904175\n",
            "Epoch 81880 | Loss: 2.2326409816741943 | Test loss: 1.3824976682662964\n",
            "Epoch 81890 | Loss: 2.231597661972046 | Test loss: 1.3816592693328857\n",
            "Epoch 81900 | Loss: 2.2305543422698975 | Test loss: 1.380820870399475\n",
            "Epoch 81910 | Loss: 2.229511022567749 | Test loss: 1.379982590675354\n",
            "Epoch 81920 | Loss: 2.2284674644470215 | Test loss: 1.3791440725326538\n",
            "Epoch 81930 | Loss: 2.227423906326294 | Test loss: 1.3783057928085327\n",
            "Epoch 81940 | Loss: 2.2263805866241455 | Test loss: 1.377467393875122\n",
            "Epoch 81950 | Loss: 2.225337266921997 | Test loss: 1.376629114151001\n",
            "Epoch 81960 | Loss: 2.2242937088012695 | Test loss: 1.3757908344268799\n",
            "Epoch 81970 | Loss: 2.223250389099121 | Test loss: 1.3749524354934692\n",
            "Epoch 81980 | Loss: 2.2222070693969727 | Test loss: 1.3741141557693481\n",
            "Epoch 81990 | Loss: 2.221163511276245 | Test loss: 1.373275637626648\n",
            "Epoch 82000 | Loss: 2.2201201915740967 | Test loss: 1.3724373579025269\n",
            "Epoch 82010 | Loss: 2.2190768718719482 | Test loss: 1.3715990781784058\n",
            "Epoch 82020 | Loss: 2.2180335521698 | Test loss: 1.3707606792449951\n",
            "Epoch 82030 | Loss: 2.2169899940490723 | Test loss: 1.3699222803115845\n",
            "Epoch 82040 | Loss: 2.215946674346924 | Test loss: 1.3690840005874634\n",
            "Epoch 82050 | Loss: 2.2149033546447754 | Test loss: 1.3682454824447632\n",
            "Epoch 82060 | Loss: 2.213859796524048 | Test loss: 1.367407202720642\n",
            "Epoch 82070 | Loss: 2.2128164768218994 | Test loss: 1.366568922996521\n",
            "Epoch 82080 | Loss: 2.211772918701172 | Test loss: 1.3657306432724\n",
            "Epoch 82090 | Loss: 2.2107295989990234 | Test loss: 1.3648922443389893\n",
            "Epoch 82100 | Loss: 2.209686279296875 | Test loss: 1.3640538454055786\n",
            "Epoch 82110 | Loss: 2.2086429595947266 | Test loss: 1.363215446472168\n",
            "Epoch 82120 | Loss: 2.207599639892578 | Test loss: 1.3623770475387573\n",
            "Epoch 82130 | Loss: 2.2065560817718506 | Test loss: 1.3615388870239258\n",
            "Epoch 82140 | Loss: 2.205512762069702 | Test loss: 1.3607004880905151\n",
            "Epoch 82150 | Loss: 2.2044694423675537 | Test loss: 1.359862208366394\n",
            "Epoch 82160 | Loss: 2.2034261226654053 | Test loss: 1.3590236902236938\n",
            "Epoch 82170 | Loss: 2.202382802963257 | Test loss: 1.3581854104995728\n",
            "Epoch 82180 | Loss: 2.2013392448425293 | Test loss: 1.357347011566162\n",
            "Epoch 82190 | Loss: 2.200295925140381 | Test loss: 1.356508731842041\n",
            "Epoch 82200 | Loss: 2.1992523670196533 | Test loss: 1.3556703329086304\n",
            "Epoch 82210 | Loss: 2.198209047317505 | Test loss: 1.3548319339752197\n",
            "Epoch 82220 | Loss: 2.1971657276153564 | Test loss: 1.3539936542510986\n",
            "Epoch 82230 | Loss: 2.196122407913208 | Test loss: 1.3531553745269775\n",
            "Epoch 82240 | Loss: 2.1950788497924805 | Test loss: 1.3523168563842773\n",
            "Epoch 82250 | Loss: 2.194035291671753 | Test loss: 1.3514785766601562\n",
            "Epoch 82260 | Loss: 2.1929919719696045 | Test loss: 1.3506402969360352\n",
            "Epoch 82270 | Loss: 2.191948652267456 | Test loss: 1.3498018980026245\n",
            "Epoch 82280 | Loss: 2.1909050941467285 | Test loss: 1.3489636182785034\n",
            "Epoch 82290 | Loss: 2.18986177444458 | Test loss: 1.3481252193450928\n",
            "Epoch 82300 | Loss: 2.1888184547424316 | Test loss: 1.3472868204116821\n",
            "Epoch 82310 | Loss: 2.187775135040283 | Test loss: 1.346448540687561\n",
            "Epoch 82320 | Loss: 2.1867318153381348 | Test loss: 1.3456100225448608\n",
            "Epoch 82330 | Loss: 2.1856884956359863 | Test loss: 1.3447717428207397\n",
            "Epoch 82340 | Loss: 2.184644937515259 | Test loss: 1.343933343887329\n",
            "Epoch 82350 | Loss: 2.1836013793945312 | Test loss: 1.343095064163208\n",
            "Epoch 82360 | Loss: 2.182558059692383 | Test loss: 1.342256784439087\n",
            "Epoch 82370 | Loss: 2.1815147399902344 | Test loss: 1.3414183855056763\n",
            "Epoch 82380 | Loss: 2.180471420288086 | Test loss: 1.3405801057815552\n",
            "Epoch 82390 | Loss: 2.1794281005859375 | Test loss: 1.339741587638855\n",
            "Epoch 82400 | Loss: 2.17838454246521 | Test loss: 1.3389033079147339\n",
            "Epoch 82410 | Loss: 2.1773412227630615 | Test loss: 1.3380650281906128\n",
            "Epoch 82420 | Loss: 2.176297903060913 | Test loss: 1.3372266292572021\n",
            "Epoch 82430 | Loss: 2.1752545833587646 | Test loss: 1.3363882303237915\n",
            "Epoch 82440 | Loss: 2.174211025238037 | Test loss: 1.3355499505996704\n",
            "Epoch 82450 | Loss: 2.1731677055358887 | Test loss: 1.3347114324569702\n",
            "Epoch 82460 | Loss: 2.172124147415161 | Test loss: 1.3338731527328491\n",
            "Epoch 82470 | Loss: 2.171081304550171 | Test loss: 1.3330353498458862\n",
            "Epoch 82480 | Loss: 2.1700379848480225 | Test loss: 1.3321970701217651\n",
            "Epoch 82490 | Loss: 2.168994665145874 | Test loss: 1.3313586711883545\n",
            "Epoch 82500 | Loss: 2.1679513454437256 | Test loss: 1.3305202722549438\n",
            "Epoch 82510 | Loss: 2.166908025741577 | Test loss: 1.3296819925308228\n",
            "Epoch 82520 | Loss: 2.1658647060394287 | Test loss: 1.3288434743881226\n",
            "Epoch 82530 | Loss: 2.164820909500122 | Test loss: 1.3280051946640015\n",
            "Epoch 82540 | Loss: 2.1637775897979736 | Test loss: 1.3271667957305908\n",
            "Epoch 82550 | Loss: 2.162734270095825 | Test loss: 1.3263285160064697\n",
            "Epoch 82560 | Loss: 2.1616907119750977 | Test loss: 1.3254902362823486\n",
            "Epoch 82570 | Loss: 2.160647392272949 | Test loss: 1.324651837348938\n",
            "Epoch 82580 | Loss: 2.159604072570801 | Test loss: 1.323813557624817\n",
            "Epoch 82590 | Loss: 2.1585605144500732 | Test loss: 1.3229750394821167\n",
            "Epoch 82600 | Loss: 2.157517194747925 | Test loss: 1.3221367597579956\n",
            "Epoch 82610 | Loss: 2.1564738750457764 | Test loss: 1.3212984800338745\n",
            "Epoch 82620 | Loss: 2.155430555343628 | Test loss: 1.3204600811004639\n",
            "Epoch 82630 | Loss: 2.1543869972229004 | Test loss: 1.3196216821670532\n",
            "Epoch 82640 | Loss: 2.153343677520752 | Test loss: 1.3187834024429321\n",
            "Epoch 82650 | Loss: 2.1523003578186035 | Test loss: 1.317944884300232\n",
            "Epoch 82660 | Loss: 2.151257038116455 | Test loss: 1.3171066045761108\n",
            "Epoch 82670 | Loss: 2.1502132415771484 | Test loss: 1.3162683248519897\n",
            "Epoch 82680 | Loss: 2.149170160293579 | Test loss: 1.3154300451278687\n",
            "Epoch 82690 | Loss: 2.1481266021728516 | Test loss: 1.314591646194458\n",
            "Epoch 82700 | Loss: 2.147083282470703 | Test loss: 1.3137532472610474\n",
            "Epoch 82710 | Loss: 2.1460399627685547 | Test loss: 1.3129148483276367\n",
            "Epoch 82720 | Loss: 2.1449966430664062 | Test loss: 1.312076449394226\n",
            "Epoch 82730 | Loss: 2.1439530849456787 | Test loss: 1.3112382888793945\n",
            "Epoch 82740 | Loss: 2.1429097652435303 | Test loss: 1.3103998899459839\n",
            "Epoch 82750 | Loss: 2.141866445541382 | Test loss: 1.3095616102218628\n",
            "Epoch 82760 | Loss: 2.1408231258392334 | Test loss: 1.3087230920791626\n",
            "Epoch 82770 | Loss: 2.139779806137085 | Test loss: 1.3078848123550415\n",
            "Epoch 82780 | Loss: 2.1387364864349365 | Test loss: 1.3070465326309204\n",
            "Epoch 82790 | Loss: 2.137692928314209 | Test loss: 1.3062080144882202\n",
            "Epoch 82800 | Loss: 2.1366493701934814 | Test loss: 1.3053697347640991\n",
            "Epoch 82810 | Loss: 2.135606050491333 | Test loss: 1.3045313358306885\n",
            "Epoch 82820 | Loss: 2.1345627307891846 | Test loss: 1.3036930561065674\n",
            "Epoch 82830 | Loss: 2.133519172668457 | Test loss: 1.3028547763824463\n",
            "Epoch 82840 | Loss: 2.1324758529663086 | Test loss: 1.302016258239746\n",
            "Epoch 82850 | Loss: 2.131432294845581 | Test loss: 1.301177978515625\n",
            "Epoch 82860 | Loss: 2.1303889751434326 | Test loss: 1.3003395795822144\n",
            "Epoch 82870 | Loss: 2.129345655441284 | Test loss: 1.2995012998580933\n",
            "Epoch 82880 | Loss: 2.1283023357391357 | Test loss: 1.2986630201339722\n",
            "Epoch 82890 | Loss: 2.1272590160369873 | Test loss: 1.2978246212005615\n",
            "Epoch 82900 | Loss: 2.1262154579162598 | Test loss: 1.2969862222671509\n",
            "Epoch 82910 | Loss: 2.1251721382141113 | Test loss: 1.2961479425430298\n",
            "Epoch 82920 | Loss: 2.124128818511963 | Test loss: 1.2953094244003296\n",
            "Epoch 82930 | Loss: 2.1230854988098145 | Test loss: 1.2944711446762085\n",
            "Epoch 82940 | Loss: 2.122041702270508 | Test loss: 1.2936327457427979\n",
            "Epoch 82950 | Loss: 2.1209983825683594 | Test loss: 1.2927944660186768\n",
            "Epoch 82960 | Loss: 2.119955062866211 | Test loss: 1.2919561862945557\n",
            "Epoch 82970 | Loss: 2.1189117431640625 | Test loss: 1.291117787361145\n",
            "Epoch 82980 | Loss: 2.117868423461914 | Test loss: 1.2902793884277344\n",
            "Epoch 82990 | Loss: 2.1168251037597656 | Test loss: 1.2894411087036133\n",
            "Epoch 83000 | Loss: 2.115781545639038 | Test loss: 1.2886027097702026\n",
            "Epoch 83010 | Loss: 2.1147382259368896 | Test loss: 1.2877644300460815\n",
            "Epoch 83020 | Loss: 2.113694906234741 | Test loss: 1.286926031112671\n",
            "Epoch 83030 | Loss: 2.1126515865325928 | Test loss: 1.2860876321792603\n",
            "Epoch 83040 | Loss: 2.1116082668304443 | Test loss: 1.2852493524551392\n",
            "Epoch 83050 | Loss: 2.110564708709717 | Test loss: 1.284410834312439\n",
            "Epoch 83060 | Loss: 2.1095211505889893 | Test loss: 1.2835725545883179\n",
            "Epoch 83070 | Loss: 2.108477830886841 | Test loss: 1.2827342748641968\n",
            "Epoch 83080 | Loss: 2.1074345111846924 | Test loss: 1.2818959951400757\n",
            "Epoch 83090 | Loss: 2.106390953063965 | Test loss: 1.281057596206665\n",
            "Epoch 83100 | Loss: 2.1053476333618164 | Test loss: 1.2802191972732544\n",
            "Epoch 83110 | Loss: 2.104304313659668 | Test loss: 1.2793807983398438\n",
            "Epoch 83120 | Loss: 2.1032607555389404 | Test loss: 1.278542399406433\n",
            "Epoch 83130 | Loss: 2.102217435836792 | Test loss: 1.2777042388916016\n",
            "Epoch 83140 | Loss: 2.1011741161346436 | Test loss: 1.276865839958191\n",
            "Epoch 83150 | Loss: 2.100130796432495 | Test loss: 1.2760275602340698\n",
            "Epoch 83160 | Loss: 2.0990874767303467 | Test loss: 1.2751890420913696\n",
            "Epoch 83170 | Loss: 2.098043918609619 | Test loss: 1.2743507623672485\n",
            "Epoch 83180 | Loss: 2.0970005989074707 | Test loss: 1.2735124826431274\n",
            "Epoch 83190 | Loss: 2.0959572792053223 | Test loss: 1.2726739645004272\n",
            "Epoch 83200 | Loss: 2.094913959503174 | Test loss: 1.2718356847763062\n",
            "Epoch 83210 | Loss: 2.0938704013824463 | Test loss: 1.2709972858428955\n",
            "Epoch 83220 | Loss: 2.0928268432617188 | Test loss: 1.2701590061187744\n",
            "Epoch 83230 | Loss: 2.0917835235595703 | Test loss: 1.2693207263946533\n",
            "Epoch 83240 | Loss: 2.090740203857422 | Test loss: 1.2684822082519531\n",
            "Epoch 83250 | Loss: 2.0896966457366943 | Test loss: 1.267643928527832\n",
            "Epoch 83260 | Loss: 2.088653326034546 | Test loss: 1.2668055295944214\n",
            "Epoch 83270 | Loss: 2.0876100063323975 | Test loss: 1.2659672498703003\n",
            "Epoch 83280 | Loss: 2.086566686630249 | Test loss: 1.2651289701461792\n",
            "Epoch 83290 | Loss: 2.0855233669281006 | Test loss: 1.2642905712127686\n",
            "Epoch 83300 | Loss: 2.084480047225952 | Test loss: 1.263452172279358\n",
            "Epoch 83310 | Loss: 2.0834364891052246 | Test loss: 1.2626138925552368\n",
            "Epoch 83320 | Loss: 2.0823934078216553 | Test loss: 1.2617753744125366\n",
            "Epoch 83330 | Loss: 2.0813496112823486 | Test loss: 1.2609370946884155\n",
            "Epoch 83340 | Loss: 2.0803062915802 | Test loss: 1.2600986957550049\n",
            "Epoch 83350 | Loss: 2.0792627334594727 | Test loss: 1.2592604160308838\n",
            "Epoch 83360 | Loss: 2.078219413757324 | Test loss: 1.2584221363067627\n",
            "Epoch 83370 | Loss: 2.077176094055176 | Test loss: 1.257583737373352\n",
            "Epoch 83380 | Loss: 2.0761327743530273 | Test loss: 1.2567453384399414\n",
            "Epoch 83390 | Loss: 2.0750892162323 | Test loss: 1.2559070587158203\n",
            "Epoch 83400 | Loss: 2.0740458965301514 | Test loss: 1.2550686597824097\n",
            "Epoch 83410 | Loss: 2.073002576828003 | Test loss: 1.2542303800582886\n",
            "Epoch 83420 | Loss: 2.0719592571258545 | Test loss: 1.253391981124878\n",
            "Epoch 83430 | Loss: 2.070915699005127 | Test loss: 1.2525535821914673\n",
            "Epoch 83440 | Loss: 2.0698723793029785 | Test loss: 1.2517153024673462\n",
            "Epoch 83450 | Loss: 2.06882905960083 | Test loss: 1.250876784324646\n",
            "Epoch 83460 | Loss: 2.0677857398986816 | Test loss: 1.250038504600525\n",
            "Epoch 83470 | Loss: 2.066741943359375 | Test loss: 1.2492002248764038\n",
            "Epoch 83480 | Loss: 2.0656988620758057 | Test loss: 1.2483619451522827\n",
            "Epoch 83490 | Loss: 2.064655303955078 | Test loss: 1.247523546218872\n",
            "Epoch 83500 | Loss: 2.0636119842529297 | Test loss: 1.2466851472854614\n",
            "Epoch 83510 | Loss: 2.062568426132202 | Test loss: 1.2458467483520508\n",
            "Epoch 83520 | Loss: 2.061525344848633 | Test loss: 1.2450083494186401\n",
            "Epoch 83530 | Loss: 2.0604817867279053 | Test loss: 1.2441701889038086\n",
            "Epoch 83540 | Loss: 2.059438467025757 | Test loss: 1.243331789970398\n",
            "Epoch 83550 | Loss: 2.0583951473236084 | Test loss: 1.2424935102462769\n",
            "Epoch 83560 | Loss: 2.05735182762146 | Test loss: 1.2416549921035767\n",
            "Epoch 83570 | Loss: 2.0563085079193115 | Test loss: 1.2408167123794556\n",
            "Epoch 83580 | Loss: 2.055265188217163 | Test loss: 1.2399784326553345\n",
            "Epoch 83590 | Loss: 2.0542216300964355 | Test loss: 1.2391399145126343\n",
            "Epoch 83600 | Loss: 2.053178071975708 | Test loss: 1.2383016347885132\n",
            "Epoch 83610 | Loss: 2.0521347522735596 | Test loss: 1.2374632358551025\n",
            "Epoch 83620 | Loss: 2.051091194152832 | Test loss: 1.2366249561309814\n",
            "Epoch 83630 | Loss: 2.0500478744506836 | Test loss: 1.2357866764068604\n",
            "Epoch 83640 | Loss: 2.049004554748535 | Test loss: 1.2349481582641602\n",
            "Epoch 83650 | Loss: 2.0479609966278076 | Test loss: 1.234109878540039\n",
            "Epoch 83660 | Loss: 2.046917676925659 | Test loss: 1.2332714796066284\n",
            "Epoch 83670 | Loss: 2.0458743572235107 | Test loss: 1.2324331998825073\n",
            "Epoch 83680 | Loss: 2.0448310375213623 | Test loss: 1.2315949201583862\n",
            "Epoch 83690 | Loss: 2.043787717819214 | Test loss: 1.2307565212249756\n",
            "Epoch 83700 | Loss: 2.0427441596984863 | Test loss: 1.229918122291565\n",
            "Epoch 83710 | Loss: 2.041700839996338 | Test loss: 1.2290798425674438\n",
            "Epoch 83720 | Loss: 2.0406575202941895 | Test loss: 1.2282413244247437\n",
            "Epoch 83730 | Loss: 2.039614200592041 | Test loss: 1.2274030447006226\n",
            "Epoch 83740 | Loss: 2.0385704040527344 | Test loss: 1.226564645767212\n",
            "Epoch 83750 | Loss: 2.037527084350586 | Test loss: 1.2257263660430908\n",
            "Epoch 83760 | Loss: 2.0364837646484375 | Test loss: 1.2248880863189697\n",
            "Epoch 83770 | Loss: 2.035440444946289 | Test loss: 1.224049687385559\n",
            "Epoch 83780 | Loss: 2.0343971252441406 | Test loss: 1.2232112884521484\n",
            "Epoch 83790 | Loss: 2.033353805541992 | Test loss: 1.2223730087280273\n",
            "Epoch 83800 | Loss: 2.0323102474212646 | Test loss: 1.2215346097946167\n",
            "Epoch 83810 | Loss: 2.031266927719116 | Test loss: 1.2206963300704956\n",
            "Epoch 83820 | Loss: 2.0302236080169678 | Test loss: 1.219857931137085\n",
            "Epoch 83830 | Loss: 2.0291802883148193 | Test loss: 1.2190195322036743\n",
            "Epoch 83840 | Loss: 2.028136968612671 | Test loss: 1.2181812524795532\n",
            "Epoch 83850 | Loss: 2.0270934104919434 | Test loss: 1.217342734336853\n",
            "Epoch 83860 | Loss: 2.026049852371216 | Test loss: 1.216504454612732\n",
            "Epoch 83870 | Loss: 2.0250065326690674 | Test loss: 1.2156661748886108\n",
            "Epoch 83880 | Loss: 2.023963212966919 | Test loss: 1.2148278951644897\n",
            "Epoch 83890 | Loss: 2.0229196548461914 | Test loss: 1.213989496231079\n",
            "Epoch 83900 | Loss: 2.021876335144043 | Test loss: 1.2131510972976685\n",
            "Epoch 83910 | Loss: 2.0208327770233154 | Test loss: 1.2123126983642578\n",
            "Epoch 83920 | Loss: 2.019789457321167 | Test loss: 1.2114742994308472\n",
            "Epoch 83930 | Loss: 2.0187461376190186 | Test loss: 1.2106361389160156\n",
            "Epoch 83940 | Loss: 2.01770281791687 | Test loss: 1.209797739982605\n",
            "Epoch 83950 | Loss: 2.0166594982147217 | Test loss: 1.2089594602584839\n",
            "Epoch 83960 | Loss: 2.0156161785125732 | Test loss: 1.2081209421157837\n",
            "Epoch 83970 | Loss: 2.0145726203918457 | Test loss: 1.2072826623916626\n",
            "Epoch 83980 | Loss: 2.0135293006896973 | Test loss: 1.2064443826675415\n",
            "Epoch 83990 | Loss: 2.012485980987549 | Test loss: 1.2056058645248413\n",
            "Epoch 84000 | Loss: 2.0114426612854004 | Test loss: 1.2047675848007202\n",
            "Epoch 84010 | Loss: 2.010399103164673 | Test loss: 1.2039291858673096\n",
            "Epoch 84020 | Loss: 2.0093555450439453 | Test loss: 1.2030909061431885\n",
            "Epoch 84030 | Loss: 2.008312225341797 | Test loss: 1.2022526264190674\n",
            "Epoch 84040 | Loss: 2.0072689056396484 | Test loss: 1.2014141082763672\n",
            "Epoch 84050 | Loss: 2.006225347518921 | Test loss: 1.200575828552246\n",
            "Epoch 84060 | Loss: 2.0051820278167725 | Test loss: 1.1997374296188354\n",
            "Epoch 84070 | Loss: 2.004138708114624 | Test loss: 1.1988991498947144\n",
            "Epoch 84080 | Loss: 2.0030953884124756 | Test loss: 1.1980608701705933\n",
            "Epoch 84090 | Loss: 2.002052068710327 | Test loss: 1.1972224712371826\n",
            "Epoch 84100 | Loss: 2.0010087490081787 | Test loss: 1.196384072303772\n",
            "Epoch 84110 | Loss: 1.9999650716781616 | Test loss: 1.1955457925796509\n",
            "Epoch 84120 | Loss: 1.9989217519760132 | Test loss: 1.1947072744369507\n",
            "Epoch 84130 | Loss: 1.9978783130645752 | Test loss: 1.1938689947128296\n",
            "Epoch 84140 | Loss: 1.9968349933624268 | Test loss: 1.193030595779419\n",
            "Epoch 84150 | Loss: 1.9957916736602783 | Test loss: 1.1921923160552979\n",
            "Epoch 84160 | Loss: 1.9947481155395508 | Test loss: 1.1913540363311768\n",
            "Epoch 84170 | Loss: 1.9937047958374023 | Test loss: 1.1905156373977661\n",
            "Epoch 84180 | Loss: 1.9926613569259644 | Test loss: 1.1896772384643555\n",
            "Epoch 84190 | Loss: 1.991618037223816 | Test loss: 1.1888389587402344\n",
            "Epoch 84200 | Loss: 1.9905747175216675 | Test loss: 1.1880005598068237\n",
            "Epoch 84210 | Loss: 1.989531397819519 | Test loss: 1.1871622800827026\n",
            "Epoch 84220 | Loss: 1.9884878396987915 | Test loss: 1.186323881149292\n",
            "Epoch 84230 | Loss: 1.987444281578064 | Test loss: 1.1854854822158813\n",
            "Epoch 84240 | Loss: 1.9864009618759155 | Test loss: 1.1846472024917603\n",
            "Epoch 84250 | Loss: 1.985357642173767 | Test loss: 1.18380868434906\n",
            "Epoch 84260 | Loss: 1.9843143224716187 | Test loss: 1.182970404624939\n",
            "Epoch 84270 | Loss: 1.9832708835601807 | Test loss: 1.1821321249008179\n",
            "Epoch 84280 | Loss: 1.9822275638580322 | Test loss: 1.1812938451766968\n",
            "Epoch 84290 | Loss: 1.9811840057373047 | Test loss: 1.1804554462432861\n",
            "Epoch 84300 | Loss: 1.9801406860351562 | Test loss: 1.1796170473098755\n",
            "Epoch 84310 | Loss: 1.9790972471237183 | Test loss: 1.1787786483764648\n",
            "Epoch 84320 | Loss: 1.9780540466308594 | Test loss: 1.1779402494430542\n",
            "Epoch 84330 | Loss: 1.9770106077194214 | Test loss: 1.1771020889282227\n",
            "Epoch 84340 | Loss: 1.9759670495986938 | Test loss: 1.176263689994812\n",
            "Epoch 84350 | Loss: 1.9749237298965454 | Test loss: 1.175425410270691\n",
            "Epoch 84360 | Loss: 1.973880410194397 | Test loss: 1.1745868921279907\n",
            "Epoch 84370 | Loss: 1.9728370904922485 | Test loss: 1.1737486124038696\n",
            "Epoch 84380 | Loss: 1.971793532371521 | Test loss: 1.1729103326797485\n",
            "Epoch 84390 | Loss: 1.970750093460083 | Test loss: 1.1720718145370483\n",
            "Epoch 84400 | Loss: 1.9697067737579346 | Test loss: 1.1712335348129272\n",
            "Epoch 84410 | Loss: 1.9686634540557861 | Test loss: 1.1703951358795166\n",
            "Epoch 84420 | Loss: 1.9676201343536377 | Test loss: 1.1695568561553955\n",
            "Epoch 84430 | Loss: 1.9665765762329102 | Test loss: 1.1687185764312744\n",
            "Epoch 84440 | Loss: 1.9655332565307617 | Test loss: 1.1678800582885742\n",
            "Epoch 84450 | Loss: 1.9644898176193237 | Test loss: 1.1670417785644531\n",
            "Epoch 84460 | Loss: 1.9634464979171753 | Test loss: 1.1662033796310425\n",
            "Epoch 84470 | Loss: 1.9624029397964478 | Test loss: 1.1653650999069214\n",
            "Epoch 84480 | Loss: 1.9613598585128784 | Test loss: 1.1645268201828003\n",
            "Epoch 84490 | Loss: 1.9603163003921509 | Test loss: 1.1636884212493896\n",
            "Epoch 84500 | Loss: 1.9592727422714233 | Test loss: 1.162850022315979\n",
            "Epoch 84510 | Loss: 1.958229422569275 | Test loss: 1.162011742591858\n",
            "Epoch 84520 | Loss: 1.9571861028671265 | Test loss: 1.1611732244491577\n",
            "Epoch 84530 | Loss: 1.956142783164978 | Test loss: 1.1603349447250366\n",
            "Epoch 84540 | Loss: 1.955099105834961 | Test loss: 1.159496545791626\n",
            "Epoch 84550 | Loss: 1.9540557861328125 | Test loss: 1.1586582660675049\n",
            "Epoch 84560 | Loss: 1.953012466430664 | Test loss: 1.1578199863433838\n",
            "Epoch 84570 | Loss: 1.9519691467285156 | Test loss: 1.1569815874099731\n",
            "Epoch 84580 | Loss: 1.9509257078170776 | Test loss: 1.1561431884765625\n",
            "Epoch 84590 | Loss: 1.9498823881149292 | Test loss: 1.1553049087524414\n",
            "Epoch 84600 | Loss: 1.9488390684127808 | Test loss: 1.1544665098190308\n",
            "Epoch 84610 | Loss: 1.9477957487106323 | Test loss: 1.1536282300949097\n",
            "Epoch 84620 | Loss: 1.9467521905899048 | Test loss: 1.152789831161499\n",
            "Epoch 84630 | Loss: 1.9457088708877563 | Test loss: 1.1519514322280884\n",
            "Epoch 84640 | Loss: 1.944665551185608 | Test loss: 1.1511131525039673\n",
            "Epoch 84650 | Loss: 1.9436218738555908 | Test loss: 1.150274634361267\n",
            "Epoch 84660 | Loss: 1.9425785541534424 | Test loss: 1.149436354637146\n",
            "Epoch 84670 | Loss: 1.941535234451294 | Test loss: 1.148598074913025\n",
            "Epoch 84680 | Loss: 1.9404919147491455 | Test loss: 1.1477597951889038\n",
            "Epoch 84690 | Loss: 1.939448595046997 | Test loss: 1.1469213962554932\n",
            "Epoch 84700 | Loss: 1.9384050369262695 | Test loss: 1.1460829973220825\n",
            "Epoch 84710 | Loss: 1.937361717224121 | Test loss: 1.1452445983886719\n",
            "Epoch 84720 | Loss: 1.936318278312683 | Test loss: 1.1444061994552612\n",
            "Epoch 84730 | Loss: 1.9352749586105347 | Test loss: 1.1435680389404297\n",
            "Epoch 84740 | Loss: 1.9342316389083862 | Test loss: 1.142729640007019\n",
            "Epoch 84750 | Loss: 1.9331880807876587 | Test loss: 1.141891360282898\n",
            "Epoch 84760 | Loss: 1.9321447610855103 | Test loss: 1.1410528421401978\n",
            "Epoch 84770 | Loss: 1.9311012029647827 | Test loss: 1.1402145624160767\n",
            "Epoch 84780 | Loss: 1.9300578832626343 | Test loss: 1.1393762826919556\n",
            "Epoch 84790 | Loss: 1.9290145635604858 | Test loss: 1.1385377645492554\n",
            "Epoch 84800 | Loss: 1.9279712438583374 | Test loss: 1.1376994848251343\n",
            "Epoch 84810 | Loss: 1.9269278049468994 | Test loss: 1.1368610858917236\n",
            "Epoch 84820 | Loss: 1.9258842468261719 | Test loss: 1.1360228061676025\n",
            "Epoch 84830 | Loss: 1.9248409271240234 | Test loss: 1.1351845264434814\n",
            "Epoch 84840 | Loss: 1.923797607421875 | Test loss: 1.1343460083007812\n",
            "Epoch 84850 | Loss: 1.922754168510437 | Test loss: 1.1335077285766602\n",
            "Epoch 84860 | Loss: 1.9217108488082886 | Test loss: 1.1326693296432495\n",
            "Epoch 84870 | Loss: 1.920667290687561 | Test loss: 1.1318310499191284\n",
            "Epoch 84880 | Loss: 1.9196242094039917 | Test loss: 1.1309927701950073\n",
            "Epoch 84890 | Loss: 1.9185806512832642 | Test loss: 1.1301543712615967\n",
            "Epoch 84900 | Loss: 1.9175373315811157 | Test loss: 1.129315972328186\n",
            "Epoch 84910 | Loss: 1.9164937734603882 | Test loss: 1.128477692604065\n",
            "Epoch 84920 | Loss: 1.9154503345489502 | Test loss: 1.1276391744613647\n",
            "Epoch 84930 | Loss: 1.9144070148468018 | Test loss: 1.1268008947372437\n",
            "Epoch 84940 | Loss: 1.9133636951446533 | Test loss: 1.125962495803833\n",
            "Epoch 84950 | Loss: 1.9123201370239258 | Test loss: 1.125124216079712\n",
            "Epoch 84960 | Loss: 1.9112768173217773 | Test loss: 1.1242859363555908\n",
            "Epoch 84970 | Loss: 1.910233497619629 | Test loss: 1.1234475374221802\n",
            "Epoch 84980 | Loss: 1.909190058708191 | Test loss: 1.1226091384887695\n",
            "Epoch 84990 | Loss: 1.9081467390060425 | Test loss: 1.1217708587646484\n",
            "Epoch 85000 | Loss: 1.907103419303894 | Test loss: 1.1209324598312378\n",
            "Epoch 85010 | Loss: 1.9060600996017456 | Test loss: 1.1200941801071167\n",
            "Epoch 85020 | Loss: 1.905016541481018 | Test loss: 1.119255781173706\n",
            "Epoch 85030 | Loss: 1.9039729833602905 | Test loss: 1.1184173822402954\n",
            "Epoch 85040 | Loss: 1.902929663658142 | Test loss: 1.1175791025161743\n",
            "Epoch 85050 | Loss: 1.9018863439559937 | Test loss: 1.1167405843734741\n",
            "Epoch 85060 | Loss: 1.9008429050445557 | Test loss: 1.115902304649353\n",
            "Epoch 85070 | Loss: 1.8997995853424072 | Test loss: 1.115064024925232\n",
            "Epoch 85080 | Loss: 1.8987562656402588 | Test loss: 1.1142257452011108\n",
            "Epoch 85090 | Loss: 1.8977127075195312 | Test loss: 1.1133873462677002\n",
            "Epoch 85100 | Loss: 1.8966693878173828 | Test loss: 1.1125489473342896\n",
            "Epoch 85110 | Loss: 1.8956259489059448 | Test loss: 1.111710548400879\n",
            "Epoch 85120 | Loss: 1.894582748413086 | Test loss: 1.1108721494674683\n",
            "Epoch 85130 | Loss: 1.893539309501648 | Test loss: 1.1100339889526367\n",
            "Epoch 85140 | Loss: 1.8924957513809204 | Test loss: 1.109195590019226\n",
            "Epoch 85150 | Loss: 1.891452431678772 | Test loss: 1.108357310295105\n",
            "Epoch 85160 | Loss: 1.8904091119766235 | Test loss: 1.1075187921524048\n",
            "Epoch 85170 | Loss: 1.889365792274475 | Test loss: 1.1066805124282837\n",
            "Epoch 85180 | Loss: 1.8883222341537476 | Test loss: 1.1058422327041626\n",
            "Epoch 85190 | Loss: 1.8872787952423096 | Test loss: 1.1050037145614624\n",
            "Epoch 85200 | Loss: 1.8862354755401611 | Test loss: 1.1041654348373413\n",
            "Epoch 85210 | Loss: 1.8851921558380127 | Test loss: 1.1033270359039307\n",
            "Epoch 85220 | Loss: 1.8841488361358643 | Test loss: 1.1024887561798096\n",
            "Epoch 85230 | Loss: 1.8831052780151367 | Test loss: 1.1016504764556885\n",
            "Epoch 85240 | Loss: 1.8820619583129883 | Test loss: 1.1008119583129883\n",
            "Epoch 85250 | Loss: 1.8810185194015503 | Test loss: 1.0999736785888672\n",
            "Epoch 85260 | Loss: 1.8799751996994019 | Test loss: 1.0991352796554565\n",
            "Epoch 85270 | Loss: 1.8789316415786743 | Test loss: 1.0982969999313354\n",
            "Epoch 85280 | Loss: 1.8778883218765259 | Test loss: 1.0974587202072144\n",
            "Epoch 85290 | Loss: 1.8768450021743774 | Test loss: 1.0966203212738037\n",
            "Epoch 85300 | Loss: 1.87580144405365 | Test loss: 1.095781922340393\n",
            "Epoch 85310 | Loss: 1.8747581243515015 | Test loss: 1.094943642616272\n",
            "Epoch 85320 | Loss: 1.873714804649353 | Test loss: 1.0941051244735718\n",
            "Epoch 85330 | Loss: 1.8726714849472046 | Test loss: 1.0932668447494507\n",
            "Epoch 85340 | Loss: 1.8716280460357666 | Test loss: 1.09242844581604\n",
            "Epoch 85350 | Loss: 1.870584487915039 | Test loss: 1.091590166091919\n",
            "Epoch 85360 | Loss: 1.8695411682128906 | Test loss: 1.0907518863677979\n",
            "Epoch 85370 | Loss: 1.8684978485107422 | Test loss: 1.0899134874343872\n",
            "Epoch 85380 | Loss: 1.8674544095993042 | Test loss: 1.0890750885009766\n",
            "Epoch 85390 | Loss: 1.8664110898971558 | Test loss: 1.0882368087768555\n",
            "Epoch 85400 | Loss: 1.8653677701950073 | Test loss: 1.0873984098434448\n",
            "Epoch 85410 | Loss: 1.8643244504928589 | Test loss: 1.0865601301193237\n",
            "Epoch 85420 | Loss: 1.8632808923721313 | Test loss: 1.085721731185913\n",
            "Epoch 85430 | Loss: 1.862237572669983 | Test loss: 1.0848833322525024\n",
            "Epoch 85440 | Loss: 1.8611942529678345 | Test loss: 1.0840450525283813\n",
            "Epoch 85450 | Loss: 1.860150694847107 | Test loss: 1.0832065343856812\n",
            "Epoch 85460 | Loss: 1.859107255935669 | Test loss: 1.08236825466156\n",
            "Epoch 85470 | Loss: 1.8580639362335205 | Test loss: 1.081529974937439\n",
            "Epoch 85480 | Loss: 1.857020616531372 | Test loss: 1.0806916952133179\n",
            "Epoch 85490 | Loss: 1.8559772968292236 | Test loss: 1.0798532962799072\n",
            "Epoch 85500 | Loss: 1.854933738708496 | Test loss: 1.0790148973464966\n",
            "Epoch 85510 | Loss: 1.8538904190063477 | Test loss: 1.078176498413086\n",
            "Epoch 85520 | Loss: 1.8528469800949097 | Test loss: 1.0773380994796753\n",
            "Epoch 85530 | Loss: 1.8518036603927612 | Test loss: 1.0764999389648438\n",
            "Epoch 85540 | Loss: 1.8507603406906128 | Test loss: 1.075661540031433\n",
            "Epoch 85550 | Loss: 1.8497170209884644 | Test loss: 1.074823260307312\n",
            "Epoch 85560 | Loss: 1.8486734628677368 | Test loss: 1.0739847421646118\n",
            "Epoch 85570 | Loss: 1.8476299047470093 | Test loss: 1.0731464624404907\n",
            "Epoch 85580 | Loss: 1.8465865850448608 | Test loss: 1.0723081827163696\n",
            "Epoch 85590 | Loss: 1.8455432653427124 | Test loss: 1.0714696645736694\n",
            "Epoch 85600 | Loss: 1.844499945640564 | Test loss: 1.0706313848495483\n",
            "Epoch 85610 | Loss: 1.843456506729126 | Test loss: 1.0697929859161377\n",
            "Epoch 85620 | Loss: 1.8424129486083984 | Test loss: 1.0689547061920166\n",
            "Epoch 85630 | Loss: 1.84136962890625 | Test loss: 1.0681164264678955\n",
            "Epoch 85640 | Loss: 1.8403263092041016 | Test loss: 1.0672779083251953\n",
            "Epoch 85650 | Loss: 1.8392829895019531 | Test loss: 1.0664396286010742\n",
            "Epoch 85660 | Loss: 1.8382395505905151 | Test loss: 1.0656012296676636\n",
            "Epoch 85670 | Loss: 1.8371959924697876 | Test loss: 1.0647629499435425\n",
            "Epoch 85680 | Loss: 1.8361526727676392 | Test loss: 1.0639246702194214\n",
            "Epoch 85690 | Loss: 1.8351093530654907 | Test loss: 1.0630862712860107\n",
            "Epoch 85700 | Loss: 1.8340660333633423 | Test loss: 1.0622478723526\n",
            "Epoch 85710 | Loss: 1.8330224752426147 | Test loss: 1.061409592628479\n",
            "Epoch 85720 | Loss: 1.8319790363311768 | Test loss: 1.0605710744857788\n",
            "Epoch 85730 | Loss: 1.8309357166290283 | Test loss: 1.0597327947616577\n",
            "Epoch 85740 | Loss: 1.8298923969268799 | Test loss: 1.058894395828247\n",
            "Epoch 85750 | Loss: 1.8288490772247314 | Test loss: 1.058056116104126\n",
            "Epoch 85760 | Loss: 1.827805519104004 | Test loss: 1.0572178363800049\n",
            "Epoch 85770 | Loss: 1.8267621994018555 | Test loss: 1.0563794374465942\n",
            "Epoch 85780 | Loss: 1.8257187604904175 | Test loss: 1.0555410385131836\n",
            "Epoch 85790 | Loss: 1.824675440788269 | Test loss: 1.0547027587890625\n",
            "Epoch 85800 | Loss: 1.8236321210861206 | Test loss: 1.0538643598556519\n",
            "Epoch 85810 | Loss: 1.8225888013839722 | Test loss: 1.0530260801315308\n",
            "Epoch 85820 | Loss: 1.8215452432632446 | Test loss: 1.0521876811981201\n",
            "Epoch 85830 | Loss: 1.820501685142517 | Test loss: 1.0513492822647095\n",
            "Epoch 85840 | Loss: 1.8194583654403687 | Test loss: 1.0505110025405884\n",
            "Epoch 85850 | Loss: 1.8184150457382202 | Test loss: 1.0496724843978882\n",
            "Epoch 85860 | Loss: 1.8173716068267822 | Test loss: 1.048834204673767\n",
            "Epoch 85870 | Loss: 1.8163282871246338 | Test loss: 1.047995924949646\n",
            "Epoch 85880 | Loss: 1.8152849674224854 | Test loss: 1.047157645225525\n",
            "Epoch 85890 | Loss: 1.8142414093017578 | Test loss: 1.0463192462921143\n",
            "Epoch 85900 | Loss: 1.8131980895996094 | Test loss: 1.0454808473587036\n",
            "Epoch 85910 | Loss: 1.812154769897461 | Test loss: 1.044642448425293\n",
            "Epoch 85920 | Loss: 1.8111114501953125 | Test loss: 1.0438040494918823\n",
            "Epoch 85930 | Loss: 1.8100680112838745 | Test loss: 1.0429658889770508\n",
            "Epoch 85940 | Loss: 1.809024453163147 | Test loss: 1.0421274900436401\n",
            "Epoch 85950 | Loss: 1.8079811334609985 | Test loss: 1.041289210319519\n",
            "Epoch 85960 | Loss: 1.80693781375885 | Test loss: 1.0404506921768188\n",
            "Epoch 85970 | Loss: 1.8058944940567017 | Test loss: 1.0396124124526978\n",
            "Epoch 85980 | Loss: 1.8048509359359741 | Test loss: 1.0387741327285767\n",
            "Epoch 85990 | Loss: 1.8038074970245361 | Test loss: 1.0379356145858765\n",
            "Epoch 86000 | Loss: 1.8027641773223877 | Test loss: 1.0370973348617554\n",
            "Epoch 86010 | Loss: 1.8017208576202393 | Test loss: 1.0362589359283447\n",
            "Epoch 86020 | Loss: 1.8006775379180908 | Test loss: 1.0354206562042236\n",
            "Epoch 86030 | Loss: 1.7996339797973633 | Test loss: 1.0345823764801025\n",
            "Epoch 86040 | Loss: 1.7985906600952148 | Test loss: 1.0337438583374023\n",
            "Epoch 86050 | Loss: 1.7975472211837769 | Test loss: 1.0329055786132812\n",
            "Epoch 86060 | Loss: 1.7965039014816284 | Test loss: 1.0320671796798706\n",
            "Epoch 86070 | Loss: 1.7954603433609009 | Test loss: 1.0312288999557495\n",
            "Epoch 86080 | Loss: 1.7944170236587524 | Test loss: 1.0303906202316284\n",
            "Epoch 86090 | Loss: 1.793373703956604 | Test loss: 1.0295522212982178\n",
            "Epoch 86100 | Loss: 1.7923301458358765 | Test loss: 1.0287138223648071\n",
            "Epoch 86110 | Loss: 1.791286826133728 | Test loss: 1.027875542640686\n",
            "Epoch 86120 | Loss: 1.7902435064315796 | Test loss: 1.0270370244979858\n",
            "Epoch 86130 | Loss: 1.7892001867294312 | Test loss: 1.0261987447738647\n",
            "Epoch 86140 | Loss: 1.7881567478179932 | Test loss: 1.025360345840454\n",
            "Epoch 86150 | Loss: 1.7871131896972656 | Test loss: 1.024522066116333\n",
            "Epoch 86160 | Loss: 1.7860698699951172 | Test loss: 1.023683786392212\n",
            "Epoch 86170 | Loss: 1.7850265502929688 | Test loss: 1.0228453874588013\n",
            "Epoch 86180 | Loss: 1.7839831113815308 | Test loss: 1.0220069885253906\n",
            "Epoch 86190 | Loss: 1.7829397916793823 | Test loss: 1.0211687088012695\n",
            "Epoch 86200 | Loss: 1.7818964719772339 | Test loss: 1.0203303098678589\n",
            "Epoch 86210 | Loss: 1.7808531522750854 | Test loss: 1.0194920301437378\n",
            "Epoch 86220 | Loss: 1.779809594154358 | Test loss: 1.0186536312103271\n",
            "Epoch 86230 | Loss: 1.7787662744522095 | Test loss: 1.0178152322769165\n",
            "Epoch 86240 | Loss: 1.777722954750061 | Test loss: 1.0169769525527954\n",
            "Epoch 86250 | Loss: 1.7766793966293335 | Test loss: 1.0161384344100952\n",
            "Epoch 86260 | Loss: 1.7756359577178955 | Test loss: 1.0153001546859741\n",
            "Epoch 86270 | Loss: 1.774592638015747 | Test loss: 1.014461874961853\n",
            "Epoch 86280 | Loss: 1.7735493183135986 | Test loss: 1.013623595237732\n",
            "Epoch 86290 | Loss: 1.7725059986114502 | Test loss: 1.0127851963043213\n",
            "Epoch 86300 | Loss: 1.7714624404907227 | Test loss: 1.0119467973709106\n",
            "Epoch 86310 | Loss: 1.7704191207885742 | Test loss: 1.0111083984375\n",
            "Epoch 86320 | Loss: 1.7693756818771362 | Test loss: 1.0102699995040894\n",
            "Epoch 86330 | Loss: 1.7683323621749878 | Test loss: 1.0094318389892578\n",
            "Epoch 86340 | Loss: 1.7672890424728394 | Test loss: 1.0085934400558472\n",
            "Epoch 86350 | Loss: 1.766245722770691 | Test loss: 1.007755160331726\n",
            "Epoch 86360 | Loss: 1.7652021646499634 | Test loss: 1.0069166421890259\n",
            "Epoch 86370 | Loss: 1.7641586065292358 | Test loss: 1.0060783624649048\n",
            "Epoch 86380 | Loss: 1.7631152868270874 | Test loss: 1.0052400827407837\n",
            "Epoch 86390 | Loss: 1.762071967124939 | Test loss: 1.0044015645980835\n",
            "Epoch 86400 | Loss: 1.7610286474227905 | Test loss: 1.0035632848739624\n",
            "Epoch 86410 | Loss: 1.7599852085113525 | Test loss: 1.0027248859405518\n",
            "Epoch 86420 | Loss: 1.758941650390625 | Test loss: 1.0018866062164307\n",
            "Epoch 86430 | Loss: 1.7578983306884766 | Test loss: 1.0010483264923096\n",
            "Epoch 86440 | Loss: 1.7568550109863281 | Test loss: 1.0002098083496094\n",
            "Epoch 86450 | Loss: 1.7558116912841797 | Test loss: 0.9993715286254883\n",
            "Epoch 86460 | Loss: 1.7547682523727417 | Test loss: 0.9985330700874329\n",
            "Epoch 86470 | Loss: 1.7537246942520142 | Test loss: 0.9976947903633118\n",
            "Epoch 86480 | Loss: 1.7526813745498657 | Test loss: 0.9968565106391907\n",
            "Epoch 86490 | Loss: 1.7516380548477173 | Test loss: 0.99601811170578\n",
            "Epoch 86500 | Loss: 1.7505947351455688 | Test loss: 0.9951797723770142\n",
            "Epoch 86510 | Loss: 1.7495511770248413 | Test loss: 0.9943413734436035\n",
            "Epoch 86520 | Loss: 1.7485077381134033 | Test loss: 0.9935030341148376\n",
            "Epoch 86530 | Loss: 1.7474644184112549 | Test loss: 0.9926647543907166\n",
            "Epoch 86540 | Loss: 1.7464210987091064 | Test loss: 0.9918263554573059\n",
            "Epoch 86550 | Loss: 1.745377779006958 | Test loss: 0.9909879565238953\n",
            "Epoch 86560 | Loss: 1.7443342208862305 | Test loss: 0.9901496767997742\n",
            "Epoch 86570 | Loss: 1.743290901184082 | Test loss: 0.9893113374710083\n",
            "Epoch 86580 | Loss: 1.742247462272644 | Test loss: 0.9884729385375977\n",
            "Epoch 86590 | Loss: 1.7412041425704956 | Test loss: 0.9876346588134766\n",
            "Epoch 86600 | Loss: 1.7401608228683472 | Test loss: 0.9867962002754211\n",
            "Epoch 86610 | Loss: 1.7391175031661987 | Test loss: 0.9859579205513\n",
            "Epoch 86620 | Loss: 1.7380739450454712 | Test loss: 0.9851195216178894\n",
            "Epoch 86630 | Loss: 1.7370303869247437 | Test loss: 0.9842811822891235\n",
            "Epoch 86640 | Loss: 1.7359870672225952 | Test loss: 0.9834429025650024\n",
            "Epoch 86650 | Loss: 1.7349437475204468 | Test loss: 0.982604444026947\n",
            "Epoch 86660 | Loss: 1.7339003086090088 | Test loss: 0.9817661643028259\n",
            "Epoch 86670 | Loss: 1.7328569889068604 | Test loss: 0.9809278845787048\n",
            "Epoch 86680 | Loss: 1.731813669204712 | Test loss: 0.9800894856452942\n",
            "Epoch 86690 | Loss: 1.7307701110839844 | Test loss: 0.9792510867118835\n",
            "Epoch 86700 | Loss: 1.729726791381836 | Test loss: 0.9784127473831177\n",
            "Epoch 86710 | Loss: 1.7286834716796875 | Test loss: 0.977574348449707\n",
            "Epoch 86720 | Loss: 1.727640151977539 | Test loss: 0.9767360091209412\n",
            "Epoch 86730 | Loss: 1.726596713066101 | Test loss: 0.9758977293968201\n",
            "Epoch 86740 | Loss: 1.7255531549453735 | Test loss: 0.9750593304634094\n",
            "Epoch 86750 | Loss: 1.724509835243225 | Test loss: 0.9742210507392883\n",
            "Epoch 86760 | Loss: 1.7234665155410767 | Test loss: 0.9733825922012329\n",
            "Epoch 86770 | Loss: 1.7224231958389282 | Test loss: 0.9725443124771118\n",
            "Epoch 86780 | Loss: 1.7213796377182007 | Test loss: 0.9717059135437012\n",
            "Epoch 86790 | Loss: 1.7203363180160522 | Test loss: 0.9708675742149353\n",
            "Epoch 86800 | Loss: 1.7192928791046143 | Test loss: 0.9700291752815247\n",
            "Epoch 86810 | Loss: 1.7182495594024658 | Test loss: 0.9691908955574036\n",
            "Epoch 86820 | Loss: 1.7172062397003174 | Test loss: 0.9683524966239929\n",
            "Epoch 86830 | Loss: 1.7161626815795898 | Test loss: 0.9675142168998718\n",
            "Epoch 86840 | Loss: 1.7151193618774414 | Test loss: 0.9666757583618164\n",
            "Epoch 86850 | Loss: 1.7140759229660034 | Test loss: 0.9658374786376953\n",
            "Epoch 86860 | Loss: 1.713032603263855 | Test loss: 0.9649990200996399\n",
            "Epoch 86870 | Loss: 1.7119892835617065 | Test loss: 0.9641607403755188\n",
            "Epoch 86880 | Loss: 1.710945725440979 | Test loss: 0.9633224606513977\n",
            "Epoch 86890 | Loss: 1.7099024057388306 | Test loss: 0.9624840617179871\n",
            "Epoch 86900 | Loss: 1.708858847618103 | Test loss: 0.9616457223892212\n",
            "Epoch 86910 | Loss: 1.7078155279159546 | Test loss: 0.9608073234558105\n",
            "Epoch 86920 | Loss: 1.7067722082138062 | Test loss: 0.9599689841270447\n",
            "Epoch 86930 | Loss: 1.7057288885116577 | Test loss: 0.9591307044029236\n",
            "Epoch 86940 | Loss: 1.7046854496002197 | Test loss: 0.9582923054695129\n",
            "Epoch 86950 | Loss: 1.7036418914794922 | Test loss: 0.9574539065361023\n",
            "Epoch 86960 | Loss: 1.7025985717773438 | Test loss: 0.9566156268119812\n",
            "Epoch 86970 | Loss: 1.7015552520751953 | Test loss: 0.9557772874832153\n",
            "Epoch 86980 | Loss: 1.7005118131637573 | Test loss: 0.9549388885498047\n",
            "Epoch 86990 | Loss: 1.6994684934616089 | Test loss: 0.9541006088256836\n",
            "Epoch 87000 | Loss: 1.6984251737594604 | Test loss: 0.9532621502876282\n",
            "Epoch 87010 | Loss: 1.697381854057312 | Test loss: 0.9524238705635071\n",
            "Epoch 87020 | Loss: 1.6963382959365845 | Test loss: 0.9515854716300964\n",
            "Epoch 87030 | Loss: 1.695294976234436 | Test loss: 0.9507471323013306\n",
            "Epoch 87040 | Loss: 1.6942514181137085 | Test loss: 0.9499088525772095\n",
            "Epoch 87050 | Loss: 1.69320809841156 | Test loss: 0.949070394039154\n",
            "Epoch 87060 | Loss: 1.692164659500122 | Test loss: 0.948232114315033\n",
            "Epoch 87070 | Loss: 1.6911213397979736 | Test loss: 0.9473938345909119\n",
            "Epoch 87080 | Loss: 1.6900780200958252 | Test loss: 0.9465554356575012\n",
            "Epoch 87090 | Loss: 1.6890347003936768 | Test loss: 0.9457170367240906\n",
            "Epoch 87100 | Loss: 1.6879911422729492 | Test loss: 0.9448786973953247\n",
            "Epoch 87110 | Loss: 1.6869478225708008 | Test loss: 0.9440402984619141\n",
            "Epoch 87120 | Loss: 1.6859043836593628 | Test loss: 0.9432019591331482\n",
            "Epoch 87130 | Loss: 1.6848610639572144 | Test loss: 0.9423636794090271\n",
            "Epoch 87140 | Loss: 1.683817744255066 | Test loss: 0.9415252804756165\n",
            "Epoch 87150 | Loss: 1.6827744245529175 | Test loss: 0.9406870007514954\n",
            "Epoch 87160 | Loss: 1.68173086643219 | Test loss: 0.9398485422134399\n",
            "Epoch 87170 | Loss: 1.6806873083114624 | Test loss: 0.9390102624893188\n",
            "Epoch 87180 | Loss: 1.679643988609314 | Test loss: 0.9381718635559082\n",
            "Epoch 87190 | Loss: 1.6786006689071655 | Test loss: 0.9373335242271423\n",
            "Epoch 87200 | Loss: 1.677557349205017 | Test loss: 0.9364951252937317\n",
            "Epoch 87210 | Loss: 1.676513910293579 | Test loss: 0.9356568455696106\n",
            "Epoch 87220 | Loss: 1.6754703521728516 | Test loss: 0.9348184466362\n",
            "Epoch 87230 | Loss: 1.6744270324707031 | Test loss: 0.9339801669120789\n",
            "Epoch 87240 | Loss: 1.6733837127685547 | Test loss: 0.9331417083740234\n",
            "Epoch 87250 | Loss: 1.6723403930664062 | Test loss: 0.9323034286499023\n",
            "Epoch 87260 | Loss: 1.6712969541549683 | Test loss: 0.9314649701118469\n",
            "Epoch 87270 | Loss: 1.6702533960342407 | Test loss: 0.9306266903877258\n",
            "Epoch 87280 | Loss: 1.6692100763320923 | Test loss: 0.9297884106636047\n",
            "Epoch 87290 | Loss: 1.6681667566299438 | Test loss: 0.9289500117301941\n",
            "Epoch 87300 | Loss: 1.6671234369277954 | Test loss: 0.9281116724014282\n",
            "Epoch 87310 | Loss: 1.6660798788070679 | Test loss: 0.9272732734680176\n",
            "Epoch 87320 | Loss: 1.6650364398956299 | Test loss: 0.9264349341392517\n",
            "Epoch 87330 | Loss: 1.6639931201934814 | Test loss: 0.9255966544151306\n",
            "Epoch 87340 | Loss: 1.662949800491333 | Test loss: 0.92475825548172\n",
            "Epoch 87350 | Loss: 1.6619064807891846 | Test loss: 0.9239198565483093\n",
            "Epoch 87360 | Loss: 1.660862922668457 | Test loss: 0.9230815768241882\n",
            "Epoch 87370 | Loss: 1.6598196029663086 | Test loss: 0.9222432374954224\n",
            "Epoch 87380 | Loss: 1.6587761640548706 | Test loss: 0.9214048385620117\n",
            "Epoch 87390 | Loss: 1.6577328443527222 | Test loss: 0.9205665588378906\n",
            "Epoch 87400 | Loss: 1.6566895246505737 | Test loss: 0.9197281002998352\n",
            "Epoch 87410 | Loss: 1.6556462049484253 | Test loss: 0.9188898205757141\n",
            "Epoch 87420 | Loss: 1.6546026468276978 | Test loss: 0.9180514216423035\n",
            "Epoch 87430 | Loss: 1.6535590887069702 | Test loss: 0.9172130823135376\n",
            "Epoch 87440 | Loss: 1.6525157690048218 | Test loss: 0.9163748025894165\n",
            "Epoch 87450 | Loss: 1.6514724493026733 | Test loss: 0.9155363440513611\n",
            "Epoch 87460 | Loss: 1.6504290103912354 | Test loss: 0.91469806432724\n",
            "Epoch 87470 | Loss: 1.649385690689087 | Test loss: 0.9138597846031189\n",
            "Epoch 87480 | Loss: 1.6483423709869385 | Test loss: 0.9130212664604187\n",
            "Epoch 87490 | Loss: 1.647298812866211 | Test loss: 0.9121829867362976\n",
            "Epoch 87500 | Loss: 1.6462554931640625 | Test loss: 0.9113446474075317\n",
            "Epoch 87510 | Loss: 1.645212173461914 | Test loss: 0.9105062484741211\n",
            "Epoch 87520 | Loss: 1.6441688537597656 | Test loss: 0.9096679091453552\n",
            "Epoch 87530 | Loss: 1.6431254148483276 | Test loss: 0.9088296294212341\n",
            "Epoch 87540 | Loss: 1.6420818567276 | Test loss: 0.9079912304878235\n",
            "Epoch 87550 | Loss: 1.6410385370254517 | Test loss: 0.9071529507637024\n",
            "Epoch 87560 | Loss: 1.6399952173233032 | Test loss: 0.906314492225647\n",
            "Epoch 87570 | Loss: 1.6389518976211548 | Test loss: 0.9054762125015259\n",
            "Epoch 87580 | Loss: 1.6379083395004272 | Test loss: 0.9046378135681152\n",
            "Epoch 87590 | Loss: 1.6368650197982788 | Test loss: 0.9037994742393494\n",
            "Epoch 87600 | Loss: 1.6358215808868408 | Test loss: 0.9029610753059387\n",
            "Epoch 87610 | Loss: 1.6347782611846924 | Test loss: 0.9021227955818176\n",
            "Epoch 87620 | Loss: 1.633734941482544 | Test loss: 0.901284396648407\n",
            "Epoch 87630 | Loss: 1.6326913833618164 | Test loss: 0.9004461169242859\n",
            "Epoch 87640 | Loss: 1.631648063659668 | Test loss: 0.8996076583862305\n",
            "Epoch 87650 | Loss: 1.63060462474823 | Test loss: 0.8987693786621094\n",
            "Epoch 87660 | Loss: 1.6295613050460815 | Test loss: 0.897930920124054\n",
            "Epoch 87670 | Loss: 1.628517985343933 | Test loss: 0.8970926403999329\n",
            "Epoch 87680 | Loss: 1.6274744272232056 | Test loss: 0.8962543606758118\n",
            "Epoch 87690 | Loss: 1.6264311075210571 | Test loss: 0.8954159617424011\n",
            "Epoch 87700 | Loss: 1.6253875494003296 | Test loss: 0.8945776224136353\n",
            "Epoch 87710 | Loss: 1.6243442296981812 | Test loss: 0.8937392234802246\n",
            "Epoch 87720 | Loss: 1.6233009099960327 | Test loss: 0.8929008841514587\n",
            "Epoch 87730 | Loss: 1.6222575902938843 | Test loss: 0.8920626044273376\n",
            "Epoch 87740 | Loss: 1.6212141513824463 | Test loss: 0.891224205493927\n",
            "Epoch 87750 | Loss: 1.6201705932617188 | Test loss: 0.8903858065605164\n",
            "Epoch 87760 | Loss: 1.6191272735595703 | Test loss: 0.8895475268363953\n",
            "Epoch 87770 | Loss: 1.6180839538574219 | Test loss: 0.8887091875076294\n",
            "Epoch 87780 | Loss: 1.6170405149459839 | Test loss: 0.8878707885742188\n",
            "Epoch 87790 | Loss: 1.6159971952438354 | Test loss: 0.8870325088500977\n",
            "Epoch 87800 | Loss: 1.614953637123108 | Test loss: 0.8861940503120422\n",
            "Epoch 87810 | Loss: 1.6139105558395386 | Test loss: 0.8853557705879211\n",
            "Epoch 87820 | Loss: 1.612866997718811 | Test loss: 0.8845173716545105\n",
            "Epoch 87830 | Loss: 1.6118236780166626 | Test loss: 0.8836790323257446\n",
            "Epoch 87840 | Loss: 1.610780119895935 | Test loss: 0.8828407526016235\n",
            "Epoch 87850 | Loss: 1.6097368001937866 | Test loss: 0.8820022940635681\n",
            "Epoch 87860 | Loss: 1.6086933612823486 | Test loss: 0.881164014339447\n",
            "Epoch 87870 | Loss: 1.6076500415802002 | Test loss: 0.8803257346153259\n",
            "Epoch 87880 | Loss: 1.6066067218780518 | Test loss: 0.8794872164726257\n",
            "Epoch 87890 | Loss: 1.6055634021759033 | Test loss: 0.8786489367485046\n",
            "Epoch 87900 | Loss: 1.6045198440551758 | Test loss: 0.8778105974197388\n",
            "Epoch 87910 | Loss: 1.6034765243530273 | Test loss: 0.8769721984863281\n",
            "Epoch 87920 | Loss: 1.6024330854415894 | Test loss: 0.8761338591575623\n",
            "Epoch 87930 | Loss: 1.601389765739441 | Test loss: 0.8752955794334412\n",
            "Epoch 87940 | Loss: 1.6003464460372925 | Test loss: 0.8744571805000305\n",
            "Epoch 87950 | Loss: 1.5993030071258545 | Test loss: 0.8736189007759094\n",
            "Epoch 87960 | Loss: 1.5982595682144165 | Test loss: 0.872780442237854\n",
            "Epoch 87970 | Loss: 1.5972161293029785 | Test loss: 0.8719421625137329\n",
            "Epoch 87980 | Loss: 1.5961726903915405 | Test loss: 0.8711037635803223\n",
            "Epoch 87990 | Loss: 1.595129370689392 | Test loss: 0.8702654242515564\n",
            "Epoch 88000 | Loss: 1.5940860509872437 | Test loss: 0.8694270253181458\n",
            "Epoch 88010 | Loss: 1.5930426120758057 | Test loss: 0.8685887455940247\n",
            "Epoch 88020 | Loss: 1.5919991731643677 | Test loss: 0.867750346660614\n",
            "Epoch 88030 | Loss: 1.5909557342529297 | Test loss: 0.8669120669364929\n",
            "Epoch 88040 | Loss: 1.5899124145507812 | Test loss: 0.8660736083984375\n",
            "Epoch 88050 | Loss: 1.5888690948486328 | Test loss: 0.8652353286743164\n",
            "Epoch 88060 | Loss: 1.5878256559371948 | Test loss: 0.864396870136261\n",
            "Epoch 88070 | Loss: 1.5867822170257568 | Test loss: 0.8635585904121399\n",
            "Epoch 88080 | Loss: 1.5857387781143188 | Test loss: 0.8627203106880188\n",
            "Epoch 88090 | Loss: 1.58469557762146 | Test loss: 0.8618819117546082\n",
            "Epoch 88100 | Loss: 1.5836520195007324 | Test loss: 0.8610435724258423\n",
            "Epoch 88110 | Loss: 1.5826085805892944 | Test loss: 0.8602051734924316\n",
            "Epoch 88120 | Loss: 1.581565260887146 | Test loss: 0.8593668341636658\n",
            "Epoch 88130 | Loss: 1.5805219411849976 | Test loss: 0.8585285544395447\n",
            "Epoch 88140 | Loss: 1.5794785022735596 | Test loss: 0.857690155506134\n",
            "Epoch 88150 | Loss: 1.5784350633621216 | Test loss: 0.8568517565727234\n",
            "Epoch 88160 | Loss: 1.5773916244506836 | Test loss: 0.8560134768486023\n",
            "Epoch 88170 | Loss: 1.5763483047485352 | Test loss: 0.8551751375198364\n",
            "Epoch 88180 | Loss: 1.5753048658370972 | Test loss: 0.8543367385864258\n",
            "Epoch 88190 | Loss: 1.5742615461349487 | Test loss: 0.8534984588623047\n",
            "Epoch 88200 | Loss: 1.5732181072235107 | Test loss: 0.8526600003242493\n",
            "Epoch 88210 | Loss: 1.5721749067306519 | Test loss: 0.8518217206001282\n",
            "Epoch 88220 | Loss: 1.5711313486099243 | Test loss: 0.8509833216667175\n",
            "Epoch 88230 | Loss: 1.5700879096984863 | Test loss: 0.8501449823379517\n",
            "Epoch 88240 | Loss: 1.5690444707870483 | Test loss: 0.8493067026138306\n",
            "Epoch 88250 | Loss: 1.5680012702941895 | Test loss: 0.8484682440757751\n",
            "Epoch 88260 | Loss: 1.566957712173462 | Test loss: 0.847629964351654\n",
            "Epoch 88270 | Loss: 1.5659143924713135 | Test loss: 0.846791684627533\n",
            "Epoch 88280 | Loss: 1.564871072769165 | Test loss: 0.8459531664848328\n",
            "Epoch 88290 | Loss: 1.563827633857727 | Test loss: 0.8451148867607117\n",
            "Epoch 88300 | Loss: 1.562784194946289 | Test loss: 0.8442765474319458\n",
            "Epoch 88310 | Loss: 1.5617408752441406 | Test loss: 0.8434381484985352\n",
            "Epoch 88320 | Loss: 1.5606974363327026 | Test loss: 0.8425998091697693\n",
            "Epoch 88330 | Loss: 1.5596541166305542 | Test loss: 0.8417615294456482\n",
            "Epoch 88340 | Loss: 1.5586106777191162 | Test loss: 0.8409231305122375\n",
            "Epoch 88350 | Loss: 1.5575673580169678 | Test loss: 0.8400848507881165\n",
            "Epoch 88360 | Loss: 1.5565239191055298 | Test loss: 0.839246392250061\n",
            "Epoch 88370 | Loss: 1.5554804801940918 | Test loss: 0.8384081125259399\n",
            "Epoch 88380 | Loss: 1.5544370412826538 | Test loss: 0.8375697135925293\n",
            "Epoch 88390 | Loss: 1.5533937215805054 | Test loss: 0.8367313742637634\n",
            "Epoch 88400 | Loss: 1.552350401878357 | Test loss: 0.8358929753303528\n",
            "Epoch 88410 | Loss: 1.551306962966919 | Test loss: 0.8350546956062317\n",
            "Epoch 88420 | Loss: 1.550263524055481 | Test loss: 0.834216296672821\n",
            "Epoch 88430 | Loss: 1.549220085144043 | Test loss: 0.8333780169487\n",
            "Epoch 88440 | Loss: 1.5481767654418945 | Test loss: 0.8325395584106445\n",
            "Epoch 88450 | Loss: 1.547133445739746 | Test loss: 0.8317012786865234\n",
            "Epoch 88460 | Loss: 1.546090006828308 | Test loss: 0.830862820148468\n",
            "Epoch 88470 | Loss: 1.5450465679168701 | Test loss: 0.8300245404243469\n",
            "Epoch 88480 | Loss: 1.5440031290054321 | Test loss: 0.8291862607002258\n",
            "Epoch 88490 | Loss: 1.5429599285125732 | Test loss: 0.8283478617668152\n",
            "Epoch 88500 | Loss: 1.5419163703918457 | Test loss: 0.8275095224380493\n",
            "Epoch 88510 | Loss: 1.5408729314804077 | Test loss: 0.8266711235046387\n",
            "Epoch 88520 | Loss: 1.5398296117782593 | Test loss: 0.8258327841758728\n",
            "Epoch 88530 | Loss: 1.5387862920761108 | Test loss: 0.8249945044517517\n",
            "Epoch 88540 | Loss: 1.5377428531646729 | Test loss: 0.8241561055183411\n",
            "Epoch 88550 | Loss: 1.5366994142532349 | Test loss: 0.8233177065849304\n",
            "Epoch 88560 | Loss: 1.5356559753417969 | Test loss: 0.8224794268608093\n",
            "Epoch 88570 | Loss: 1.5346126556396484 | Test loss: 0.8216410875320435\n",
            "Epoch 88580 | Loss: 1.5335692167282104 | Test loss: 0.8208026885986328\n",
            "Epoch 88590 | Loss: 1.532525897026062 | Test loss: 0.8199644088745117\n",
            "Epoch 88600 | Loss: 1.531482458114624 | Test loss: 0.8191259503364563\n",
            "Epoch 88610 | Loss: 1.5304392576217651 | Test loss: 0.8182876706123352\n",
            "Epoch 88620 | Loss: 1.5293956995010376 | Test loss: 0.8174492716789246\n",
            "Epoch 88630 | Loss: 1.5283522605895996 | Test loss: 0.8166109323501587\n",
            "Epoch 88640 | Loss: 1.5273088216781616 | Test loss: 0.8157726526260376\n",
            "Epoch 88650 | Loss: 1.5262655019760132 | Test loss: 0.8149341940879822\n",
            "Epoch 88660 | Loss: 1.5252220630645752 | Test loss: 0.8140959143638611\n",
            "Epoch 88670 | Loss: 1.5241787433624268 | Test loss: 0.81325763463974\n",
            "Epoch 88680 | Loss: 1.5231354236602783 | Test loss: 0.8124191164970398\n",
            "Epoch 88690 | Loss: 1.5220919847488403 | Test loss: 0.8115808367729187\n",
            "Epoch 88700 | Loss: 1.5210485458374023 | Test loss: 0.8107424974441528\n",
            "Epoch 88710 | Loss: 1.520005226135254 | Test loss: 0.8099040985107422\n",
            "Epoch 88720 | Loss: 1.518961787223816 | Test loss: 0.8090657591819763\n",
            "Epoch 88730 | Loss: 1.5179184675216675 | Test loss: 0.8082274794578552\n",
            "Epoch 88740 | Loss: 1.5168750286102295 | Test loss: 0.8073890805244446\n",
            "Epoch 88750 | Loss: 1.515831708908081 | Test loss: 0.8065508008003235\n",
            "Epoch 88760 | Loss: 1.514788269996643 | Test loss: 0.8057123422622681\n",
            "Epoch 88770 | Loss: 1.513744831085205 | Test loss: 0.804874062538147\n",
            "Epoch 88780 | Loss: 1.512701392173767 | Test loss: 0.8040356636047363\n",
            "Epoch 88790 | Loss: 1.5116580724716187 | Test loss: 0.8031973242759705\n",
            "Epoch 88800 | Loss: 1.5106147527694702 | Test loss: 0.8023589253425598\n",
            "Epoch 88810 | Loss: 1.5095713138580322 | Test loss: 0.8015206456184387\n",
            "Epoch 88820 | Loss: 1.5085278749465942 | Test loss: 0.8006822466850281\n",
            "Epoch 88830 | Loss: 1.5074844360351562 | Test loss: 0.799843966960907\n",
            "Epoch 88840 | Loss: 1.5064411163330078 | Test loss: 0.7990055680274963\n",
            "Epoch 88850 | Loss: 1.5053977966308594 | Test loss: 0.7981671690940857\n",
            "Epoch 88860 | Loss: 1.5043543577194214 | Test loss: 0.7973288297653198\n",
            "Epoch 88870 | Loss: 1.5033109188079834 | Test loss: 0.7964905500411987\n",
            "Epoch 88880 | Loss: 1.5022674798965454 | Test loss: 0.7956521511077881\n",
            "Epoch 88890 | Loss: 1.501224160194397 | Test loss: 0.7948138117790222\n",
            "Epoch 88900 | Loss: 1.500180721282959 | Test loss: 0.7939754724502563\n",
            "Epoch 88910 | Loss: 1.499137282371521 | Test loss: 0.7931370735168457\n",
            "Epoch 88920 | Loss: 1.4980939626693726 | Test loss: 0.7922987341880798\n",
            "Epoch 88930 | Loss: 1.4970506429672241 | Test loss: 0.7914604544639587\n",
            "Epoch 88940 | Loss: 1.4960072040557861 | Test loss: 0.7906220555305481\n",
            "Epoch 88950 | Loss: 1.4949637651443481 | Test loss: 0.7897837162017822\n",
            "Epoch 88960 | Loss: 1.4939203262329102 | Test loss: 0.7889453768730164\n",
            "Epoch 88970 | Loss: 1.4928770065307617 | Test loss: 0.7881070375442505\n",
            "Epoch 88980 | Loss: 1.4918335676193237 | Test loss: 0.7872686386108398\n",
            "Epoch 88990 | Loss: 1.4907902479171753 | Test loss: 0.786430299282074\n",
            "Epoch 89000 | Loss: 1.4897468090057373 | Test loss: 0.7855919003486633\n",
            "Epoch 89010 | Loss: 1.4887036085128784 | Test loss: 0.7847536206245422\n",
            "Epoch 89020 | Loss: 1.4876600503921509 | Test loss: 0.7839152216911316\n",
            "Epoch 89030 | Loss: 1.486616611480713 | Test loss: 0.7830769419670105\n",
            "Epoch 89040 | Loss: 1.485573172569275 | Test loss: 0.7822385430335999\n",
            "Epoch 89050 | Loss: 1.4845298528671265 | Test loss: 0.7814001441001892\n",
            "Epoch 89060 | Loss: 1.4834864139556885 | Test loss: 0.7805618047714233\n",
            "Epoch 89070 | Loss: 1.48244309425354 | Test loss: 0.7797235250473022\n",
            "Epoch 89080 | Loss: 1.4813997745513916 | Test loss: 0.7788851261138916\n",
            "Epoch 89090 | Loss: 1.4803563356399536 | Test loss: 0.7780467867851257\n",
            "Epoch 89100 | Loss: 1.4793128967285156 | Test loss: 0.7772084474563599\n",
            "Epoch 89110 | Loss: 1.4782695770263672 | Test loss: 0.7763700485229492\n",
            "Epoch 89120 | Loss: 1.4772261381149292 | Test loss: 0.7755317091941833\n",
            "Epoch 89130 | Loss: 1.4761828184127808 | Test loss: 0.7746934294700623\n",
            "Epoch 89140 | Loss: 1.4751393795013428 | Test loss: 0.7738550305366516\n",
            "Epoch 89150 | Loss: 1.4740960597991943 | Test loss: 0.7730166912078857\n",
            "Epoch 89160 | Loss: 1.4730526208877563 | Test loss: 0.7721783518791199\n",
            "Epoch 89170 | Loss: 1.4720091819763184 | Test loss: 0.771340012550354\n",
            "Epoch 89180 | Loss: 1.4709657430648804 | Test loss: 0.7705016136169434\n",
            "Epoch 89190 | Loss: 1.469922423362732 | Test loss: 0.7696632742881775\n",
            "Epoch 89200 | Loss: 1.4688791036605835 | Test loss: 0.7688248753547668\n",
            "Epoch 89210 | Loss: 1.4678356647491455 | Test loss: 0.7679865956306458\n",
            "Epoch 89220 | Loss: 1.4667922258377075 | Test loss: 0.7671481966972351\n",
            "Epoch 89230 | Loss: 1.4657487869262695 | Test loss: 0.766309916973114\n",
            "Epoch 89240 | Loss: 1.464705467224121 | Test loss: 0.7654715180397034\n",
            "Epoch 89250 | Loss: 1.4636621475219727 | Test loss: 0.7646331191062927\n",
            "Epoch 89260 | Loss: 1.4626187086105347 | Test loss: 0.7637947797775269\n",
            "Epoch 89270 | Loss: 1.4615752696990967 | Test loss: 0.7629565000534058\n",
            "Epoch 89280 | Loss: 1.4605318307876587 | Test loss: 0.7621181011199951\n",
            "Epoch 89290 | Loss: 1.4594885110855103 | Test loss: 0.7612797617912292\n",
            "Epoch 89300 | Loss: 1.4584450721740723 | Test loss: 0.7604414224624634\n",
            "Epoch 89310 | Loss: 1.4574016332626343 | Test loss: 0.7596030235290527\n",
            "Epoch 89320 | Loss: 1.4563583135604858 | Test loss: 0.7587646842002869\n",
            "Epoch 89330 | Loss: 1.4553149938583374 | Test loss: 0.7579264044761658\n",
            "Epoch 89340 | Loss: 1.4542715549468994 | Test loss: 0.7570880055427551\n",
            "Epoch 89350 | Loss: 1.4532281160354614 | Test loss: 0.7562496662139893\n",
            "Epoch 89360 | Loss: 1.4521846771240234 | Test loss: 0.7554113268852234\n",
            "Epoch 89370 | Loss: 1.451141357421875 | Test loss: 0.7545729875564575\n",
            "Epoch 89380 | Loss: 1.450097918510437 | Test loss: 0.7537345886230469\n",
            "Epoch 89390 | Loss: 1.4490545988082886 | Test loss: 0.752896249294281\n",
            "Epoch 89400 | Loss: 1.4480111598968506 | Test loss: 0.7520578503608704\n",
            "Epoch 89410 | Loss: 1.4469679594039917 | Test loss: 0.7512195706367493\n",
            "Epoch 89420 | Loss: 1.4459244012832642 | Test loss: 0.7503811717033386\n",
            "Epoch 89430 | Loss: 1.4448809623718262 | Test loss: 0.7495428919792175\n",
            "Epoch 89440 | Loss: 1.4438375234603882 | Test loss: 0.7487044930458069\n",
            "Epoch 89450 | Loss: 1.4427942037582397 | Test loss: 0.7478660941123962\n",
            "Epoch 89460 | Loss: 1.4417507648468018 | Test loss: 0.7470277547836304\n",
            "Epoch 89470 | Loss: 1.4407074451446533 | Test loss: 0.7461894750595093\n",
            "Epoch 89480 | Loss: 1.4396641254425049 | Test loss: 0.7453510761260986\n",
            "Epoch 89490 | Loss: 1.438620686531067 | Test loss: 0.7445127367973328\n",
            "Epoch 89500 | Loss: 1.437577247619629 | Test loss: 0.7436743974685669\n",
            "Epoch 89510 | Loss: 1.4365339279174805 | Test loss: 0.7428359985351562\n",
            "Epoch 89520 | Loss: 1.4354904890060425 | Test loss: 0.7419976592063904\n",
            "Epoch 89530 | Loss: 1.434447169303894 | Test loss: 0.7411593794822693\n",
            "Epoch 89540 | Loss: 1.433403730392456 | Test loss: 0.7403209805488586\n",
            "Epoch 89550 | Loss: 1.4323604106903076 | Test loss: 0.7394826412200928\n",
            "Epoch 89560 | Loss: 1.4313169717788696 | Test loss: 0.7386443018913269\n",
            "Epoch 89570 | Loss: 1.4302735328674316 | Test loss: 0.737805962562561\n",
            "Epoch 89580 | Loss: 1.4292300939559937 | Test loss: 0.7369675636291504\n",
            "Epoch 89590 | Loss: 1.4281867742538452 | Test loss: 0.7361292243003845\n",
            "Epoch 89600 | Loss: 1.4271434545516968 | Test loss: 0.7352908253669739\n",
            "Epoch 89610 | Loss: 1.4261000156402588 | Test loss: 0.7344525456428528\n",
            "Epoch 89620 | Loss: 1.4250565767288208 | Test loss: 0.7336141467094421\n",
            "Epoch 89630 | Loss: 1.4240132570266724 | Test loss: 0.732775866985321\n",
            "Epoch 89640 | Loss: 1.4229698181152344 | Test loss: 0.7319374680519104\n",
            "Epoch 89650 | Loss: 1.421926498413086 | Test loss: 0.7310990691184998\n",
            "Epoch 89660 | Loss: 1.420883059501648 | Test loss: 0.7302607297897339\n",
            "Epoch 89670 | Loss: 1.41983962059021 | Test loss: 0.7294224500656128\n",
            "Epoch 89680 | Loss: 1.418796181678772 | Test loss: 0.7285840511322021\n",
            "Epoch 89690 | Loss: 1.4177528619766235 | Test loss: 0.7277457118034363\n",
            "Epoch 89700 | Loss: 1.4167094230651855 | Test loss: 0.7269073724746704\n",
            "Epoch 89710 | Loss: 1.4156659841537476 | Test loss: 0.7260689735412598\n",
            "Epoch 89720 | Loss: 1.4146226644515991 | Test loss: 0.7252306342124939\n",
            "Epoch 89730 | Loss: 1.4135793447494507 | Test loss: 0.7243923544883728\n",
            "Epoch 89740 | Loss: 1.4125359058380127 | Test loss: 0.7235539555549622\n",
            "Epoch 89750 | Loss: 1.4114925861358643 | Test loss: 0.7227156162261963\n",
            "Epoch 89760 | Loss: 1.4104490280151367 | Test loss: 0.7218772768974304\n",
            "Epoch 89770 | Loss: 1.4094057083129883 | Test loss: 0.7210389375686646\n",
            "Epoch 89780 | Loss: 1.4083622694015503 | Test loss: 0.7202005386352539\n",
            "Epoch 89790 | Loss: 1.4073189496994019 | Test loss: 0.719362199306488\n",
            "Epoch 89800 | Loss: 1.4062755107879639 | Test loss: 0.7185238003730774\n",
            "Epoch 89810 | Loss: 1.405232310295105 | Test loss: 0.7176855206489563\n",
            "Epoch 89820 | Loss: 1.4041887521743774 | Test loss: 0.7168471217155457\n",
            "Epoch 89830 | Loss: 1.403145432472229 | Test loss: 0.7160088419914246\n",
            "Epoch 89840 | Loss: 1.4021018743515015 | Test loss: 0.7151704430580139\n",
            "Epoch 89850 | Loss: 1.401058554649353 | Test loss: 0.7143320441246033\n",
            "Epoch 89860 | Loss: 1.400015115737915 | Test loss: 0.7134937047958374\n",
            "Epoch 89870 | Loss: 1.3989717960357666 | Test loss: 0.7126554250717163\n",
            "Epoch 89880 | Loss: 1.3979284763336182 | Test loss: 0.7118170261383057\n",
            "Epoch 89890 | Loss: 1.3968850374221802 | Test loss: 0.7109786868095398\n",
            "Epoch 89900 | Loss: 1.3958415985107422 | Test loss: 0.7101403474807739\n",
            "Epoch 89910 | Loss: 1.3947982788085938 | Test loss: 0.7093019485473633\n",
            "Epoch 89920 | Loss: 1.3937548398971558 | Test loss: 0.7084636092185974\n",
            "Epoch 89930 | Loss: 1.3927115201950073 | Test loss: 0.7076253294944763\n",
            "Epoch 89940 | Loss: 1.3916680812835693 | Test loss: 0.7067869305610657\n",
            "Epoch 89950 | Loss: 1.390624761581421 | Test loss: 0.7059485912322998\n",
            "Epoch 89960 | Loss: 1.389581322669983 | Test loss: 0.7051102519035339\n",
            "Epoch 89970 | Loss: 1.388537883758545 | Test loss: 0.7042719125747681\n",
            "Epoch 89980 | Loss: 1.387494444847107 | Test loss: 0.7034335136413574\n",
            "Epoch 89990 | Loss: 1.3864511251449585 | Test loss: 0.7025951743125916\n",
            "Epoch 90000 | Loss: 1.38540780544281 | Test loss: 0.7017567753791809\n",
            "Epoch 90010 | Loss: 1.384364366531372 | Test loss: 0.7009184956550598\n",
            "Epoch 90020 | Loss: 1.383320927619934 | Test loss: 0.7000800967216492\n",
            "Epoch 90030 | Loss: 1.3822776079177856 | Test loss: 0.6992418169975281\n",
            "Epoch 90040 | Loss: 1.3812341690063477 | Test loss: 0.6984034180641174\n",
            "Epoch 90050 | Loss: 1.3801908493041992 | Test loss: 0.6975650191307068\n",
            "Epoch 90060 | Loss: 1.3791474103927612 | Test loss: 0.6967266798019409\n",
            "Epoch 90070 | Loss: 1.3781039714813232 | Test loss: 0.6958884000778198\n",
            "Epoch 90080 | Loss: 1.3770605325698853 | Test loss: 0.6950500011444092\n",
            "Epoch 90090 | Loss: 1.3760172128677368 | Test loss: 0.6942116618156433\n",
            "Epoch 90100 | Loss: 1.3749737739562988 | Test loss: 0.6933733224868774\n",
            "Epoch 90110 | Loss: 1.3739303350448608 | Test loss: 0.6925349235534668\n",
            "Epoch 90120 | Loss: 1.3728870153427124 | Test loss: 0.6916965842247009\n",
            "Epoch 90130 | Loss: 1.371843695640564 | Test loss: 0.6908583045005798\n",
            "Epoch 90140 | Loss: 1.370800256729126 | Test loss: 0.6900199055671692\n",
            "Epoch 90150 | Loss: 1.3697569370269775 | Test loss: 0.6891815662384033\n",
            "Epoch 90160 | Loss: 1.36871337890625 | Test loss: 0.6883432269096375\n",
            "Epoch 90170 | Loss: 1.3676700592041016 | Test loss: 0.6875048875808716\n",
            "Epoch 90180 | Loss: 1.3666266202926636 | Test loss: 0.6866664886474609\n",
            "Epoch 90190 | Loss: 1.3655833005905151 | Test loss: 0.6858281493186951\n",
            "Epoch 90200 | Loss: 1.3645398616790771 | Test loss: 0.6849897503852844\n",
            "Epoch 90210 | Loss: 1.3634966611862183 | Test loss: 0.6841514706611633\n",
            "Epoch 90220 | Loss: 1.3624531030654907 | Test loss: 0.6833130717277527\n",
            "Epoch 90230 | Loss: 1.3614096641540527 | Test loss: 0.6824747920036316\n",
            "Epoch 90240 | Loss: 1.3603662252426147 | Test loss: 0.681636393070221\n",
            "Epoch 90250 | Loss: 1.3593229055404663 | Test loss: 0.6807979941368103\n",
            "Epoch 90260 | Loss: 1.3582794666290283 | Test loss: 0.6799596548080444\n",
            "Epoch 90270 | Loss: 1.3572361469268799 | Test loss: 0.6791213750839233\n",
            "Epoch 90280 | Loss: 1.3561928272247314 | Test loss: 0.6782829761505127\n",
            "Epoch 90290 | Loss: 1.3551493883132935 | Test loss: 0.6774446368217468\n",
            "Epoch 90300 | Loss: 1.3541059494018555 | Test loss: 0.676606297492981\n",
            "Epoch 90310 | Loss: 1.353062629699707 | Test loss: 0.6757678985595703\n",
            "Epoch 90320 | Loss: 1.352019190788269 | Test loss: 0.6749295592308044\n",
            "Epoch 90330 | Loss: 1.3509758710861206 | Test loss: 0.6740912795066833\n",
            "Epoch 90340 | Loss: 1.3499324321746826 | Test loss: 0.6732528805732727\n",
            "Epoch 90350 | Loss: 1.3488891124725342 | Test loss: 0.6724145412445068\n",
            "Epoch 90360 | Loss: 1.3478456735610962 | Test loss: 0.671576201915741\n",
            "Epoch 90370 | Loss: 1.3468022346496582 | Test loss: 0.6707378625869751\n",
            "Epoch 90380 | Loss: 1.3457587957382202 | Test loss: 0.6698994636535645\n",
            "Epoch 90390 | Loss: 1.3447154760360718 | Test loss: 0.6690611243247986\n",
            "Epoch 90400 | Loss: 1.3436721563339233 | Test loss: 0.6682227253913879\n",
            "Epoch 90410 | Loss: 1.3426287174224854 | Test loss: 0.6673844456672668\n",
            "Epoch 90420 | Loss: 1.3415852785110474 | Test loss: 0.6665460467338562\n",
            "Epoch 90430 | Loss: 1.340541958808899 | Test loss: 0.6657077670097351\n",
            "Epoch 90440 | Loss: 1.339498519897461 | Test loss: 0.6648693680763245\n",
            "Epoch 90450 | Loss: 1.3384552001953125 | Test loss: 0.6640309691429138\n",
            "Epoch 90460 | Loss: 1.3374117612838745 | Test loss: 0.663192629814148\n",
            "Epoch 90470 | Loss: 1.3363683223724365 | Test loss: 0.6623543500900269\n",
            "Epoch 90480 | Loss: 1.3353248834609985 | Test loss: 0.6615159511566162\n",
            "Epoch 90490 | Loss: 1.3342816829681396 | Test loss: 0.6606776118278503\n",
            "Epoch 90500 | Loss: 1.333238124847412 | Test loss: 0.6598392724990845\n",
            "Epoch 90510 | Loss: 1.3321946859359741 | Test loss: 0.6590008735656738\n",
            "Epoch 90520 | Loss: 1.3311513662338257 | Test loss: 0.658162534236908\n",
            "Epoch 90530 | Loss: 1.3301080465316772 | Test loss: 0.6573242545127869\n",
            "Epoch 90540 | Loss: 1.3290646076202393 | Test loss: 0.6564858555793762\n",
            "Epoch 90550 | Loss: 1.3280212879180908 | Test loss: 0.6556475162506104\n",
            "Epoch 90560 | Loss: 1.3269777297973633 | Test loss: 0.6548091173171997\n",
            "Epoch 90570 | Loss: 1.3259344100952148 | Test loss: 0.6539708375930786\n",
            "Epoch 90580 | Loss: 1.3248909711837769 | Test loss: 0.6531324982643127\n",
            "Epoch 90590 | Loss: 1.3238476514816284 | Test loss: 0.6522940993309021\n",
            "Epoch 90600 | Loss: 1.3228042125701904 | Test loss: 0.6514557003974915\n",
            "Epoch 90610 | Loss: 1.3217610120773315 | Test loss: 0.6506174206733704\n",
            "Epoch 90620 | Loss: 1.320717453956604 | Test loss: 0.6497790217399597\n",
            "Epoch 90630 | Loss: 1.3196741342544556 | Test loss: 0.6489407420158386\n",
            "Epoch 90640 | Loss: 1.318630576133728 | Test loss: 0.6481024026870728\n",
            "Epoch 90650 | Loss: 1.3175872564315796 | Test loss: 0.6472640037536621\n",
            "Epoch 90660 | Loss: 1.3165438175201416 | Test loss: 0.6464256644248962\n",
            "Epoch 90670 | Loss: 1.3155004978179932 | Test loss: 0.6455873250961304\n",
            "Epoch 90680 | Loss: 1.3144571781158447 | Test loss: 0.6447489261627197\n",
            "Epoch 90690 | Loss: 1.3134137392044067 | Test loss: 0.6439105868339539\n",
            "Epoch 90700 | Loss: 1.3123703002929688 | Test loss: 0.6430721879005432\n",
            "Epoch 90710 | Loss: 1.3113269805908203 | Test loss: 0.6422339081764221\n",
            "Epoch 90720 | Loss: 1.3102835416793823 | Test loss: 0.6413955092430115\n",
            "Epoch 90730 | Loss: 1.3092402219772339 | Test loss: 0.6405572295188904\n",
            "Epoch 90740 | Loss: 1.308196783065796 | Test loss: 0.6397188305854797\n",
            "Epoch 90750 | Loss: 1.3071534633636475 | Test loss: 0.6388804316520691\n",
            "Epoch 90760 | Loss: 1.3061100244522095 | Test loss: 0.6380420923233032\n",
            "Epoch 90770 | Loss: 1.3050665855407715 | Test loss: 0.6372038125991821\n",
            "Epoch 90780 | Loss: 1.3040231466293335 | Test loss: 0.6363654136657715\n",
            "Epoch 90790 | Loss: 1.302979826927185 | Test loss: 0.6355270743370056\n",
            "Epoch 90800 | Loss: 1.3019365072250366 | Test loss: 0.634688675403595\n",
            "Epoch 90810 | Loss: 1.3008930683135986 | Test loss: 0.6338503956794739\n",
            "Epoch 90820 | Loss: 1.2998496294021606 | Test loss: 0.6330119967460632\n",
            "Epoch 90830 | Loss: 1.2988063097000122 | Test loss: 0.6321736574172974\n",
            "Epoch 90840 | Loss: 1.2977628707885742 | Test loss: 0.6313353776931763\n",
            "Epoch 90850 | Loss: 1.2967195510864258 | Test loss: 0.6304969787597656\n",
            "Epoch 90860 | Loss: 1.2956761121749878 | Test loss: 0.6296586394309998\n",
            "Epoch 90870 | Loss: 1.2946326732635498 | Test loss: 0.6288203001022339\n",
            "Epoch 90880 | Loss: 1.2935892343521118 | Test loss: 0.6279819011688232\n",
            "Epoch 90890 | Loss: 1.292546033859253 | Test loss: 0.6271435618400574\n",
            "Epoch 90900 | Loss: 1.2915024757385254 | Test loss: 0.6263051629066467\n",
            "Epoch 90910 | Loss: 1.2904590368270874 | Test loss: 0.6254668831825256\n",
            "Epoch 90920 | Loss: 1.289415717124939 | Test loss: 0.624628484249115\n",
            "Epoch 90930 | Loss: 1.2883723974227905 | Test loss: 0.6237902045249939\n",
            "Epoch 90940 | Loss: 1.2873289585113525 | Test loss: 0.6229518055915833\n",
            "Epoch 90950 | Loss: 1.286285638809204 | Test loss: 0.6221134066581726\n",
            "Epoch 90960 | Loss: 1.2852420806884766 | Test loss: 0.6212750673294067\n",
            "Epoch 90970 | Loss: 1.2841987609863281 | Test loss: 0.6204367876052856\n",
            "Epoch 90980 | Loss: 1.2831553220748901 | Test loss: 0.619598388671875\n",
            "Epoch 90990 | Loss: 1.2821120023727417 | Test loss: 0.6187600493431091\n",
            "Epoch 91000 | Loss: 1.2810685634613037 | Test loss: 0.6179216504096985\n",
            "Epoch 91010 | Loss: 1.2800253629684448 | Test loss: 0.6170833706855774\n",
            "Epoch 91020 | Loss: 1.2789818048477173 | Test loss: 0.6162449717521667\n",
            "Epoch 91030 | Loss: 1.2779384851455688 | Test loss: 0.6154066324234009\n",
            "Epoch 91040 | Loss: 1.2768949270248413 | Test loss: 0.6145683526992798\n",
            "Epoch 91050 | Loss: 1.2758516073226929 | Test loss: 0.6137299537658691\n",
            "Epoch 91060 | Loss: 1.2748081684112549 | Test loss: 0.6128916144371033\n",
            "Epoch 91070 | Loss: 1.2737648487091064 | Test loss: 0.6120532751083374\n",
            "Epoch 91080 | Loss: 1.272721529006958 | Test loss: 0.6112148761749268\n",
            "Epoch 91090 | Loss: 1.27167809009552 | Test loss: 0.6103765368461609\n",
            "Epoch 91100 | Loss: 1.270634651184082 | Test loss: 0.6095381379127502\n",
            "Epoch 91110 | Loss: 1.2695913314819336 | Test loss: 0.6086998581886292\n",
            "Epoch 91120 | Loss: 1.2685478925704956 | Test loss: 0.6078614592552185\n",
            "Epoch 91130 | Loss: 1.2675045728683472 | Test loss: 0.6070231795310974\n",
            "Epoch 91140 | Loss: 1.2664611339569092 | Test loss: 0.6061847805976868\n",
            "Epoch 91150 | Loss: 1.2654178142547607 | Test loss: 0.6053463816642761\n",
            "Epoch 91160 | Loss: 1.2643743753433228 | Test loss: 0.604508101940155\n",
            "Epoch 91170 | Loss: 1.2633309364318848 | Test loss: 0.6036697626113892\n",
            "Epoch 91180 | Loss: 1.2622874975204468 | Test loss: 0.6028314232826233\n",
            "Epoch 91190 | Loss: 1.2612441778182983 | Test loss: 0.6019930243492126\n",
            "Epoch 91200 | Loss: 1.2602007389068604 | Test loss: 0.601154625415802\n",
            "Epoch 91210 | Loss: 1.259157419204712 | Test loss: 0.6003163456916809\n",
            "Epoch 91220 | Loss: 1.258113980293274 | Test loss: 0.5994779467582703\n",
            "Epoch 91230 | Loss: 1.2570706605911255 | Test loss: 0.5986396074295044\n",
            "Epoch 91240 | Loss: 1.2560272216796875 | Test loss: 0.5978013277053833\n",
            "Epoch 91250 | Loss: 1.254983901977539 | Test loss: 0.5969629287719727\n",
            "Epoch 91260 | Loss: 1.253940463066101 | Test loss: 0.5961245894432068\n",
            "Epoch 91270 | Loss: 1.252897024154663 | Test loss: 0.5952862501144409\n",
            "Epoch 91280 | Loss: 1.251853585243225 | Test loss: 0.5944478511810303\n",
            "Epoch 91290 | Loss: 1.2508102655410767 | Test loss: 0.5936095118522644\n",
            "Epoch 91300 | Loss: 1.2497668266296387 | Test loss: 0.5927711129188538\n",
            "Epoch 91310 | Loss: 1.2487233877182007 | Test loss: 0.5919328331947327\n",
            "Epoch 91320 | Loss: 1.2476800680160522 | Test loss: 0.591094434261322\n",
            "Epoch 91330 | Loss: 1.2466367483139038 | Test loss: 0.5902561545372009\n",
            "Epoch 91340 | Loss: 1.2455933094024658 | Test loss: 0.5894177556037903\n",
            "Epoch 91350 | Loss: 1.2445499897003174 | Test loss: 0.5885793566703796\n",
            "Epoch 91360 | Loss: 1.2435064315795898 | Test loss: 0.5877410173416138\n",
            "Epoch 91370 | Loss: 1.2424631118774414 | Test loss: 0.5869027376174927\n",
            "Epoch 91380 | Loss: 1.2414196729660034 | Test loss: 0.5860643982887268\n",
            "Epoch 91390 | Loss: 1.240376353263855 | Test loss: 0.5852259993553162\n",
            "Epoch 91400 | Loss: 1.2393330335617065 | Test loss: 0.5843876004219055\n",
            "Epoch 91410 | Loss: 1.238289713859558 | Test loss: 0.5835493206977844\n",
            "Epoch 91420 | Loss: 1.2372461557388306 | Test loss: 0.5827109217643738\n",
            "Epoch 91430 | Loss: 1.2362028360366821 | Test loss: 0.5818725824356079\n",
            "Epoch 91440 | Loss: 1.2351592779159546 | Test loss: 0.5810343027114868\n",
            "Epoch 91450 | Loss: 1.2341159582138062 | Test loss: 0.5801959037780762\n",
            "Epoch 91460 | Loss: 1.2330726385116577 | Test loss: 0.5793575644493103\n",
            "Epoch 91470 | Loss: 1.2320291996002197 | Test loss: 0.5785192251205444\n",
            "Epoch 91480 | Loss: 1.2309858798980713 | Test loss: 0.5776808261871338\n",
            "Epoch 91490 | Loss: 1.2299425601959229 | Test loss: 0.5768424868583679\n",
            "Epoch 91500 | Loss: 1.2288990020751953 | Test loss: 0.576004147529602\n",
            "Epoch 91510 | Loss: 1.2278556823730469 | Test loss: 0.5751658082008362\n",
            "Epoch 91520 | Loss: 1.2268122434616089 | Test loss: 0.5743274092674255\n",
            "Epoch 91530 | Loss: 1.2257689237594604 | Test loss: 0.5734891295433044\n",
            "Epoch 91540 | Loss: 1.2247254848480225 | Test loss: 0.5726507306098938\n",
            "Epoch 91550 | Loss: 1.223682165145874 | Test loss: 0.5718123316764832\n",
            "Epoch 91560 | Loss: 1.222638726234436 | Test loss: 0.5709739923477173\n",
            "Epoch 91570 | Loss: 1.221595287322998 | Test loss: 0.5701357126235962\n",
            "Epoch 91580 | Loss: 1.22055184841156 | Test loss: 0.5692973732948303\n",
            "Epoch 91590 | Loss: 1.2195085287094116 | Test loss: 0.5684589743614197\n",
            "Epoch 91600 | Loss: 1.2184650897979736 | Test loss: 0.567620575428009\n",
            "Epoch 91610 | Loss: 1.2174217700958252 | Test loss: 0.5667822957038879\n",
            "Epoch 91620 | Loss: 1.2163783311843872 | Test loss: 0.5659438967704773\n",
            "Epoch 91630 | Loss: 1.2153350114822388 | Test loss: 0.5651056170463562\n",
            "Epoch 91640 | Loss: 1.2142915725708008 | Test loss: 0.5642672777175903\n",
            "Epoch 91650 | Loss: 1.2132482528686523 | Test loss: 0.5634288787841797\n",
            "Epoch 91660 | Loss: 1.2122048139572144 | Test loss: 0.5625905394554138\n",
            "Epoch 91670 | Loss: 1.2111613750457764 | Test loss: 0.5617521405220032\n",
            "Epoch 91680 | Loss: 1.2101179361343384 | Test loss: 0.5609138011932373\n",
            "Epoch 91690 | Loss: 1.20907461643219 | Test loss: 0.5600754618644714\n",
            "Epoch 91700 | Loss: 1.208031177520752 | Test loss: 0.5592371225357056\n",
            "Epoch 91710 | Loss: 1.206987738609314 | Test loss: 0.5583987832069397\n",
            "Epoch 91720 | Loss: 1.2059444189071655 | Test loss: 0.557560384273529\n",
            "Epoch 91730 | Loss: 1.204901099205017 | Test loss: 0.556722104549408\n",
            "Epoch 91740 | Loss: 1.203857660293579 | Test loss: 0.5558837056159973\n",
            "Epoch 91750 | Loss: 1.2028143405914307 | Test loss: 0.5550453066825867\n",
            "Epoch 91760 | Loss: 1.2017707824707031 | Test loss: 0.5542069673538208\n",
            "Epoch 91770 | Loss: 1.2007274627685547 | Test loss: 0.5533686876296997\n",
            "Epoch 91780 | Loss: 1.1996840238571167 | Test loss: 0.5525303483009338\n",
            "Epoch 91790 | Loss: 1.1986407041549683 | Test loss: 0.5516919493675232\n",
            "Epoch 91800 | Loss: 1.1975973844528198 | Test loss: 0.5508535504341125\n",
            "Epoch 91810 | Loss: 1.1965540647506714 | Test loss: 0.5500152707099915\n",
            "Epoch 91820 | Loss: 1.1955105066299438 | Test loss: 0.5491768717765808\n",
            "Epoch 91830 | Loss: 1.1944671869277954 | Test loss: 0.5483385920524597\n",
            "Epoch 91840 | Loss: 1.1934236288070679 | Test loss: 0.5475001931190491\n",
            "Epoch 91850 | Loss: 1.1923803091049194 | Test loss: 0.5466618537902832\n",
            "Epoch 91860 | Loss: 1.191336989402771 | Test loss: 0.5458235144615173\n",
            "Epoch 91870 | Loss: 1.190293550491333 | Test loss: 0.5449851155281067\n",
            "Epoch 91880 | Loss: 1.1892502307891846 | Test loss: 0.5441467761993408\n",
            "Epoch 91890 | Loss: 1.1882069110870361 | Test loss: 0.543308436870575\n",
            "Epoch 91900 | Loss: 1.1871633529663086 | Test loss: 0.5424700975418091\n",
            "Epoch 91910 | Loss: 1.1861200332641602 | Test loss: 0.5416317582130432\n",
            "Epoch 91920 | Loss: 1.1850765943527222 | Test loss: 0.5407933592796326\n",
            "Epoch 91930 | Loss: 1.1840332746505737 | Test loss: 0.5399550795555115\n",
            "Epoch 91940 | Loss: 1.1829898357391357 | Test loss: 0.5391166806221008\n",
            "Epoch 91950 | Loss: 1.1819465160369873 | Test loss: 0.5382782816886902\n",
            "Epoch 91960 | Loss: 1.1809030771255493 | Test loss: 0.5374400019645691\n",
            "Epoch 91970 | Loss: 1.1798596382141113 | Test loss: 0.5366016626358032\n",
            "Epoch 91980 | Loss: 1.1788161993026733 | Test loss: 0.5357633233070374\n",
            "Epoch 91990 | Loss: 1.177772879600525 | Test loss: 0.5349249243736267\n",
            "Epoch 92000 | Loss: 1.176729440689087 | Test loss: 0.5340865254402161\n",
            "Epoch 92010 | Loss: 1.1756861209869385 | Test loss: 0.533248245716095\n",
            "Epoch 92020 | Loss: 1.174642562866211 | Test loss: 0.5324098467826843\n",
            "Epoch 92030 | Loss: 1.173599362373352 | Test loss: 0.5315715670585632\n",
            "Epoch 92040 | Loss: 1.172555923461914 | Test loss: 0.5307331681251526\n",
            "Epoch 92050 | Loss: 1.1715126037597656 | Test loss: 0.5298948287963867\n",
            "Epoch 92060 | Loss: 1.1704691648483276 | Test loss: 0.5290564894676208\n",
            "Epoch 92070 | Loss: 1.1694257259368896 | Test loss: 0.5282180905342102\n",
            "Epoch 92080 | Loss: 1.1683822870254517 | Test loss: 0.5273797512054443\n",
            "Epoch 92090 | Loss: 1.1673389673233032 | Test loss: 0.5265414118766785\n",
            "Epoch 92100 | Loss: 1.1662955284118652 | Test loss: 0.5257030725479126\n",
            "Epoch 92110 | Loss: 1.1652520895004272 | Test loss: 0.5248647928237915\n",
            "Epoch 92120 | Loss: 1.1642087697982788 | Test loss: 0.5240263342857361\n",
            "Epoch 92130 | Loss: 1.1631654500961304 | Test loss: 0.523188054561615\n",
            "Epoch 92140 | Loss: 1.1621220111846924 | Test loss: 0.5223496556282043\n",
            "Epoch 92150 | Loss: 1.161078691482544 | Test loss: 0.5215112566947937\n",
            "Epoch 92160 | Loss: 1.1600351333618164 | Test loss: 0.5206729769706726\n",
            "Epoch 92170 | Loss: 1.158991813659668 | Test loss: 0.5198346376419067\n",
            "Epoch 92180 | Loss: 1.15794837474823 | Test loss: 0.5189962983131409\n",
            "Epoch 92190 | Loss: 1.1569050550460815 | Test loss: 0.5181578993797302\n",
            "Epoch 92200 | Loss: 1.155861735343933 | Test loss: 0.5173195004463196\n",
            "Epoch 92210 | Loss: 1.1548184156417847 | Test loss: 0.5164812207221985\n",
            "Epoch 92220 | Loss: 1.1537748575210571 | Test loss: 0.5156428217887878\n",
            "Epoch 92230 | Loss: 1.1527315378189087 | Test loss: 0.5148045420646667\n",
            "Epoch 92240 | Loss: 1.1516879796981812 | Test loss: 0.5139662027359009\n",
            "Epoch 92250 | Loss: 1.1506446599960327 | Test loss: 0.5131278038024902\n",
            "Epoch 92260 | Loss: 1.1496013402938843 | Test loss: 0.5122894644737244\n",
            "Epoch 92270 | Loss: 1.1485579013824463 | Test loss: 0.5114510655403137\n",
            "Epoch 92280 | Loss: 1.1475145816802979 | Test loss: 0.5106127262115479\n",
            "Epoch 92290 | Loss: 1.1464712619781494 | Test loss: 0.509774386882782\n",
            "Epoch 92300 | Loss: 1.1454277038574219 | Test loss: 0.5089360475540161\n",
            "Epoch 92310 | Loss: 1.1443843841552734 | Test loss: 0.508097767829895\n",
            "Epoch 92320 | Loss: 1.1433409452438354 | Test loss: 0.5072593092918396\n",
            "Epoch 92330 | Loss: 1.142297625541687 | Test loss: 0.5064210295677185\n",
            "Epoch 92340 | Loss: 1.141254186630249 | Test loss: 0.5055826306343079\n",
            "Epoch 92350 | Loss: 1.1402108669281006 | Test loss: 0.5047442317008972\n",
            "Epoch 92360 | Loss: 1.1391674280166626 | Test loss: 0.5039059519767761\n",
            "Epoch 92370 | Loss: 1.1381239891052246 | Test loss: 0.5030676126480103\n",
            "Epoch 92380 | Loss: 1.1370805501937866 | Test loss: 0.5022292733192444\n",
            "Epoch 92390 | Loss: 1.1360372304916382 | Test loss: 0.5013908743858337\n",
            "Epoch 92400 | Loss: 1.1349937915802002 | Test loss: 0.5005524754524231\n",
            "Epoch 92410 | Loss: 1.1339504718780518 | Test loss: 0.499714195728302\n",
            "Epoch 92420 | Loss: 1.1329070329666138 | Test loss: 0.49887582659721375\n",
            "Epoch 92430 | Loss: 1.1318637132644653 | Test loss: 0.4980374872684479\n",
            "Epoch 92440 | Loss: 1.1308202743530273 | Test loss: 0.4971991181373596\n",
            "Epoch 92450 | Loss: 1.129776954650879 | Test loss: 0.49636077880859375\n",
            "Epoch 92460 | Loss: 1.128733515739441 | Test loss: 0.4955224096775055\n",
            "Epoch 92470 | Loss: 1.127690076828003 | Test loss: 0.4946840703487396\n",
            "Epoch 92480 | Loss: 1.126646637916565 | Test loss: 0.49384576082229614\n",
            "Epoch 92490 | Loss: 1.1256033182144165 | Test loss: 0.4930073916912079\n",
            "Epoch 92500 | Loss: 1.1245598793029785 | Test loss: 0.49216899275779724\n",
            "Epoch 92510 | Loss: 1.1235164403915405 | Test loss: 0.49133071303367615\n",
            "Epoch 92520 | Loss: 1.122473120689392 | Test loss: 0.4904923439025879\n",
            "Epoch 92530 | Loss: 1.1214298009872437 | Test loss: 0.4896540343761444\n",
            "Epoch 92540 | Loss: 1.1203863620758057 | Test loss: 0.4888156056404114\n",
            "Epoch 92550 | Loss: 1.1193430423736572 | Test loss: 0.4879772365093231\n",
            "Epoch 92560 | Loss: 1.1182994842529297 | Test loss: 0.4871388375759125\n",
            "Epoch 92570 | Loss: 1.1172561645507812 | Test loss: 0.4863005578517914\n",
            "Epoch 92580 | Loss: 1.1162127256393433 | Test loss: 0.4854622483253479\n",
            "Epoch 92590 | Loss: 1.1151694059371948 | Test loss: 0.48462381958961487\n",
            "Epoch 92600 | Loss: 1.1141260862350464 | Test loss: 0.4837854504585266\n",
            "Epoch 92610 | Loss: 1.113082766532898 | Test loss: 0.4829471707344055\n",
            "Epoch 92620 | Loss: 1.1120392084121704 | Test loss: 0.48210880160331726\n",
            "Epoch 92630 | Loss: 1.110995888710022 | Test loss: 0.4812704622745514\n",
            "Epoch 92640 | Loss: 1.1099523305892944 | Test loss: 0.4804321229457855\n",
            "Epoch 92650 | Loss: 1.108909010887146 | Test loss: 0.47959375381469727\n",
            "Epoch 92660 | Loss: 1.1078656911849976 | Test loss: 0.478755384683609\n",
            "Epoch 92670 | Loss: 1.1068222522735596 | Test loss: 0.47791704535484314\n",
            "Epoch 92680 | Loss: 1.1057789325714111 | Test loss: 0.47707873582839966\n",
            "Epoch 92690 | Loss: 1.1047356128692627 | Test loss: 0.4762403666973114\n",
            "Epoch 92700 | Loss: 1.1036920547485352 | Test loss: 0.47540196776390076\n",
            "Epoch 92710 | Loss: 1.1026487350463867 | Test loss: 0.47456368803977966\n",
            "Epoch 92720 | Loss: 1.1016052961349487 | Test loss: 0.4737253189086914\n",
            "Epoch 92730 | Loss: 1.1005619764328003 | Test loss: 0.4728870093822479\n",
            "Epoch 92740 | Loss: 1.0995185375213623 | Test loss: 0.4720485806465149\n",
            "Epoch 92750 | Loss: 1.0984752178192139 | Test loss: 0.47121021151542664\n",
            "Epoch 92760 | Loss: 1.0974317789077759 | Test loss: 0.47037187218666077\n",
            "Epoch 92770 | Loss: 1.096388339996338 | Test loss: 0.4695335328578949\n",
            "Epoch 92780 | Loss: 1.0953449010849 | Test loss: 0.4686952233314514\n",
            "Epoch 92790 | Loss: 1.0943015813827515 | Test loss: 0.4678567945957184\n",
            "Epoch 92800 | Loss: 1.0932581424713135 | Test loss: 0.4670184254646301\n",
            "Epoch 92810 | Loss: 1.092214822769165 | Test loss: 0.46618014574050903\n",
            "Epoch 92820 | Loss: 1.091171383857727 | Test loss: 0.4653417766094208\n",
            "Epoch 92830 | Loss: 1.0901280641555786 | Test loss: 0.4645034372806549\n",
            "Epoch 92840 | Loss: 1.0890846252441406 | Test loss: 0.46366509795188904\n",
            "Epoch 92850 | Loss: 1.0880411863327026 | Test loss: 0.4628267288208008\n",
            "Epoch 92860 | Loss: 1.0869978666305542 | Test loss: 0.4619884192943573\n",
            "Epoch 92870 | Loss: 1.0859544277191162 | Test loss: 0.46115002036094666\n",
            "Epoch 92880 | Loss: 1.0849109888076782 | Test loss: 0.4603117108345032\n",
            "Epoch 92890 | Loss: 1.0838676691055298 | Test loss: 0.4594733417034149\n",
            "Epoch 92900 | Loss: 1.0828242301940918 | Test loss: 0.4586349427700043\n",
            "Epoch 92910 | Loss: 1.0817807912826538 | Test loss: 0.4577966630458832\n",
            "Epoch 92920 | Loss: 1.0807374715805054 | Test loss: 0.4569582939147949\n",
            "Epoch 92930 | Loss: 1.079694151878357 | Test loss: 0.45611992478370667\n",
            "Epoch 92940 | Loss: 1.078650712966919 | Test loss: 0.4552815854549408\n",
            "Epoch 92950 | Loss: 1.0776073932647705 | Test loss: 0.45444318652153015\n",
            "Epoch 92960 | Loss: 1.076563835144043 | Test loss: 0.4536048471927643\n",
            "Epoch 92970 | Loss: 1.0755205154418945 | Test loss: 0.4527665078639984\n",
            "Epoch 92980 | Loss: 1.0744770765304565 | Test loss: 0.45192813873291016\n",
            "Epoch 92990 | Loss: 1.073433756828308 | Test loss: 0.4510898292064667\n",
            "Epoch 93000 | Loss: 1.0723904371261597 | Test loss: 0.45025140047073364\n",
            "Epoch 93010 | Loss: 1.0713471174240112 | Test loss: 0.44941312074661255\n",
            "Epoch 93020 | Loss: 1.0703035593032837 | Test loss: 0.4485747516155243\n",
            "Epoch 93030 | Loss: 1.0692601203918457 | Test loss: 0.4477364122867584\n",
            "Epoch 93040 | Loss: 1.0682168006896973 | Test loss: 0.44689807295799255\n",
            "Epoch 93050 | Loss: 1.0671733617782593 | Test loss: 0.4460597038269043\n",
            "Epoch 93060 | Loss: 1.0661300420761108 | Test loss: 0.4452213943004608\n",
            "Epoch 93070 | Loss: 1.0650866031646729 | Test loss: 0.44438299536705017\n",
            "Epoch 93080 | Loss: 1.0640432834625244 | Test loss: 0.4435446858406067\n",
            "Epoch 93090 | Loss: 1.0629998445510864 | Test loss: 0.44270631670951843\n",
            "Epoch 93100 | Loss: 1.0619564056396484 | Test loss: 0.44186797738075256\n",
            "Epoch 93110 | Loss: 1.0609130859375 | Test loss: 0.4410296380519867\n",
            "Epoch 93120 | Loss: 1.059869647026062 | Test loss: 0.44019126892089844\n",
            "Epoch 93130 | Loss: 1.0588263273239136 | Test loss: 0.4393528997898102\n",
            "Epoch 93140 | Loss: 1.0577828884124756 | Test loss: 0.4385145604610443\n",
            "Epoch 93150 | Loss: 1.0567395687103271 | Test loss: 0.43767616152763367\n",
            "Epoch 93160 | Loss: 1.0556961297988892 | Test loss: 0.4368378221988678\n",
            "Epoch 93170 | Loss: 1.0546526908874512 | Test loss: 0.43599948287010193\n",
            "Epoch 93180 | Loss: 1.0536092519760132 | Test loss: 0.43516111373901367\n",
            "Epoch 93190 | Loss: 1.0525659322738647 | Test loss: 0.4343228042125702\n",
            "Epoch 93200 | Loss: 1.0515224933624268 | Test loss: 0.43348437547683716\n",
            "Epoch 93210 | Loss: 1.0504791736602783 | Test loss: 0.43264609575271606\n",
            "Epoch 93220 | Loss: 1.0494357347488403 | Test loss: 0.4318077266216278\n",
            "Epoch 93230 | Loss: 1.0483922958374023 | Test loss: 0.43096938729286194\n",
            "Epoch 93240 | Loss: 1.047348976135254 | Test loss: 0.43013104796409607\n",
            "Epoch 93250 | Loss: 1.046305537223816 | Test loss: 0.4292926788330078\n",
            "Epoch 93260 | Loss: 1.0452622175216675 | Test loss: 0.42845430970191956\n",
            "Epoch 93270 | Loss: 1.0442187786102295 | Test loss: 0.4276159703731537\n",
            "Epoch 93280 | Loss: 1.0431753396987915 | Test loss: 0.4267776608467102\n",
            "Epoch 93290 | Loss: 1.042132019996643 | Test loss: 0.42593929171562195\n",
            "Epoch 93300 | Loss: 1.041088581085205 | Test loss: 0.4251009523868561\n",
            "Epoch 93310 | Loss: 1.040045142173767 | Test loss: 0.4242626130580902\n",
            "Epoch 93320 | Loss: 1.0390018224716187 | Test loss: 0.42342424392700195\n",
            "Epoch 93330 | Loss: 1.0379585027694702 | Test loss: 0.4225858747959137\n",
            "Epoch 93340 | Loss: 1.0369150638580322 | Test loss: 0.4217475354671478\n",
            "Epoch 93350 | Loss: 1.0358717441558838 | Test loss: 0.4209091365337372\n",
            "Epoch 93360 | Loss: 1.0348281860351562 | Test loss: 0.4200707972049713\n",
            "Epoch 93370 | Loss: 1.0337848663330078 | Test loss: 0.41923245787620544\n",
            "Epoch 93380 | Loss: 1.0327414274215698 | Test loss: 0.41839414834976196\n",
            "Epoch 93390 | Loss: 1.0316981077194214 | Test loss: 0.4175557792186737\n",
            "Epoch 93400 | Loss: 1.030654788017273 | Test loss: 0.4167173504829407\n",
            "Epoch 93410 | Loss: 1.0296114683151245 | Test loss: 0.4158790707588196\n",
            "Epoch 93420 | Loss: 1.028567910194397 | Test loss: 0.4150407314300537\n",
            "Epoch 93430 | Loss: 1.027524471282959 | Test loss: 0.41420236229896545\n",
            "Epoch 93440 | Loss: 1.0264811515808105 | Test loss: 0.4133640229701996\n",
            "Epoch 93450 | Loss: 1.0254377126693726 | Test loss: 0.41252565383911133\n",
            "Epoch 93460 | Loss: 1.0243943929672241 | Test loss: 0.41168728470802307\n",
            "Epoch 93470 | Loss: 1.0233509540557861 | Test loss: 0.4108489453792572\n",
            "Epoch 93480 | Loss: 1.0223076343536377 | Test loss: 0.4100106358528137\n",
            "Epoch 93490 | Loss: 1.0212641954421997 | Test loss: 0.40917226672172546\n",
            "Epoch 93500 | Loss: 1.0202207565307617 | Test loss: 0.4083339273929596\n",
            "Epoch 93510 | Loss: 1.0191774368286133 | Test loss: 0.4074955880641937\n",
            "Epoch 93520 | Loss: 1.0181339979171753 | Test loss: 0.40665721893310547\n",
            "Epoch 93530 | Loss: 1.0170906782150269 | Test loss: 0.4058188498020172\n",
            "Epoch 93540 | Loss: 1.0160472393035889 | Test loss: 0.40498051047325134\n",
            "Epoch 93550 | Loss: 1.0150039196014404 | Test loss: 0.4041421115398407\n",
            "Epoch 93560 | Loss: 1.0139604806900024 | Test loss: 0.40330377221107483\n",
            "Epoch 93570 | Loss: 1.0129170417785645 | Test loss: 0.40246543288230896\n",
            "Epoch 93580 | Loss: 1.0118736028671265 | Test loss: 0.4016270637512207\n",
            "Epoch 93590 | Loss: 1.010830283164978 | Test loss: 0.40078869462013245\n",
            "Epoch 93600 | Loss: 1.00978684425354 | Test loss: 0.3999503552913666\n",
            "Epoch 93610 | Loss: 1.0087435245513916 | Test loss: 0.3991120457649231\n",
            "Epoch 93620 | Loss: 1.0077000856399536 | Test loss: 0.3982737064361572\n",
            "Epoch 93630 | Loss: 1.0066566467285156 | Test loss: 0.39743533730506897\n",
            "Epoch 93640 | Loss: 1.0056133270263672 | Test loss: 0.3965969979763031\n",
            "Epoch 93650 | Loss: 1.0045698881149292 | Test loss: 0.39575859904289246\n",
            "Epoch 93660 | Loss: 1.0035265684127808 | Test loss: 0.394920289516449\n",
            "Epoch 93670 | Loss: 1.0024831295013428 | Test loss: 0.3940819203853607\n",
            "Epoch 93680 | Loss: 1.0014396905899048 | Test loss: 0.39324361085891724\n",
            "Epoch 93690 | Loss: 1.0003963708877563 | Test loss: 0.3924052119255066\n",
            "Epoch 93700 | Loss: 0.9993529319763184 | Test loss: 0.3915669023990631\n",
            "Epoch 93710 | Loss: 0.9983095526695251 | Test loss: 0.39072856307029724\n",
            "Epoch 93720 | Loss: 0.9972662329673767 | Test loss: 0.389890193939209\n",
            "Epoch 93730 | Loss: 0.9962227940559387 | Test loss: 0.3890518248081207\n",
            "Epoch 93740 | Loss: 0.9951793551445007 | Test loss: 0.38821348547935486\n",
            "Epoch 93750 | Loss: 0.9941360354423523 | Test loss: 0.3873750865459442\n",
            "Epoch 93760 | Loss: 0.9930925369262695 | Test loss: 0.38653674721717834\n",
            "Epoch 93770 | Loss: 0.9920492172241211 | Test loss: 0.3856984078884125\n",
            "Epoch 93780 | Loss: 0.9910058379173279 | Test loss: 0.3848600685596466\n",
            "Epoch 93790 | Loss: 0.9899625182151794 | Test loss: 0.38402169942855835\n",
            "Epoch 93800 | Loss: 0.9889190793037415 | Test loss: 0.3831833302974701\n",
            "Epoch 93810 | Loss: 0.9878756403923035 | Test loss: 0.3823450207710266\n",
            "Epoch 93820 | Loss: 0.9868322610855103 | Test loss: 0.38150668144226074\n",
            "Epoch 93830 | Loss: 0.9857889413833618 | Test loss: 0.3806683123111725\n",
            "Epoch 93840 | Loss: 0.984745442867279 | Test loss: 0.3798299729824066\n",
            "Epoch 93850 | Loss: 0.9837021231651306 | Test loss: 0.37899157404899597\n",
            "Epoch 93860 | Loss: 0.9826586842536926 | Test loss: 0.3781532645225525\n",
            "Epoch 93870 | Loss: 0.9816152453422546 | Test loss: 0.37731489539146423\n",
            "Epoch 93880 | Loss: 0.9805719256401062 | Test loss: 0.37647658586502075\n",
            "Epoch 93890 | Loss: 0.979528546333313 | Test loss: 0.3756382167339325\n",
            "Epoch 93900 | Loss: 0.978485107421875 | Test loss: 0.3747998774051666\n",
            "Epoch 93910 | Loss: 0.9774417877197266 | Test loss: 0.37396153807640076\n",
            "Epoch 93920 | Loss: 0.9763982892036438 | Test loss: 0.3731231689453125\n",
            "Epoch 93930 | Loss: 0.9753549695014954 | Test loss: 0.37228479981422424\n",
            "Epoch 93940 | Loss: 0.9743115305900574 | Test loss: 0.371446430683136\n",
            "Epoch 93950 | Loss: 0.9732681512832642 | Test loss: 0.37060806155204773\n",
            "Epoch 93960 | Loss: 0.9722248315811157 | Test loss: 0.36976972222328186\n",
            "Epoch 93970 | Loss: 0.9711813926696777 | Test loss: 0.368931382894516\n",
            "Epoch 93980 | Loss: 0.9701380133628845 | Test loss: 0.3680930733680725\n",
            "Epoch 93990 | Loss: 0.9690946936607361 | Test loss: 0.36725467443466187\n",
            "Epoch 94000 | Loss: 0.9680511355400085 | Test loss: 0.3664163053035736\n",
            "Epoch 94010 | Loss: 0.9670078158378601 | Test loss: 0.3655779957771301\n",
            "Epoch 94020 | Loss: 0.9659644365310669 | Test loss: 0.36473965644836426\n",
            "Epoch 94030 | Loss: 0.9649209976196289 | Test loss: 0.363901287317276\n",
            "Epoch 94040 | Loss: 0.9638776183128357 | Test loss: 0.36306294798851013\n",
            "Epoch 94050 | Loss: 0.9628342986106873 | Test loss: 0.3622245788574219\n",
            "Epoch 94060 | Loss: 0.9617908596992493 | Test loss: 0.361386239528656\n",
            "Epoch 94070 | Loss: 0.9607474207878113 | Test loss: 0.36054784059524536\n",
            "Epoch 94080 | Loss: 0.9597040414810181 | Test loss: 0.35970956087112427\n",
            "Epoch 94090 | Loss: 0.9586607217788696 | Test loss: 0.3588711619377136\n",
            "Epoch 94100 | Loss: 0.9576172828674316 | Test loss: 0.35803288221359253\n",
            "Epoch 94110 | Loss: 0.9565739035606384 | Test loss: 0.3571945130825043\n",
            "Epoch 94120 | Loss: 0.95553058385849 | Test loss: 0.356356143951416\n",
            "Epoch 94130 | Loss: 0.954487144947052 | Test loss: 0.35551777482032776\n",
            "Epoch 94140 | Loss: 0.953443706035614 | Test loss: 0.3546794354915619\n",
            "Epoch 94150 | Loss: 0.9524003863334656 | Test loss: 0.35384103655815125\n",
            "Epoch 94160 | Loss: 0.9513568878173828 | Test loss: 0.3530026972293854\n",
            "Epoch 94170 | Loss: 0.9503135681152344 | Test loss: 0.3521643579006195\n",
            "Epoch 94180 | Loss: 0.9492701888084412 | Test loss: 0.35132601857185364\n",
            "Epoch 94190 | Loss: 0.9482268691062927 | Test loss: 0.3504876494407654\n",
            "Epoch 94200 | Loss: 0.9471834301948547 | Test loss: 0.3496492803096771\n",
            "Epoch 94210 | Loss: 0.946139931678772 | Test loss: 0.34881097078323364\n",
            "Epoch 94220 | Loss: 0.9450966119766235 | Test loss: 0.3479726314544678\n",
            "Epoch 94230 | Loss: 0.9440531730651855 | Test loss: 0.3471342921257019\n",
            "Epoch 94240 | Loss: 0.9430097937583923 | Test loss: 0.34629592299461365\n",
            "Epoch 94250 | Loss: 0.9419664740562439 | Test loss: 0.345457524061203\n",
            "Epoch 94260 | Loss: 0.9409230351448059 | Test loss: 0.3446192145347595\n",
            "Epoch 94270 | Loss: 0.9398795962333679 | Test loss: 0.3437808156013489\n",
            "Epoch 94280 | Loss: 0.9388362765312195 | Test loss: 0.3429425358772278\n",
            "Epoch 94290 | Loss: 0.9377928972244263 | Test loss: 0.3421041667461395\n",
            "Epoch 94300 | Loss: 0.9367494583129883 | Test loss: 0.34126585721969604\n",
            "Epoch 94310 | Loss: 0.9357061386108398 | Test loss: 0.3404274880886078\n",
            "Epoch 94320 | Loss: 0.9346626400947571 | Test loss: 0.33958911895751953\n",
            "Epoch 94330 | Loss: 0.9336193203926086 | Test loss: 0.3387507498264313\n",
            "Epoch 94340 | Loss: 0.9325758814811707 | Test loss: 0.3379124104976654\n",
            "Epoch 94350 | Loss: 0.9315325021743774 | Test loss: 0.33707401156425476\n",
            "Epoch 94360 | Loss: 0.930489182472229 | Test loss: 0.3362356722354889\n",
            "Epoch 94370 | Loss: 0.929445743560791 | Test loss: 0.335397332906723\n",
            "Epoch 94380 | Loss: 0.9284023642539978 | Test loss: 0.33455899357795715\n",
            "Epoch 94390 | Loss: 0.9273590445518494 | Test loss: 0.3337206542491913\n",
            "Epoch 94400 | Loss: 0.9263154864311218 | Test loss: 0.33288225531578064\n",
            "Epoch 94410 | Loss: 0.9252721667289734 | Test loss: 0.33204394578933716\n",
            "Epoch 94420 | Loss: 0.9242287874221802 | Test loss: 0.3312056064605713\n",
            "Epoch 94430 | Loss: 0.9231853485107422 | Test loss: 0.33036723732948303\n",
            "Epoch 94440 | Loss: 0.922141969203949 | Test loss: 0.32952889800071716\n",
            "Epoch 94450 | Loss: 0.9210986495018005 | Test loss: 0.3286904990673065\n",
            "Epoch 94460 | Loss: 0.9200552105903625 | Test loss: 0.3278522193431854\n",
            "Epoch 94470 | Loss: 0.9190117716789246 | Test loss: 0.3270137906074524\n",
            "Epoch 94480 | Loss: 0.9179683923721313 | Test loss: 0.3261755108833313\n",
            "Epoch 94490 | Loss: 0.9169250726699829 | Test loss: 0.32533714175224304\n",
            "Epoch 94500 | Loss: 0.9158816337585449 | Test loss: 0.32449886202812195\n",
            "Epoch 94510 | Loss: 0.9148382544517517 | Test loss: 0.3236604630947113\n",
            "Epoch 94520 | Loss: 0.9137949347496033 | Test loss: 0.32282209396362305\n",
            "Epoch 94530 | Loss: 0.9127514958381653 | Test loss: 0.3219837248325348\n",
            "Epoch 94540 | Loss: 0.9117080569267273 | Test loss: 0.3211453855037689\n",
            "Epoch 94550 | Loss: 0.9106647372245789 | Test loss: 0.3203069865703583\n",
            "Epoch 94560 | Loss: 0.9096213579177856 | Test loss: 0.3194686472415924\n",
            "Epoch 94570 | Loss: 0.9085779190063477 | Test loss: 0.31863030791282654\n",
            "Epoch 94580 | Loss: 0.9075345396995544 | Test loss: 0.31779196858406067\n",
            "Epoch 94590 | Loss: 0.906491219997406 | Test loss: 0.31695356965065\n",
            "Epoch 94600 | Loss: 0.905447781085968 | Test loss: 0.31611523032188416\n",
            "Epoch 94610 | Loss: 0.9044042825698853 | Test loss: 0.3152769207954407\n",
            "Epoch 94620 | Loss: 0.9033609628677368 | Test loss: 0.3144385814666748\n",
            "Epoch 94630 | Loss: 0.9023175239562988 | Test loss: 0.31360021233558655\n",
            "Epoch 94640 | Loss: 0.9012741446495056 | Test loss: 0.3127618730068207\n",
            "Epoch 94650 | Loss: 0.9002308249473572 | Test loss: 0.3119235038757324\n",
            "Epoch 94660 | Loss: 0.8991873860359192 | Test loss: 0.31108519434928894\n",
            "Epoch 94670 | Loss: 0.8981439471244812 | Test loss: 0.3102467656135559\n",
            "Epoch 94680 | Loss: 0.8971006274223328 | Test loss: 0.3094084560871124\n",
            "Epoch 94690 | Loss: 0.8960572481155396 | Test loss: 0.30857008695602417\n",
            "Epoch 94700 | Loss: 0.8950138092041016 | Test loss: 0.30773183703422546\n",
            "Epoch 94710 | Loss: 0.8939704895019531 | Test loss: 0.3068934381008148\n",
            "Epoch 94720 | Loss: 0.8929269909858704 | Test loss: 0.3060550391674042\n",
            "Epoch 94730 | Loss: 0.8918836712837219 | Test loss: 0.3052166998386383\n",
            "Epoch 94740 | Loss: 0.8908402323722839 | Test loss: 0.30437836050987244\n",
            "Epoch 94750 | Loss: 0.8897968530654907 | Test loss: 0.3035399615764618\n",
            "Epoch 94760 | Loss: 0.8887535333633423 | Test loss: 0.3027016222476959\n",
            "Epoch 94770 | Loss: 0.8877100944519043 | Test loss: 0.30186328291893005\n",
            "Epoch 94780 | Loss: 0.8866667151451111 | Test loss: 0.3010249733924866\n",
            "Epoch 94790 | Loss: 0.8856233954429626 | Test loss: 0.30018654465675354\n",
            "Epoch 94800 | Loss: 0.8845798373222351 | Test loss: 0.29934820532798767\n",
            "Epoch 94810 | Loss: 0.8835365176200867 | Test loss: 0.2985098958015442\n",
            "Epoch 94820 | Loss: 0.8824931383132935 | Test loss: 0.29767152667045593\n",
            "Epoch 94830 | Loss: 0.8814496994018555 | Test loss: 0.29683318734169006\n",
            "Epoch 94840 | Loss: 0.8804063200950623 | Test loss: 0.2959948480129242\n",
            "Epoch 94850 | Loss: 0.8793630003929138 | Test loss: 0.29515644907951355\n",
            "Epoch 94860 | Loss: 0.8783195614814758 | Test loss: 0.29431816935539246\n",
            "Epoch 94870 | Loss: 0.8772761225700378 | Test loss: 0.2934797406196594\n",
            "Epoch 94880 | Loss: 0.8762327432632446 | Test loss: 0.29264146089553833\n",
            "Epoch 94890 | Loss: 0.8751894235610962 | Test loss: 0.2918030917644501\n",
            "Epoch 94900 | Loss: 0.8741459846496582 | Test loss: 0.290964812040329\n",
            "Epoch 94910 | Loss: 0.873102605342865 | Test loss: 0.29012641310691833\n",
            "Epoch 94920 | Loss: 0.8720592856407166 | Test loss: 0.2892880439758301\n",
            "Epoch 94930 | Loss: 0.8710158467292786 | Test loss: 0.2884496748447418\n",
            "Epoch 94940 | Loss: 0.8699724078178406 | Test loss: 0.28761133551597595\n",
            "Epoch 94950 | Loss: 0.8689290881156921 | Test loss: 0.2867729365825653\n",
            "Epoch 94960 | Loss: 0.8678857088088989 | Test loss: 0.2859346270561218\n",
            "Epoch 94970 | Loss: 0.8668422698974609 | Test loss: 0.28509625792503357\n",
            "Epoch 94980 | Loss: 0.8657988905906677 | Test loss: 0.2842579483985901\n",
            "Epoch 94990 | Loss: 0.8647555708885193 | Test loss: 0.28341951966285706\n",
            "Epoch 95000 | Loss: 0.8637121319770813 | Test loss: 0.2825811505317688\n",
            "Epoch 95010 | Loss: 0.8626686334609985 | Test loss: 0.2817428708076477\n",
            "Epoch 95020 | Loss: 0.8616253137588501 | Test loss: 0.28090453147888184\n",
            "Epoch 95030 | Loss: 0.8605818748474121 | Test loss: 0.2800661623477936\n",
            "Epoch 95040 | Loss: 0.8595384955406189 | Test loss: 0.2792278230190277\n",
            "Epoch 95050 | Loss: 0.8584951758384705 | Test loss: 0.27838942408561707\n",
            "Epoch 95060 | Loss: 0.8574517369270325 | Test loss: 0.27755114436149597\n",
            "Epoch 95070 | Loss: 0.8564082980155945 | Test loss: 0.27671271562576294\n",
            "Epoch 95080 | Loss: 0.855364978313446 | Test loss: 0.27587443590164185\n",
            "Epoch 95090 | Loss: 0.8543215990066528 | Test loss: 0.2750360667705536\n",
            "Epoch 95100 | Loss: 0.8532781600952148 | Test loss: 0.2741977870464325\n",
            "Epoch 95110 | Loss: 0.8522348403930664 | Test loss: 0.27335938811302185\n",
            "Epoch 95120 | Loss: 0.8511913418769836 | Test loss: 0.2725210189819336\n",
            "Epoch 95130 | Loss: 0.8501480221748352 | Test loss: 0.27168264985084534\n",
            "Epoch 95140 | Loss: 0.8491045832633972 | Test loss: 0.27084431052207947\n",
            "Epoch 95150 | Loss: 0.848061203956604 | Test loss: 0.2700059115886688\n",
            "Epoch 95160 | Loss: 0.8470178842544556 | Test loss: 0.26916760206222534\n",
            "Epoch 95170 | Loss: 0.8459744453430176 | Test loss: 0.2683292329311371\n",
            "Epoch 95180 | Loss: 0.8449310660362244 | Test loss: 0.2674909234046936\n",
            "Epoch 95190 | Loss: 0.8438877463340759 | Test loss: 0.26665249466896057\n",
            "Epoch 95200 | Loss: 0.8428441882133484 | Test loss: 0.2658141255378723\n",
            "Epoch 95210 | Loss: 0.8418008685112 | Test loss: 0.2649758756160736\n",
            "Epoch 95220 | Loss: 0.8407574892044067 | Test loss: 0.26413750648498535\n",
            "Epoch 95230 | Loss: 0.8397140502929688 | Test loss: 0.2632991373538971\n",
            "Epoch 95240 | Loss: 0.8386706709861755 | Test loss: 0.2624607980251312\n",
            "Epoch 95250 | Loss: 0.8376273512840271 | Test loss: 0.2616223990917206\n",
            "Epoch 95260 | Loss: 0.8365839123725891 | Test loss: 0.2607841193675995\n",
            "Epoch 95270 | Loss: 0.8355404734611511 | Test loss: 0.25994569063186646\n",
            "Epoch 95280 | Loss: 0.8344970941543579 | Test loss: 0.259107381105423\n",
            "Epoch 95290 | Loss: 0.8334537744522095 | Test loss: 0.2582690119743347\n",
            "Epoch 95300 | Loss: 0.8324103355407715 | Test loss: 0.2574307322502136\n",
            "Epoch 95310 | Loss: 0.8313669562339783 | Test loss: 0.25659236311912537\n",
            "Epoch 95320 | Loss: 0.8303236365318298 | Test loss: 0.2557539939880371\n",
            "Epoch 95330 | Loss: 0.8292801976203918 | Test loss: 0.25491562485694885\n",
            "Epoch 95340 | Loss: 0.8282367587089539 | Test loss: 0.254077285528183\n",
            "Epoch 95350 | Loss: 0.8271934390068054 | Test loss: 0.25323888659477234\n",
            "Epoch 95360 | Loss: 0.8261500597000122 | Test loss: 0.25240054726600647\n",
            "Epoch 95370 | Loss: 0.8251066207885742 | Test loss: 0.2515622079372406\n",
            "Epoch 95380 | Loss: 0.824063241481781 | Test loss: 0.2507238984107971\n",
            "Epoch 95390 | Loss: 0.8230199217796326 | Test loss: 0.2498854696750641\n",
            "Epoch 95400 | Loss: 0.8219764828681946 | Test loss: 0.24904708564281464\n",
            "Epoch 95410 | Loss: 0.8209329843521118 | Test loss: 0.24820883572101593\n",
            "Epoch 95420 | Loss: 0.8198896646499634 | Test loss: 0.24737046658992767\n",
            "Epoch 95430 | Loss: 0.8188462257385254 | Test loss: 0.2465321123600006\n",
            "Epoch 95440 | Loss: 0.8178028464317322 | Test loss: 0.24569375813007355\n",
            "Epoch 95450 | Loss: 0.8167595267295837 | Test loss: 0.2448553889989853\n",
            "Epoch 95460 | Loss: 0.8157160878181458 | Test loss: 0.2440170794725418\n",
            "Epoch 95470 | Loss: 0.8146726489067078 | Test loss: 0.24317865073680878\n",
            "Epoch 95480 | Loss: 0.8136293292045593 | Test loss: 0.24234037101268768\n",
            "Epoch 95490 | Loss: 0.8125859498977661 | Test loss: 0.24150197207927704\n",
            "Epoch 95500 | Loss: 0.8115425109863281 | Test loss: 0.24066369235515594\n",
            "Epoch 95510 | Loss: 0.8104991912841797 | Test loss: 0.2398253232240677\n",
            "Epoch 95520 | Loss: 0.8094556927680969 | Test loss: 0.23898696899414062\n",
            "Epoch 95530 | Loss: 0.8084123730659485 | Test loss: 0.23814859986305237\n",
            "Epoch 95540 | Loss: 0.8073689341545105 | Test loss: 0.2373102456331253\n",
            "Epoch 95550 | Loss: 0.8063255548477173 | Test loss: 0.23647187650203705\n",
            "Epoch 95560 | Loss: 0.8052822351455688 | Test loss: 0.23563353717327118\n",
            "Epoch 95570 | Loss: 0.8042387962341309 | Test loss: 0.23479513823986053\n",
            "Epoch 95580 | Loss: 0.8031954169273376 | Test loss: 0.23395685851573944\n",
            "Epoch 95590 | Loss: 0.8021519780158997 | Test loss: 0.2331184595823288\n",
            "Epoch 95600 | Loss: 0.8011085391044617 | Test loss: 0.23228004574775696\n",
            "Epoch 95610 | Loss: 0.8000652194023132 | Test loss: 0.23144181072711945\n",
            "Epoch 95620 | Loss: 0.79902184009552 | Test loss: 0.23060345649719238\n",
            "Epoch 95630 | Loss: 0.797978401184082 | Test loss: 0.22976505756378174\n",
            "Epoch 95640 | Loss: 0.7969350218772888 | Test loss: 0.22892676293849945\n",
            "Epoch 95650 | Loss: 0.7958917021751404 | Test loss: 0.22808833420276642\n",
            "Epoch 95660 | Loss: 0.7948482632637024 | Test loss: 0.22725005447864532\n",
            "Epoch 95670 | Loss: 0.7938048243522644 | Test loss: 0.2264116257429123\n",
            "Epoch 95680 | Loss: 0.792761504650116 | Test loss: 0.2255733460187912\n",
            "Epoch 95690 | Loss: 0.7917181253433228 | Test loss: 0.22473497688770294\n",
            "Epoch 95700 | Loss: 0.7906746864318848 | Test loss: 0.22389665246009827\n",
            "Epoch 95710 | Loss: 0.7896313071250916 | Test loss: 0.2230582982301712\n",
            "Epoch 95720 | Loss: 0.7885879278182983 | Test loss: 0.22221994400024414\n",
            "Epoch 95730 | Loss: 0.7875445485115051 | Test loss: 0.22138157486915588\n",
            "Epoch 95740 | Loss: 0.7865011096000671 | Test loss: 0.2205432504415512\n",
            "Epoch 95750 | Loss: 0.7854577898979187 | Test loss: 0.21970482170581818\n",
            "Epoch 95760 | Loss: 0.7844144105911255 | Test loss: 0.2188665121793747\n",
            "Epoch 95770 | Loss: 0.7833709716796875 | Test loss: 0.21802811324596405\n",
            "Epoch 95780 | Loss: 0.7823275923728943 | Test loss: 0.21718983352184296\n",
            "Epoch 95790 | Loss: 0.7812841534614563 | Test loss: 0.2163514643907547\n",
            "Epoch 95800 | Loss: 0.7802407741546631 | Test loss: 0.21551299095153809\n",
            "Epoch 95810 | Loss: 0.7791973948478699 | Test loss: 0.21467475593090057\n",
            "Epoch 95820 | Loss: 0.7781539559364319 | Test loss: 0.2138364166021347\n",
            "Epoch 95830 | Loss: 0.7771105766296387 | Test loss: 0.21299801766872406\n",
            "Epoch 95840 | Loss: 0.7760671973228455 | Test loss: 0.21215973794460297\n",
            "Epoch 95850 | Loss: 0.775023877620697 | Test loss: 0.21132130920886993\n",
            "Epoch 95860 | Loss: 0.773980438709259 | Test loss: 0.21048302948474884\n",
            "Epoch 95870 | Loss: 0.772936999797821 | Test loss: 0.2096446007490158\n",
            "Epoch 95880 | Loss: 0.7718936800956726 | Test loss: 0.20880632102489471\n",
            "Epoch 95890 | Loss: 0.7708503007888794 | Test loss: 0.20796795189380646\n",
            "Epoch 95900 | Loss: 0.7698068618774414 | Test loss: 0.20712962746620178\n",
            "Epoch 95910 | Loss: 0.7687634825706482 | Test loss: 0.20629127323627472\n",
            "Epoch 95920 | Loss: 0.767720103263855 | Test loss: 0.20545291900634766\n",
            "Epoch 95930 | Loss: 0.7666767239570618 | Test loss: 0.2046145498752594\n",
            "Epoch 95940 | Loss: 0.7656332850456238 | Test loss: 0.20377622544765472\n",
            "Epoch 95950 | Loss: 0.7645899653434753 | Test loss: 0.20293782651424408\n",
            "Epoch 95960 | Loss: 0.7635465860366821 | Test loss: 0.2020995169878006\n",
            "Epoch 95970 | Loss: 0.7625031471252441 | Test loss: 0.20126111805438995\n",
            "Epoch 95980 | Loss: 0.7614597678184509 | Test loss: 0.20042280852794647\n",
            "Epoch 95990 | Loss: 0.7604163289070129 | Test loss: 0.19958440959453583\n",
            "Epoch 96000 | Loss: 0.7593729496002197 | Test loss: 0.1987459510564804\n",
            "Epoch 96010 | Loss: 0.7583295702934265 | Test loss: 0.19790776073932648\n",
            "Epoch 96020 | Loss: 0.7572861909866333 | Test loss: 0.19706939160823822\n",
            "Epoch 96030 | Loss: 0.7562427520751953 | Test loss: 0.19623100757598877\n",
            "Epoch 96040 | Loss: 0.7551993727684021 | Test loss: 0.1953926980495453\n",
            "Epoch 96050 | Loss: 0.7541559934616089 | Test loss: 0.19455429911613464\n",
            "Epoch 96060 | Loss: 0.7531126141548157 | Test loss: 0.19371600449085236\n",
            "Epoch 96070 | Loss: 0.7520691752433777 | Test loss: 0.19287759065628052\n",
            "Epoch 96080 | Loss: 0.7510258555412292 | Test loss: 0.19203929603099823\n",
            "Epoch 96090 | Loss: 0.749982476234436 | Test loss: 0.19120089709758759\n",
            "Epoch 96100 | Loss: 0.748939037322998 | Test loss: 0.1903626024723053\n",
            "Epoch 96110 | Loss: 0.7478956580162048 | Test loss: 0.18952426314353943\n",
            "Epoch 96120 | Loss: 0.7468522787094116 | Test loss: 0.18868589401245117\n",
            "Epoch 96130 | Loss: 0.7458088994026184 | Test loss: 0.1878475397825241\n",
            "Epoch 96140 | Loss: 0.7447654604911804 | Test loss: 0.18700920045375824\n",
            "Epoch 96150 | Loss: 0.743722140789032 | Test loss: 0.1861708015203476\n",
            "Epoch 96160 | Loss: 0.7426787614822388 | Test loss: 0.18533246219158173\n",
            "Epoch 96170 | Loss: 0.7416353225708008 | Test loss: 0.18449410796165466\n",
            "Epoch 96180 | Loss: 0.7405919432640076 | Test loss: 0.18365578353405\n",
            "Epoch 96190 | Loss: 0.7395485043525696 | Test loss: 0.18281738460063934\n",
            "Epoch 96200 | Loss: 0.7385051250457764 | Test loss: 0.18197892606258392\n",
            "Epoch 96210 | Loss: 0.7374617457389832 | Test loss: 0.1811407506465912\n",
            "Epoch 96220 | Loss: 0.7364183068275452 | Test loss: 0.18030236661434174\n",
            "Epoch 96230 | Loss: 0.735374927520752 | Test loss: 0.17946398258209229\n",
            "Epoch 96240 | Loss: 0.7343315482139587 | Test loss: 0.1786256581544876\n",
            "Epoch 96250 | Loss: 0.7332881689071655 | Test loss: 0.17778728902339935\n",
            "Epoch 96260 | Loss: 0.7322447896003723 | Test loss: 0.17694897949695587\n",
            "Epoch 96270 | Loss: 0.7312013506889343 | Test loss: 0.17611056566238403\n",
            "Epoch 96280 | Loss: 0.7301580309867859 | Test loss: 0.17527225613594055\n",
            "Epoch 96290 | Loss: 0.7291146516799927 | Test loss: 0.1744338870048523\n",
            "Epoch 96300 | Loss: 0.7280712127685547 | Test loss: 0.1735955774784088\n",
            "Epoch 96310 | Loss: 0.7270278334617615 | Test loss: 0.17275723814964294\n",
            "Epoch 96320 | Loss: 0.7259844541549683 | Test loss: 0.1719188541173935\n",
            "Epoch 96330 | Loss: 0.724941074848175 | Test loss: 0.17108052968978882\n",
            "Epoch 96340 | Loss: 0.7238976359367371 | Test loss: 0.17024217545986176\n",
            "Epoch 96350 | Loss: 0.7228543162345886 | Test loss: 0.16940376162528992\n",
            "Epoch 96360 | Loss: 0.7218109369277954 | Test loss: 0.16856543719768524\n",
            "Epoch 96370 | Loss: 0.7207674980163574 | Test loss: 0.16772708296775818\n",
            "Epoch 96380 | Loss: 0.7197241187095642 | Test loss: 0.1668887734413147\n",
            "Epoch 96390 | Loss: 0.7186806797981262 | Test loss: 0.16605035960674286\n",
            "Epoch 96400 | Loss: 0.717637300491333 | Test loss: 0.16521188616752625\n",
            "Epoch 96410 | Loss: 0.7165939211845398 | Test loss: 0.1643737256526947\n",
            "Epoch 96420 | Loss: 0.7155505418777466 | Test loss: 0.16353534162044525\n",
            "Epoch 96430 | Loss: 0.7145071029663086 | Test loss: 0.162696972489357\n",
            "Epoch 96440 | Loss: 0.7134637236595154 | Test loss: 0.16185863316059113\n",
            "Epoch 96450 | Loss: 0.7124203443527222 | Test loss: 0.16102024912834167\n",
            "Epoch 96460 | Loss: 0.711376965045929 | Test loss: 0.1601819545030594\n",
            "Epoch 96470 | Loss: 0.710333526134491 | Test loss: 0.15934354066848755\n",
            "Epoch 96480 | Loss: 0.7092902064323425 | Test loss: 0.15850524604320526\n",
            "Epoch 96490 | Loss: 0.7082468271255493 | Test loss: 0.1576668620109558\n",
            "Epoch 96500 | Loss: 0.7072033882141113 | Test loss: 0.15682853758335114\n",
            "Epoch 96510 | Loss: 0.7061600089073181 | Test loss: 0.15599022805690765\n",
            "Epoch 96520 | Loss: 0.7051166296005249 | Test loss: 0.155151829123497\n",
            "Epoch 96530 | Loss: 0.7040732502937317 | Test loss: 0.15431351959705353\n",
            "Epoch 96540 | Loss: 0.7030298113822937 | Test loss: 0.15347515046596527\n",
            "Epoch 96550 | Loss: 0.7019864916801453 | Test loss: 0.15263675153255463\n",
            "Epoch 96560 | Loss: 0.700943112373352 | Test loss: 0.15179841220378876\n",
            "Epoch 96570 | Loss: 0.6998996734619141 | Test loss: 0.1509600579738617\n",
            "Epoch 96580 | Loss: 0.6988562941551208 | Test loss: 0.15012173354625702\n",
            "Epoch 96590 | Loss: 0.6978128552436829 | Test loss: 0.14928333461284637\n",
            "Epoch 96600 | Loss: 0.6967694759368896 | Test loss: 0.14844484627246857\n",
            "Epoch 96610 | Loss: 0.6957260966300964 | Test loss: 0.14760670065879822\n",
            "Epoch 96620 | Loss: 0.6946826577186584 | Test loss: 0.14676833152770996\n",
            "Epoch 96630 | Loss: 0.6936392784118652 | Test loss: 0.1459299474954605\n",
            "Epoch 96640 | Loss: 0.692595899105072 | Test loss: 0.14509162306785583\n",
            "Epoch 96650 | Loss: 0.6915525197982788 | Test loss: 0.1442532241344452\n",
            "Epoch 96660 | Loss: 0.6905091404914856 | Test loss: 0.1434149295091629\n",
            "Epoch 96670 | Loss: 0.6894657015800476 | Test loss: 0.14257650077342987\n",
            "Epoch 96680 | Loss: 0.6884223818778992 | Test loss: 0.14173822104930878\n",
            "Epoch 96690 | Loss: 0.687379002571106 | Test loss: 0.14089982211589813\n",
            "Epoch 96700 | Loss: 0.686335563659668 | Test loss: 0.14006149768829346\n",
            "Epoch 96710 | Loss: 0.6852921843528748 | Test loss: 0.13922320306301117\n",
            "Epoch 96720 | Loss: 0.6842488050460815 | Test loss: 0.13838480412960052\n",
            "Epoch 96730 | Loss: 0.6832054257392883 | Test loss: 0.13754650950431824\n",
            "Epoch 96740 | Loss: 0.6821619868278503 | Test loss: 0.1367081105709076\n",
            "Epoch 96750 | Loss: 0.6811186671257019 | Test loss: 0.13586972653865814\n",
            "Epoch 96760 | Loss: 0.6800752878189087 | Test loss: 0.13503138720989227\n",
            "Epoch 96770 | Loss: 0.6790318489074707 | Test loss: 0.13419301807880402\n",
            "Epoch 96780 | Loss: 0.6779884696006775 | Test loss: 0.13335470855236053\n",
            "Epoch 96790 | Loss: 0.6769450306892395 | Test loss: 0.1325163096189499\n",
            "Epoch 96800 | Loss: 0.6759016513824463 | Test loss: 0.1316778063774109\n",
            "Epoch 96810 | Loss: 0.6748582720756531 | Test loss: 0.13083967566490173\n",
            "Epoch 96820 | Loss: 0.6738148927688599 | Test loss: 0.13000130653381348\n",
            "Epoch 96830 | Loss: 0.6727714538574219 | Test loss: 0.12916293740272522\n",
            "Epoch 96840 | Loss: 0.6717280745506287 | Test loss: 0.12832458317279816\n",
            "Epoch 96850 | Loss: 0.6706846952438354 | Test loss: 0.1274861991405487\n",
            "Epoch 96860 | Loss: 0.6696413159370422 | Test loss: 0.1266479194164276\n",
            "Epoch 96870 | Loss: 0.6685978770256042 | Test loss: 0.1258094608783722\n",
            "Epoch 96880 | Loss: 0.6675545573234558 | Test loss: 0.1249711737036705\n",
            "Epoch 96890 | Loss: 0.6665111780166626 | Test loss: 0.12413280457258224\n",
            "Epoch 96900 | Loss: 0.6654677391052246 | Test loss: 0.12329447269439697\n",
            "Epoch 96910 | Loss: 0.6644243597984314 | Test loss: 0.12245617061853409\n",
            "Epoch 96920 | Loss: 0.6633809804916382 | Test loss: 0.12161779403686523\n",
            "Epoch 96930 | Loss: 0.662337601184845 | Test loss: 0.12077949196100235\n",
            "Epoch 96940 | Loss: 0.661294162273407 | Test loss: 0.11994107067584991\n",
            "Epoch 96950 | Loss: 0.6602508425712585 | Test loss: 0.11910269409418106\n",
            "Epoch 96960 | Loss: 0.6592074632644653 | Test loss: 0.11826436966657639\n",
            "Epoch 96970 | Loss: 0.6581640243530273 | Test loss: 0.11742599308490753\n",
            "Epoch 96980 | Loss: 0.6571206450462341 | Test loss: 0.11658769100904465\n",
            "Epoch 96990 | Loss: 0.6560772061347961 | Test loss: 0.115749292075634\n",
            "Epoch 97000 | Loss: 0.6550338268280029 | Test loss: 0.11491077393293381\n",
            "Epoch 97010 | Loss: 0.6539904475212097 | Test loss: 0.11407263576984406\n",
            "Epoch 97020 | Loss: 0.6529470086097717 | Test loss: 0.11323428153991699\n",
            "Epoch 97030 | Loss: 0.6519036293029785 | Test loss: 0.11239590495824814\n",
            "Epoch 97040 | Loss: 0.6508602499961853 | Test loss: 0.11155755817890167\n",
            "Epoch 97050 | Loss: 0.6498168706893921 | Test loss: 0.11071918159723282\n",
            "Epoch 97060 | Loss: 0.6487734913825989 | Test loss: 0.10988090187311172\n",
            "Epoch 97070 | Loss: 0.6477300524711609 | Test loss: 0.10904242843389511\n",
            "Epoch 97080 | Loss: 0.6466867327690125 | Test loss: 0.10820414870977402\n",
            "Epoch 97090 | Loss: 0.6456433534622192 | Test loss: 0.10736577957868576\n",
            "Epoch 97100 | Loss: 0.6445999145507812 | Test loss: 0.10652744770050049\n",
            "Epoch 97110 | Loss: 0.643556535243988 | Test loss: 0.1056891456246376\n",
            "Epoch 97120 | Loss: 0.6425131559371948 | Test loss: 0.10485076904296875\n",
            "Epoch 97130 | Loss: 0.6414697766304016 | Test loss: 0.10405099391937256\n",
            "Epoch 97140 | Loss: 0.6404263377189636 | Test loss: 0.10325253009796143\n",
            "Epoch 97150 | Loss: 0.6393830180168152 | Test loss: 0.10245406627655029\n",
            "Epoch 97160 | Loss: 0.638339638710022 | Test loss: 0.10165565460920334\n",
            "Epoch 97170 | Loss: 0.637296199798584 | Test loss: 0.1008571907877922\n",
            "Epoch 97180 | Loss: 0.6362528204917908 | Test loss: 0.10005879402160645\n",
            "Epoch 97190 | Loss: 0.6352093815803528 | Test loss: 0.09927032142877579\n",
            "Epoch 97200 | Loss: 0.6341660022735596 | Test loss: 0.09851181507110596\n",
            "Epoch 97210 | Loss: 0.6331226229667664 | Test loss: 0.0977535992860794\n",
            "Epoch 97220 | Loss: 0.6320792436599731 | Test loss: 0.09699516743421555\n",
            "Epoch 97230 | Loss: 0.6310358047485352 | Test loss: 0.0962367057800293\n",
            "Epoch 97240 | Loss: 0.6299924254417419 | Test loss: 0.09547831863164902\n",
            "Epoch 97250 | Loss: 0.6289490461349487 | Test loss: 0.09471986442804337\n",
            "Epoch 97260 | Loss: 0.6279056668281555 | Test loss: 0.09398434311151505\n",
            "Epoch 97270 | Loss: 0.6268622279167175 | Test loss: 0.09326603263616562\n",
            "Epoch 97280 | Loss: 0.6258189082145691 | Test loss: 0.09254777431488037\n",
            "Epoch 97290 | Loss: 0.6247755289077759 | Test loss: 0.09182947129011154\n",
            "Epoch 97300 | Loss: 0.6237320899963379 | Test loss: 0.0911112055182457\n",
            "Epoch 97310 | Loss: 0.6226887106895447 | Test loss: 0.09039292484521866\n",
            "Epoch 97320 | Loss: 0.6216453313827515 | Test loss: 0.08967461436986923\n",
            "Epoch 97330 | Loss: 0.6206019520759583 | Test loss: 0.0889933854341507\n",
            "Epoch 97340 | Loss: 0.6195585131645203 | Test loss: 0.08831536769866943\n",
            "Epoch 97350 | Loss: 0.6185151934623718 | Test loss: 0.08763726055622101\n",
            "Epoch 97360 | Loss: 0.6174718141555786 | Test loss: 0.08695919811725616\n",
            "Epoch 97370 | Loss: 0.6164283752441406 | Test loss: 0.08628113567829132\n",
            "Epoch 97380 | Loss: 0.6153849959373474 | Test loss: 0.08560309559106827\n",
            "Epoch 97390 | Loss: 0.6143415570259094 | Test loss: 0.08493735641241074\n",
            "Epoch 97400 | Loss: 0.6132981777191162 | Test loss: 0.08429958671331406\n",
            "Epoch 97410 | Loss: 0.612254798412323 | Test loss: 0.08366196602582932\n",
            "Epoch 97420 | Loss: 0.611211359500885 | Test loss: 0.08302421867847443\n",
            "Epoch 97430 | Loss: 0.6101679801940918 | Test loss: 0.08238644897937775\n",
            "Epoch 97440 | Loss: 0.6091246008872986 | Test loss: 0.08174874633550644\n",
            "Epoch 97450 | Loss: 0.6080812811851501 | Test loss: 0.08111097663640976\n",
            "Epoch 97460 | Loss: 0.6070378422737122 | Test loss: 0.08050268143415451\n",
            "Epoch 97470 | Loss: 0.6059944033622742 | Test loss: 0.07990539073944092\n",
            "Epoch 97480 | Loss: 0.6049510836601257 | Test loss: 0.07930810749530792\n",
            "Epoch 97490 | Loss: 0.6039077043533325 | Test loss: 0.07871077209711075\n",
            "Epoch 97500 | Loss: 0.6028642654418945 | Test loss: 0.07811353355646133\n",
            "Epoch 97510 | Loss: 0.6018209457397461 | Test loss: 0.07751617580652237\n",
            "Epoch 97520 | Loss: 0.6007775068283081 | Test loss: 0.07692618668079376\n",
            "Epoch 97530 | Loss: 0.5997341275215149 | Test loss: 0.07636936008930206\n",
            "Epoch 97540 | Loss: 0.5986906886100769 | Test loss: 0.07581265270709991\n",
            "Epoch 97550 | Loss: 0.5976473689079285 | Test loss: 0.07525584846735\n",
            "Epoch 97560 | Loss: 0.5966039896011353 | Test loss: 0.07469909638166428\n",
            "Epoch 97570 | Loss: 0.5955605506896973 | Test loss: 0.07414229214191437\n",
            "Epoch 97580 | Loss: 0.594517171382904 | Test loss: 0.07358556240797043\n",
            "Epoch 97590 | Loss: 0.5934737324714661 | Test loss: 0.07305586338043213\n",
            "Epoch 97600 | Loss: 0.5924303531646729 | Test loss: 0.07253973931074142\n",
            "Epoch 97610 | Loss: 0.5913869738578796 | Test loss: 0.0720236524939537\n",
            "Epoch 97620 | Loss: 0.5903435945510864 | Test loss: 0.07150750607252121\n",
            "Epoch 97630 | Loss: 0.5893001556396484 | Test loss: 0.07099132984876633\n",
            "Epoch 97640 | Loss: 0.5882567763328552 | Test loss: 0.07047522068023682\n",
            "Epoch 97650 | Loss: 0.5872134566307068 | Test loss: 0.0699666291475296\n",
            "Epoch 97660 | Loss: 0.5861700177192688 | Test loss: 0.06949126720428467\n",
            "Epoch 97670 | Loss: 0.5851265788078308 | Test loss: 0.06901586055755615\n",
            "Epoch 97680 | Loss: 0.5840832591056824 | Test loss: 0.06854045391082764\n",
            "Epoch 97690 | Loss: 0.5830398797988892 | Test loss: 0.06806504726409912\n",
            "Epoch 97700 | Loss: 0.5819964408874512 | Test loss: 0.06758973747491837\n",
            "Epoch 97710 | Loss: 0.5809531211853027 | Test loss: 0.06711430847644806\n",
            "Epoch 97720 | Loss: 0.5799096822738647 | Test loss: 0.0666690394282341\n",
            "Epoch 97730 | Loss: 0.5788663029670715 | Test loss: 0.06623437255620956\n",
            "Epoch 97740 | Loss: 0.5778228640556335 | Test loss: 0.06579992920160294\n",
            "Epoch 97750 | Loss: 0.5767795443534851 | Test loss: 0.06536533683538437\n",
            "Epoch 97760 | Loss: 0.5757361054420471 | Test loss: 0.06493079662322998\n",
            "Epoch 97770 | Loss: 0.5746927261352539 | Test loss: 0.06449621170759201\n",
            "Epoch 97780 | Loss: 0.5736493468284607 | Test loss: 0.06407480686903\n",
            "Epoch 97790 | Loss: 0.5726059079170227 | Test loss: 0.06368114799261093\n",
            "Epoch 97800 | Loss: 0.5715625286102295 | Test loss: 0.06328761577606201\n",
            "Epoch 97810 | Loss: 0.5705191493034363 | Test loss: 0.06289398670196533\n",
            "Epoch 97820 | Loss: 0.5694757699966431 | Test loss: 0.06250037997961044\n",
            "Epoch 97830 | Loss: 0.5684323310852051 | Test loss: 0.06210675463080406\n",
            "Epoch 97840 | Loss: 0.5673889517784119 | Test loss: 0.06171314790844917\n",
            "Epoch 97850 | Loss: 0.5663456320762634 | Test loss: 0.061358001083135605\n",
            "Epoch 97860 | Loss: 0.5653021931648254 | Test loss: 0.061005473136901855\n",
            "Epoch 97870 | Loss: 0.5642588138580322 | Test loss: 0.060652852058410645\n",
            "Epoch 97880 | Loss: 0.563215434551239 | Test loss: 0.060300327837467194\n",
            "Epoch 97890 | Loss: 0.5621720552444458 | Test loss: 0.05994775518774986\n",
            "Epoch 97900 | Loss: 0.5611286163330078 | Test loss: 0.05959527567028999\n",
            "Epoch 97910 | Loss: 0.5600852966308594 | Test loss: 0.05926666408777237\n",
            "Epoch 97920 | Loss: 0.5590418577194214 | Test loss: 0.05895524099469185\n",
            "Epoch 97930 | Loss: 0.5579984784126282 | Test loss: 0.05864374712109566\n",
            "Epoch 97940 | Loss: 0.5569550395011902 | Test loss: 0.05833246931433678\n",
            "Epoch 97950 | Loss: 0.5559117197990417 | Test loss: 0.058020997792482376\n",
            "Epoch 97960 | Loss: 0.5548682808876038 | Test loss: 0.057709574699401855\n",
            "Epoch 97970 | Loss: 0.5538249015808105 | Test loss: 0.057409025728702545\n",
            "Epoch 97980 | Loss: 0.5527815222740173 | Test loss: 0.057138919830322266\n",
            "Epoch 97990 | Loss: 0.5517380833625793 | Test loss: 0.056868720799684525\n",
            "Epoch 98000 | Loss: 0.5506947040557861 | Test loss: 0.056598640978336334\n",
            "Epoch 98010 | Loss: 0.5496513247489929 | Test loss: 0.056328415870666504\n",
            "Epoch 98020 | Loss: 0.5486079454421997 | Test loss: 0.05605826526880264\n",
            "Epoch 98030 | Loss: 0.5475645065307617 | Test loss: 0.05578808858990669\n",
            "Epoch 98040 | Loss: 0.5465211272239685 | Test loss: 0.055558230727910995\n",
            "Epoch 98050 | Loss: 0.5454778075218201 | Test loss: 0.05532939359545708\n",
            "Epoch 98060 | Loss: 0.5444343686103821 | Test loss: 0.05510056018829346\n",
            "Epoch 98070 | Loss: 0.5433909893035889 | Test loss: 0.05487172678112984\n",
            "Epoch 98080 | Loss: 0.5423476099967957 | Test loss: 0.05464298650622368\n",
            "Epoch 98090 | Loss: 0.5413042306900024 | Test loss: 0.05441415309906006\n",
            "Epoch 98100 | Loss: 0.5402607917785645 | Test loss: 0.054215122014284134\n",
            "Epoch 98110 | Loss: 0.5392174124717712 | Test loss: 0.054027702659368515\n",
            "Epoch 98120 | Loss: 0.538174033164978 | Test loss: 0.05384035035967827\n",
            "Epoch 98130 | Loss: 0.5371306538581848 | Test loss: 0.05365288257598877\n",
            "Epoch 98140 | Loss: 0.5360872149467468 | Test loss: 0.05346563085913658\n",
            "Epoch 98150 | Loss: 0.5350438952445984 | Test loss: 0.053278278559446335\n",
            "Epoch 98160 | Loss: 0.5340004563331604 | Test loss: 0.05311119556427002\n",
            "Epoch 98170 | Loss: 0.5329570770263672 | Test loss: 0.05296538025140762\n",
            "Epoch 98180 | Loss: 0.531913697719574 | Test loss: 0.0528196357190609\n",
            "Epoch 98190 | Loss: 0.530870258808136 | Test loss: 0.05267379432916641\n",
            "Epoch 98200 | Loss: 0.5298268795013428 | Test loss: 0.05252812057733536\n",
            "Epoch 98210 | Loss: 0.5287835001945496 | Test loss: 0.0523822084069252\n",
            "Epoch 98220 | Loss: 0.5277401208877563 | Test loss: 0.052248623222112656\n",
            "Epoch 98230 | Loss: 0.5266967415809631 | Test loss: 0.052144479006528854\n",
            "Epoch 98240 | Loss: 0.5256533026695251 | Test loss: 0.05204036459326744\n",
            "Epoch 98250 | Loss: 0.5246099829673767 | Test loss: 0.05193622037768364\n",
            "Epoch 98260 | Loss: 0.5235665440559387 | Test loss: 0.05183203145861626\n",
            "Epoch 98270 | Loss: 0.5225231647491455 | Test loss: 0.051727939397096634\n",
            "Epoch 98280 | Loss: 0.5214797854423523 | Test loss: 0.051629044115543365\n",
            "Epoch 98290 | Loss: 0.5204364061355591 | Test loss: 0.051566626876592636\n",
            "Epoch 98300 | Loss: 0.5193929672241211 | Test loss: 0.05150435119867325\n",
            "Epoch 98310 | Loss: 0.5183495879173279 | Test loss: 0.05144190788269043\n",
            "Epoch 98320 | Loss: 0.5173062086105347 | Test loss: 0.05137953907251358\n",
            "Epoch 98330 | Loss: 0.5162628293037415 | Test loss: 0.05131709575653076\n",
            "Epoch 98340 | Loss: 0.5152193903923035 | Test loss: 0.051254820078611374\n",
            "Epoch 98350 | Loss: 0.514176070690155 | Test loss: 0.051233887672424316\n",
            "Epoch 98360 | Loss: 0.513132631778717 | Test loss: 0.0512133352458477\n",
            "Epoch 98370 | Loss: 0.5120892524719238 | Test loss: 0.05119283124804497\n",
            "Epoch 98380 | Loss: 0.5110458731651306 | Test loss: 0.05117235332727432\n",
            "Epoch 98390 | Loss: 0.5100024342536926 | Test loss: 0.0511518232524395\n",
            "Epoch 98400 | Loss: 0.5089590549468994 | Test loss: 0.051131464540958405\n",
            "Epoch 98410 | Loss: 0.5079156756401062 | Test loss: 0.05114786699414253\n",
            "Epoch 98420 | Loss: 0.506872296333313 | Test loss: 0.051169347018003464\n",
            "Epoch 98430 | Loss: 0.5058289170265198 | Test loss: 0.051190804690122604\n",
            "Epoch 98440 | Loss: 0.5047854781150818 | Test loss: 0.051212288439273834\n",
            "Epoch 98450 | Loss: 0.5037421584129333 | Test loss: 0.051233794540166855\n",
            "Epoch 98460 | Loss: 0.5026987195014954 | Test loss: 0.051255203783512115\n",
            "Epoch 98470 | Loss: 0.5016553401947021 | Test loss: 0.05131061002612114\n",
            "Epoch 98480 | Loss: 0.5006119608879089 | Test loss: 0.051374126225709915\n",
            "Epoch 98490 | Loss: 0.49956855177879333 | Test loss: 0.05143764242529869\n",
            "Epoch 98500 | Loss: 0.49852514266967773 | Test loss: 0.05150125175714493\n",
            "Epoch 98510 | Loss: 0.4974817931652069 | Test loss: 0.051564741879701614\n",
            "Epoch 98520 | Loss: 0.4964383542537689 | Test loss: 0.05162825807929039\n",
            "Epoch 98530 | Loss: 0.4953950047492981 | Test loss: 0.05172378942370415\n",
            "Epoch 98540 | Loss: 0.4943515956401825 | Test loss: 0.05182955414056778\n",
            "Epoch 98550 | Loss: 0.4933082163333893 | Test loss: 0.05193526670336723\n",
            "Epoch 98560 | Loss: 0.4922648072242737 | Test loss: 0.052040934562683105\n",
            "Epoch 98570 | Loss: 0.49122142791748047 | Test loss: 0.05214665085077286\n",
            "Epoch 98580 | Loss: 0.49017801880836487 | Test loss: 0.0522523894906044\n",
            "Epoch 98590 | Loss: 0.48913460969924927 | Test loss: 0.05238945409655571\n",
            "Epoch 98600 | Loss: 0.48809123039245605 | Test loss: 0.05253760889172554\n",
            "Epoch 98610 | Loss: 0.48704788088798523 | Test loss: 0.05268537998199463\n",
            "Epoch 98620 | Loss: 0.48600444197654724 | Test loss: 0.05283341556787491\n",
            "Epoch 98630 | Loss: 0.4849610924720764 | Test loss: 0.0529814250767231\n",
            "Epoch 98640 | Loss: 0.4839176833629608 | Test loss: 0.0531294122338295\n",
            "Epoch 98650 | Loss: 0.4828743040561676 | Test loss: 0.0533093698322773\n",
            "Epoch 98660 | Loss: 0.481830894947052 | Test loss: 0.053499676287174225\n",
            "Epoch 98670 | Loss: 0.4807875156402588 | Test loss: 0.05369007587432861\n",
            "Epoch 98680 | Loss: 0.4797441065311432 | Test loss: 0.05388043075799942\n",
            "Epoch 98690 | Loss: 0.47870072722435 | Test loss: 0.05407078191637993\n",
            "Epoch 98700 | Loss: 0.4776573181152344 | Test loss: 0.054261185228824615\n",
            "Epoch 98710 | Loss: 0.47661396861076355 | Test loss: 0.05448522791266441\n",
            "Epoch 98720 | Loss: 0.47557052969932556 | Test loss: 0.05471806600689888\n",
            "Epoch 98730 | Loss: 0.47452718019485474 | Test loss: 0.054950930178165436\n",
            "Epoch 98740 | Loss: 0.47348377108573914 | Test loss: 0.05518371984362602\n",
            "Epoch 98750 | Loss: 0.4724403917789459 | Test loss: 0.05541661009192467\n",
            "Epoch 98760 | Loss: 0.4713969826698303 | Test loss: 0.055649399757385254\n",
            "Epoch 98770 | Loss: 0.4703536033630371 | Test loss: 0.05591905117034912\n",
            "Epoch 98780 | Loss: 0.4693101942539215 | Test loss: 0.0561944954097271\n",
            "Epoch 98790 | Loss: 0.4682667851448059 | Test loss: 0.05646989494562149\n",
            "Epoch 98800 | Loss: 0.4672234058380127 | Test loss: 0.0567455068230629\n",
            "Epoch 98810 | Loss: 0.46618005633354187 | Test loss: 0.05702073499560356\n",
            "Epoch 98820 | Loss: 0.4651366174221039 | Test loss: 0.057296182960271835\n",
            "Epoch 98830 | Loss: 0.46409326791763306 | Test loss: 0.05761265754699707\n",
            "Epoch 98840 | Loss: 0.46304985880851746 | Test loss: 0.057930778712034225\n",
            "Epoch 98850 | Loss: 0.46200647950172424 | Test loss: 0.05824892595410347\n",
            "Epoch 98860 | Loss: 0.46096307039260864 | Test loss: 0.058566976338624954\n",
            "Epoch 98870 | Loss: 0.45991969108581543 | Test loss: 0.0588851235806942\n",
            "Epoch 98880 | Loss: 0.45887628197669983 | Test loss: 0.05920694023370743\n",
            "Epoch 98890 | Loss: 0.4578329026699066 | Test loss: 0.059567857533693314\n",
            "Epoch 98900 | Loss: 0.456789493560791 | Test loss: 0.0599287748336792\n",
            "Epoch 98910 | Loss: 0.4557461440563202 | Test loss: 0.060289669781923294\n",
            "Epoch 98920 | Loss: 0.4547027051448822 | Test loss: 0.0606505386531353\n",
            "Epoch 98930 | Loss: 0.4536593556404114 | Test loss: 0.061011530458927155\n",
            "Epoch 98940 | Loss: 0.4526159465312958 | Test loss: 0.06138269975781441\n",
            "Epoch 98950 | Loss: 0.45157256722450256 | Test loss: 0.061786483973264694\n",
            "Epoch 98960 | Loss: 0.45052915811538696 | Test loss: 0.06219027191400528\n",
            "Epoch 98970 | Loss: 0.44948577880859375 | Test loss: 0.06259405612945557\n",
            "Epoch 98980 | Loss: 0.44844236969947815 | Test loss: 0.06299787014722824\n",
            "Epoch 98990 | Loss: 0.44739896059036255 | Test loss: 0.06340167671442032\n",
            "Epoch 99000 | Loss: 0.44635558128356934 | Test loss: 0.06382375210523605\n",
            "Epoch 99010 | Loss: 0.4453122317790985 | Test loss: 0.06427037715911865\n",
            "Epoch 99020 | Loss: 0.4442687928676605 | Test loss: 0.06471719592809677\n",
            "Epoch 99030 | Loss: 0.4432254433631897 | Test loss: 0.0651639923453331\n",
            "Epoch 99040 | Loss: 0.4421820342540741 | Test loss: 0.06561081856489182\n",
            "Epoch 99050 | Loss: 0.4411386549472809 | Test loss: 0.06605761498212814\n",
            "Epoch 99060 | Loss: 0.4400952458381653 | Test loss: 0.06653159111738205\n",
            "Epoch 99070 | Loss: 0.43905186653137207 | Test loss: 0.06702148914337158\n",
            "Epoch 99080 | Loss: 0.43800845742225647 | Test loss: 0.06751134246587753\n",
            "Epoch 99090 | Loss: 0.43696507811546326 | Test loss: 0.06800124794244766\n",
            "Epoch 99100 | Loss: 0.43592166900634766 | Test loss: 0.0684911236166954\n",
            "Epoch 99110 | Loss: 0.43487831950187683 | Test loss: 0.06898102909326553\n",
            "Epoch 99120 | Loss: 0.43383488059043884 | Test loss: 0.06950845569372177\n",
            "Epoch 99130 | Loss: 0.432791531085968 | Test loss: 0.07004153728485107\n",
            "Epoch 99140 | Loss: 0.4317481219768524 | Test loss: 0.07057464122772217\n",
            "Epoch 99150 | Loss: 0.4307047426700592 | Test loss: 0.07110774517059326\n",
            "Epoch 99160 | Loss: 0.4296613335609436 | Test loss: 0.07164080440998077\n",
            "Epoch 99170 | Loss: 0.4286179542541504 | Test loss: 0.07217967510223389\n",
            "Epoch 99180 | Loss: 0.4275745451450348 | Test loss: 0.07275605201721191\n",
            "Epoch 99190 | Loss: 0.4265311360359192 | Test loss: 0.07333242893218994\n",
            "Epoch 99200 | Loss: 0.425487756729126 | Test loss: 0.07390894740819931\n",
            "Epoch 99210 | Loss: 0.42444440722465515 | Test loss: 0.074485182762146\n",
            "Epoch 99220 | Loss: 0.42340096831321716 | Test loss: 0.0750616118311882\n",
            "Epoch 99230 | Loss: 0.42235761880874634 | Test loss: 0.07565641403198242\n",
            "Epoch 99240 | Loss: 0.42131420969963074 | Test loss: 0.07627623528242111\n",
            "Epoch 99250 | Loss: 0.4202708303928375 | Test loss: 0.07689602673053741\n",
            "Epoch 99260 | Loss: 0.4192274212837219 | Test loss: 0.07751581817865372\n",
            "Epoch 99270 | Loss: 0.4181840419769287 | Test loss: 0.0781356617808342\n",
            "Epoch 99280 | Loss: 0.4171406328678131 | Test loss: 0.07875540107488632\n",
            "Epoch 99290 | Loss: 0.4160972535610199 | Test loss: 0.07940752804279327\n",
            "Epoch 99300 | Loss: 0.4150538444519043 | Test loss: 0.08007073402404785\n",
            "Epoch 99310 | Loss: 0.41401049494743347 | Test loss: 0.08073406666517258\n",
            "Epoch 99320 | Loss: 0.4129670560359955 | Test loss: 0.0813973918557167\n",
            "Epoch 99330 | Loss: 0.41192370653152466 | Test loss: 0.08206064999103546\n",
            "Epoch 99340 | Loss: 0.41088029742240906 | Test loss: 0.0827278420329094\n",
            "Epoch 99350 | Loss: 0.40983691811561584 | Test loss: 0.08343475311994553\n",
            "Epoch 99360 | Loss: 0.40879350900650024 | Test loss: 0.08414166420698166\n",
            "Epoch 99370 | Loss: 0.40775012969970703 | Test loss: 0.08484857529401779\n",
            "Epoch 99380 | Loss: 0.40670672059059143 | Test loss: 0.08555545657873154\n",
            "Epoch 99390 | Loss: 0.40566331148147583 | Test loss: 0.08626244217157364\n",
            "Epoch 99400 | Loss: 0.4046199321746826 | Test loss: 0.08698949962854385\n",
            "Epoch 99410 | Loss: 0.4035765826702118 | Test loss: 0.0877399668097496\n",
            "Epoch 99420 | Loss: 0.4025331437587738 | Test loss: 0.08849058300256729\n",
            "Epoch 99430 | Loss: 0.401489794254303 | Test loss: 0.08924122154712677\n",
            "Epoch 99440 | Loss: 0.4004463851451874 | Test loss: 0.08999188244342804\n",
            "Epoch 99450 | Loss: 0.3994029760360718 | Test loss: 0.09074252098798752\n",
            "Epoch 99460 | Loss: 0.3983595669269562 | Test loss: 0.09153063595294952\n",
            "Epoch 99470 | Loss: 0.39731621742248535 | Test loss: 0.09232518821954727\n",
            "Epoch 99480 | Loss: 0.39627280831336975 | Test loss: 0.0931195542216301\n",
            "Epoch 99490 | Loss: 0.39522942900657654 | Test loss: 0.09391400963068008\n",
            "Epoch 99500 | Loss: 0.39418599009513855 | Test loss: 0.0947083979845047\n",
            "Epoch 99510 | Loss: 0.3931426703929901 | Test loss: 0.09551515430212021\n",
            "Epoch 99520 | Loss: 0.3920992314815521 | Test loss: 0.09635353088378906\n",
            "Epoch 99530 | Loss: 0.3910558819770813 | Test loss: 0.09719183295965195\n",
            "Epoch 99540 | Loss: 0.3900124728679657 | Test loss: 0.09803026169538498\n",
            "Epoch 99550 | Loss: 0.3889690935611725 | Test loss: 0.09886863082647324\n",
            "Epoch 99560 | Loss: 0.3879256844520569 | Test loss: 0.09970696270465851\n",
            "Epoch 99570 | Loss: 0.38689398765563965 | Test loss: 0.10053463280200958\n",
            "Epoch 99580 | Loss: 0.38586950302124023 | Test loss: 0.10135779529809952\n",
            "Epoch 99590 | Loss: 0.38484498858451843 | Test loss: 0.10218093544244766\n",
            "Epoch 99600 | Loss: 0.3838205337524414 | Test loss: 0.1030040755867958\n",
            "Epoch 99610 | Loss: 0.382796049118042 | Test loss: 0.10382717102766037\n",
            "Epoch 99620 | Loss: 0.3817717730998993 | Test loss: 0.10464785248041153\n",
            "Epoch 99630 | Loss: 0.38077518343925476 | Test loss: 0.1054462417960167\n",
            "Epoch 99640 | Loss: 0.37977859377861023 | Test loss: 0.10624458640813828\n",
            "Epoch 99650 | Loss: 0.3787820041179657 | Test loss: 0.10704298317432404\n",
            "Epoch 99660 | Loss: 0.3777853548526764 | Test loss: 0.10784132778644562\n",
            "Epoch 99670 | Loss: 0.37678876519203186 | Test loss: 0.10863969475030899\n",
            "Epoch 99680 | Loss: 0.37579670548439026 | Test loss: 0.10943350940942764\n",
            "Epoch 99690 | Loss: 0.37481823563575745 | Test loss: 0.11021661758422852\n",
            "Epoch 99700 | Loss: 0.3738398253917694 | Test loss: 0.11099977791309357\n",
            "Epoch 99710 | Loss: 0.3728613555431366 | Test loss: 0.11178290843963623\n",
            "Epoch 99720 | Loss: 0.37188291549682617 | Test loss: 0.11256606876850128\n",
            "Epoch 99730 | Loss: 0.37090444564819336 | Test loss: 0.11334922164678574\n",
            "Epoch 99740 | Loss: 0.3699340522289276 | Test loss: 0.11412473022937775\n",
            "Epoch 99750 | Loss: 0.36897343397140503 | Test loss: 0.1148926243185997\n",
            "Epoch 99760 | Loss: 0.36801281571388245 | Test loss: 0.11566057056188583\n",
            "Epoch 99770 | Loss: 0.36705222725868225 | Test loss: 0.11642847210168839\n",
            "Epoch 99780 | Loss: 0.36609163880348206 | Test loss: 0.11719644069671631\n",
            "Epoch 99790 | Loss: 0.3651309907436371 | Test loss: 0.11796436458826065\n",
            "Epoch 99800 | Loss: 0.3641865849494934 | Test loss: 0.11871495097875595\n",
            "Epoch 99810 | Loss: 0.3632524311542511 | Test loss: 0.11945810168981552\n",
            "Epoch 99820 | Loss: 0.3623183071613312 | Test loss: 0.12020125240087509\n",
            "Epoch 99830 | Loss: 0.36138415336608887 | Test loss: 0.12094443291425705\n",
            "Epoch 99840 | Loss: 0.36045002937316895 | Test loss: 0.12168758362531662\n",
            "Epoch 99850 | Loss: 0.35951587557792664 | Test loss: 0.12243075668811798\n",
            "Epoch 99860 | Loss: 0.35859236121177673 | Test loss: 0.12316322326660156\n",
            "Epoch 99870 | Loss: 0.35767537355422974 | Test loss: 0.1238911896944046\n",
            "Epoch 99880 | Loss: 0.35675832629203796 | Test loss: 0.1246192455291748\n",
            "Epoch 99890 | Loss: 0.3558413088321686 | Test loss: 0.1253470629453659\n",
            "Epoch 99900 | Loss: 0.3549242913722992 | Test loss: 0.12607501447200775\n",
            "Epoch 99910 | Loss: 0.3540072739124298 | Test loss: 0.12680292129516602\n",
            "Epoch 99920 | Loss: 0.353099524974823 | Test loss: 0.1275223344564438\n",
            "Epoch 99930 | Loss: 0.35219961404800415 | Test loss: 0.12823598086833954\n",
            "Epoch 99940 | Loss: 0.3512997031211853 | Test loss: 0.12894964218139648\n",
            "Epoch 99950 | Loss: 0.35039979219436646 | Test loss: 0.12966330349445343\n",
            "Epoch 99960 | Loss: 0.3494998812675476 | Test loss: 0.13037697970867157\n",
            "Epoch 99970 | Loss: 0.34860000014305115 | Test loss: 0.1310906708240509\n",
            "Epoch 99980 | Loss: 0.34771060943603516 | Test loss: 0.13179193437099457\n",
            "Epoch 99990 | Loss: 0.34683582186698914 | Test loss: 0.13248084485530853\n",
            "Epoch 100000 | Loss: 0.34596097469329834 | Test loss: 0.1331697702407837\n",
            "Epoch 100010 | Loss: 0.3450861871242523 | Test loss: 0.13385868072509766\n",
            "Epoch 100020 | Loss: 0.3442113995552063 | Test loss: 0.13454754650592804\n",
            "Epoch 100030 | Loss: 0.3433365523815155 | Test loss: 0.1352364867925644\n",
            "Epoch 100040 | Loss: 0.34246429800987244 | Test loss: 0.13592253625392914\n",
            "Epoch 100050 | Loss: 0.34160587191581726 | Test loss: 0.1365971565246582\n",
            "Epoch 100060 | Loss: 0.34074750542640686 | Test loss: 0.1372717171907425\n",
            "Epoch 100070 | Loss: 0.3398891091346741 | Test loss: 0.13794641196727753\n",
            "Epoch 100080 | Loss: 0.3390306830406189 | Test loss: 0.1386210173368454\n",
            "Epoch 100090 | Loss: 0.3381722867488861 | Test loss: 0.13929568231105804\n",
            "Epoch 100100 | Loss: 0.33731386065483093 | Test loss: 0.1399703025817871\n",
            "Epoch 100110 | Loss: 0.3364691436290741 | Test loss: 0.14063206315040588\n",
            "Epoch 100120 | Loss: 0.33562687039375305 | Test loss: 0.14129243791103363\n",
            "Epoch 100130 | Loss: 0.33478453755378723 | Test loss: 0.1419527530670166\n",
            "Epoch 100140 | Loss: 0.33394232392311096 | Test loss: 0.14261315762996674\n",
            "Epoch 100150 | Loss: 0.3331000506877899 | Test loss: 0.14327318966388702\n",
            "Epoch 100160 | Loss: 0.33225780725479126 | Test loss: 0.14393381774425507\n",
            "Epoch 100170 | Loss: 0.33142605423927307 | Test loss: 0.14458230137825012\n",
            "Epoch 100180 | Loss: 0.3306078612804413 | Test loss: 0.1452188342809677\n",
            "Epoch 100190 | Loss: 0.3297896981239319 | Test loss: 0.1458553522825241\n",
            "Epoch 100200 | Loss: 0.3289714753627777 | Test loss: 0.1464919149875641\n",
            "Epoch 100210 | Loss: 0.3281533122062683 | Test loss: 0.14712844789028168\n",
            "Epoch 100220 | Loss: 0.3273351192474365 | Test loss: 0.14776499569416046\n",
            "Epoch 100230 | Loss: 0.3265169560909271 | Test loss: 0.14840149879455566\n",
            "Epoch 100240 | Loss: 0.32571345567703247 | Test loss: 0.14902381598949432\n",
            "Epoch 100250 | Loss: 0.3249106705188751 | Test loss: 0.1496460735797882\n",
            "Epoch 100260 | Loss: 0.3241078853607178 | Test loss: 0.1502683460712433\n",
            "Epoch 100270 | Loss: 0.3233051896095276 | Test loss: 0.15089061856269836\n",
            "Epoch 100280 | Loss: 0.322502464056015 | Test loss: 0.15151290595531464\n",
            "Epoch 100290 | Loss: 0.32169970870018005 | Test loss: 0.15213511884212494\n",
            "Epoch 100300 | Loss: 0.32090234756469727 | Test loss: 0.15275172889232635\n",
            "Epoch 100310 | Loss: 0.32011473178863525 | Test loss: 0.15335969626903534\n",
            "Epoch 100320 | Loss: 0.3193272650241852 | Test loss: 0.15396766364574432\n",
            "Epoch 100330 | Loss: 0.31853970885276794 | Test loss: 0.1545756608247757\n",
            "Epoch 100340 | Loss: 0.3177521824836731 | Test loss: 0.15518365800380707\n",
            "Epoch 100350 | Loss: 0.31696465611457825 | Test loss: 0.15579159557819366\n",
            "Epoch 100360 | Loss: 0.316177099943161 | Test loss: 0.15639963746070862\n",
            "Epoch 100370 | Loss: 0.3154003620147705 | Test loss: 0.1569962501525879\n",
            "Epoch 100380 | Loss: 0.3146277368068695 | Test loss: 0.15758994221687317\n",
            "Epoch 100390 | Loss: 0.3138551115989685 | Test loss: 0.15818364918231964\n",
            "Epoch 100400 | Loss: 0.3130825161933899 | Test loss: 0.15877754986286163\n",
            "Epoch 100410 | Loss: 0.3123098909854889 | Test loss: 0.15937106311321259\n",
            "Epoch 100420 | Loss: 0.3115372657775879 | Test loss: 0.15996478497982025\n",
            "Epoch 100430 | Loss: 0.310764878988266 | Test loss: 0.1605561524629593\n",
            "Epoch 100440 | Loss: 0.31001463532447815 | Test loss: 0.16112597286701202\n",
            "Epoch 100450 | Loss: 0.3092644214630127 | Test loss: 0.1616959124803543\n",
            "Epoch 100460 | Loss: 0.30851420760154724 | Test loss: 0.1622658222913742\n",
            "Epoch 100470 | Loss: 0.3077639639377594 | Test loss: 0.1628357619047165\n",
            "Epoch 100480 | Loss: 0.30701375007629395 | Test loss: 0.1634056568145752\n",
            "Epoch 100490 | Loss: 0.3062635362148285 | Test loss: 0.1639755815267563\n",
            "Epoch 100500 | Loss: 0.305515319108963 | Test loss: 0.1645427793264389\n",
            "Epoch 100510 | Loss: 0.3047797381877899 | Test loss: 0.1650993376970291\n",
            "Epoch 100520 | Loss: 0.30404412746429443 | Test loss: 0.1656559258699417\n",
            "Epoch 100530 | Loss: 0.30330851674079895 | Test loss: 0.16621248424053192\n",
            "Epoch 100540 | Loss: 0.30257293581962585 | Test loss: 0.16676902770996094\n",
            "Epoch 100550 | Loss: 0.30183732509613037 | Test loss: 0.16732561588287354\n",
            "Epoch 100560 | Loss: 0.3011017441749573 | Test loss: 0.16788223385810852\n",
            "Epoch 100570 | Loss: 0.3003692626953125 | Test loss: 0.16843454539775848\n",
            "Epoch 100580 | Loss: 0.2996476888656616 | Test loss: 0.16897688806056976\n",
            "Epoch 100590 | Loss: 0.29892608523368835 | Test loss: 0.16951917111873627\n",
            "Epoch 100600 | Loss: 0.2982045114040375 | Test loss: 0.17006142437458038\n",
            "Epoch 100610 | Loss: 0.2974829077720642 | Test loss: 0.17060373723506927\n",
            "Epoch 100620 | Loss: 0.29676130414009094 | Test loss: 0.17114603519439697\n",
            "Epoch 100630 | Loss: 0.2960396707057953 | Test loss: 0.17168834805488586\n",
            "Epoch 100640 | Loss: 0.29532286524772644 | Test loss: 0.1722237914800644\n",
            "Epoch 100650 | Loss: 0.2946228086948395 | Test loss: 0.17274320125579834\n",
            "Epoch 100660 | Loss: 0.2939227223396301 | Test loss: 0.17326267063617706\n",
            "Epoch 100670 | Loss: 0.2932226359844208 | Test loss: 0.1737820953130722\n",
            "Epoch 100680 | Loss: 0.29252251982688904 | Test loss: 0.17430153489112854\n",
            "Epoch 100690 | Loss: 0.2918224632740021 | Test loss: 0.17482097446918488\n",
            "Epoch 100700 | Loss: 0.2911223769187927 | Test loss: 0.17534039914608002\n",
            "Epoch 100710 | Loss: 0.290423721075058 | Test loss: 0.17585699260234833\n",
            "Epoch 100720 | Loss: 0.2897369861602783 | Test loss: 0.1763620674610138\n",
            "Epoch 100730 | Loss: 0.28905031085014343 | Test loss: 0.1768673211336136\n",
            "Epoch 100740 | Loss: 0.2883635461330414 | Test loss: 0.17737248539924622\n",
            "Epoch 100750 | Loss: 0.2876768708229065 | Test loss: 0.17787759006023407\n",
            "Epoch 100760 | Loss: 0.28699013590812683 | Test loss: 0.17838279902935028\n",
            "Epoch 100770 | Loss: 0.28630343079566956 | Test loss: 0.17888787388801575\n",
            "Epoch 100780 | Loss: 0.2856167256832123 | Test loss: 0.17939312756061554\n",
            "Epoch 100790 | Loss: 0.2849421501159668 | Test loss: 0.1798849105834961\n",
            "Epoch 100800 | Loss: 0.28426894545555115 | Test loss: 0.180376797914505\n",
            "Epoch 100810 | Loss: 0.2835957109928131 | Test loss: 0.18086858093738556\n",
            "Epoch 100820 | Loss: 0.28292250633239746 | Test loss: 0.1813604086637497\n",
            "Epoch 100830 | Loss: 0.2822493016719818 | Test loss: 0.18185226619243622\n",
            "Epoch 100840 | Loss: 0.2815760672092438 | Test loss: 0.18234418332576752\n",
            "Epoch 100850 | Loss: 0.2809028625488281 | Test loss: 0.1828359216451645\n",
            "Epoch 100860 | Loss: 0.28024211525917053 | Test loss: 0.18331177532672882\n",
            "Epoch 100870 | Loss: 0.2795891761779785 | Test loss: 0.18378077447414398\n",
            "Epoch 100880 | Loss: 0.2789362370967865 | Test loss: 0.18424971401691437\n",
            "Epoch 100890 | Loss: 0.27828332781791687 | Test loss: 0.18471872806549072\n",
            "Epoch 100900 | Loss: 0.27763041853904724 | Test loss: 0.18518765270709991\n",
            "Epoch 100910 | Loss: 0.2769774794578552 | Test loss: 0.18565665185451508\n",
            "Epoch 100920 | Loss: 0.2763245403766632 | Test loss: 0.18612559139728546\n",
            "Epoch 100930 | Loss: 0.2756742238998413 | Test loss: 0.1865905374288559\n",
            "Epoch 100940 | Loss: 0.27503418922424316 | Test loss: 0.18704627454280853\n",
            "Epoch 100950 | Loss: 0.2743941843509674 | Test loss: 0.18750183284282684\n",
            "Epoch 100960 | Loss: 0.2737541198730469 | Test loss: 0.18795745074748993\n",
            "Epoch 100970 | Loss: 0.27311408519744873 | Test loss: 0.1884130984544754\n",
            "Epoch 100980 | Loss: 0.2724740505218506 | Test loss: 0.18886879086494446\n",
            "Epoch 100990 | Loss: 0.27183398604393005 | Test loss: 0.18932437896728516\n",
            "Epoch 101000 | Loss: 0.2711939811706543 | Test loss: 0.1897800713777542\n",
            "Epoch 101010 | Loss: 0.2705628573894501 | Test loss: 0.1902250051498413\n",
            "Epoch 101020 | Loss: 0.2699354588985443 | Test loss: 0.19066737592220306\n",
            "Epoch 101030 | Loss: 0.26930809020996094 | Test loss: 0.19110968708992004\n",
            "Epoch 101040 | Loss: 0.26868072152137756 | Test loss: 0.19155199825763702\n",
            "Epoch 101050 | Loss: 0.26805320382118225 | Test loss: 0.1919943392276764\n",
            "Epoch 101060 | Loss: 0.2674259841442108 | Test loss: 0.1924365758895874\n",
            "Epoch 101070 | Loss: 0.26679858565330505 | Test loss: 0.19287894666194916\n",
            "Epoch 101080 | Loss: 0.2661735713481903 | Test loss: 0.19331662356853485\n",
            "Epoch 101090 | Loss: 0.26556530594825745 | Test loss: 0.19373607635498047\n",
            "Epoch 101100 | Loss: 0.2649569511413574 | Test loss: 0.19415555894374847\n",
            "Epoch 101110 | Loss: 0.26434871554374695 | Test loss: 0.1945749968290329\n",
            "Epoch 101120 | Loss: 0.2637403905391693 | Test loss: 0.1949944794178009\n",
            "Epoch 101130 | Loss: 0.26313212513923645 | Test loss: 0.19541390240192413\n",
            "Epoch 101140 | Loss: 0.2625238299369812 | Test loss: 0.19583342969417572\n",
            "Epoch 101150 | Loss: 0.26191553473472595 | Test loss: 0.19625277817249298\n",
            "Epoch 101160 | Loss: 0.261312335729599 | Test loss: 0.19666610658168793\n",
            "Epoch 101170 | Loss: 0.26071661710739136 | Test loss: 0.19707314670085907\n",
            "Epoch 101180 | Loss: 0.26012077927589417 | Test loss: 0.1974802017211914\n",
            "Epoch 101190 | Loss: 0.25952500104904175 | Test loss: 0.19788728654384613\n",
            "Epoch 101200 | Loss: 0.25892916321754456 | Test loss: 0.19829438626766205\n",
            "Epoch 101210 | Loss: 0.25833335518836975 | Test loss: 0.198701411485672\n",
            "Epoch 101220 | Loss: 0.25773757696151733 | Test loss: 0.19910843670368195\n",
            "Epoch 101230 | Loss: 0.25714173913002014 | Test loss: 0.19951558113098145\n",
            "Epoch 101240 | Loss: 0.2565533220767975 | Test loss: 0.19991329312324524\n",
            "Epoch 101250 | Loss: 0.255969375371933 | Test loss: 0.20030704140663147\n",
            "Epoch 101260 | Loss: 0.2553853690624237 | Test loss: 0.2007007598876953\n",
            "Epoch 101270 | Loss: 0.25480136275291443 | Test loss: 0.20109450817108154\n",
            "Epoch 101280 | Loss: 0.25421735644340515 | Test loss: 0.20148825645446777\n",
            "Epoch 101290 | Loss: 0.25363343954086304 | Test loss: 0.2018820345401764\n",
            "Epoch 101300 | Loss: 0.25304946303367615 | Test loss: 0.20227575302124023\n",
            "Epoch 101310 | Loss: 0.25246545672416687 | Test loss: 0.20266945660114288\n",
            "Epoch 101320 | Loss: 0.2518961429595947 | Test loss: 0.20304353535175323\n",
            "Epoch 101330 | Loss: 0.25133052468299866 | Test loss: 0.20341531932353973\n",
            "Epoch 101340 | Loss: 0.2507649064064026 | Test loss: 0.203787162899971\n",
            "Epoch 101350 | Loss: 0.25019922852516174 | Test loss: 0.2041589766740799\n",
            "Epoch 101360 | Loss: 0.2496335804462433 | Test loss: 0.20453079044818878\n",
            "Epoch 101370 | Loss: 0.24906793236732483 | Test loss: 0.20490267872810364\n",
            "Epoch 101380 | Loss: 0.24850229918956757 | Test loss: 0.20527441799640656\n",
            "Epoch 101390 | Loss: 0.2479366511106491 | Test loss: 0.2056463062763214\n",
            "Epoch 101400 | Loss: 0.2473793476819992 | Test loss: 0.2060074359178543\n",
            "Epoch 101410 | Loss: 0.2468249797821045 | Test loss: 0.20636597275733948\n",
            "Epoch 101420 | Loss: 0.2462705820798874 | Test loss: 0.2067243903875351\n",
            "Epoch 101430 | Loss: 0.2457161694765091 | Test loss: 0.20708297193050385\n",
            "Epoch 101440 | Loss: 0.24516181647777557 | Test loss: 0.20744143426418304\n",
            "Epoch 101450 | Loss: 0.24460740387439728 | Test loss: 0.20779995620250702\n",
            "Epoch 101460 | Loss: 0.24405303597450256 | Test loss: 0.20815840363502502\n",
            "Epoch 101470 | Loss: 0.24349860846996307 | Test loss: 0.2085169404745102\n",
            "Epoch 101480 | Loss: 0.2429516315460205 | Test loss: 0.20886676013469696\n",
            "Epoch 101490 | Loss: 0.24240870773792267 | Test loss: 0.20921288430690765\n",
            "Epoch 101500 | Loss: 0.24186578392982483 | Test loss: 0.20955899357795715\n",
            "Epoch 101510 | Loss: 0.241322860121727 | Test loss: 0.20990513265132904\n",
            "Epoch 101520 | Loss: 0.24077987670898438 | Test loss: 0.21025124192237854\n",
            "Epoch 101530 | Loss: 0.24023692309856415 | Test loss: 0.21059735119342804\n",
            "Epoch 101540 | Loss: 0.2396940290927887 | Test loss: 0.21094346046447754\n",
            "Epoch 101550 | Loss: 0.23915107548236847 | Test loss: 0.21128962934017181\n",
            "Epoch 101560 | Loss: 0.2386140376329422 | Test loss: 0.21162831783294678\n",
            "Epoch 101570 | Loss: 0.23808236420154572 | Test loss: 0.2119620144367218\n",
            "Epoch 101580 | Loss: 0.23755064606666565 | Test loss: 0.2122957706451416\n",
            "Epoch 101590 | Loss: 0.23701901733875275 | Test loss: 0.21262948215007782\n",
            "Epoch 101600 | Loss: 0.2364872545003891 | Test loss: 0.21296322345733643\n",
            "Epoch 101610 | Loss: 0.23595552146434784 | Test loss: 0.21329696476459503\n",
            "Epoch 101620 | Loss: 0.23542387783527374 | Test loss: 0.21363070607185364\n",
            "Epoch 101630 | Loss: 0.23489215970039368 | Test loss: 0.21396449208259583\n",
            "Epoch 101640 | Loss: 0.23436589539051056 | Test loss: 0.21428942680358887\n",
            "Epoch 101650 | Loss: 0.23385106027126312 | Test loss: 0.21460123360157013\n",
            "Epoch 101660 | Loss: 0.2333362102508545 | Test loss: 0.21491308510303497\n",
            "Epoch 101670 | Loss: 0.23282137513160706 | Test loss: 0.21522490680217743\n",
            "Epoch 101680 | Loss: 0.23230652511119843 | Test loss: 0.21553674340248108\n",
            "Epoch 101690 | Loss: 0.2317916601896286 | Test loss: 0.21584852039813995\n",
            "Epoch 101700 | Loss: 0.23127679526805878 | Test loss: 0.2161603718996048\n",
            "Epoch 101710 | Loss: 0.23076200485229492 | Test loss: 0.21647217869758606\n",
            "Epoch 101720 | Loss: 0.2302471250295639 | Test loss: 0.21678400039672852\n",
            "Epoch 101730 | Loss: 0.22974233329296112 | Test loss: 0.21708345413208008\n",
            "Epoch 101740 | Loss: 0.22923822700977325 | Test loss: 0.21738289296627045\n",
            "Epoch 101750 | Loss: 0.22873401641845703 | Test loss: 0.21768231689929962\n",
            "Epoch 101760 | Loss: 0.22822986543178558 | Test loss: 0.21798180043697357\n",
            "Epoch 101770 | Loss: 0.22772569954395294 | Test loss: 0.21828117966651917\n",
            "Epoch 101780 | Loss: 0.2272215336561203 | Test loss: 0.2185806781053543\n",
            "Epoch 101790 | Loss: 0.22671738266944885 | Test loss: 0.2188800871372223\n",
            "Epoch 101800 | Loss: 0.2262132167816162 | Test loss: 0.21917954087257385\n",
            "Epoch 101810 | Loss: 0.22571444511413574 | Test loss: 0.21947155892848969\n",
            "Epoch 101820 | Loss: 0.22522075474262238 | Test loss: 0.21975867450237274\n",
            "Epoch 101830 | Loss: 0.224727064371109 | Test loss: 0.22004568576812744\n",
            "Epoch 101840 | Loss: 0.22423337399959564 | Test loss: 0.22033272683620453\n",
            "Epoch 101850 | Loss: 0.2237396538257599 | Test loss: 0.22061984241008759\n",
            "Epoch 101860 | Loss: 0.22324596345424652 | Test loss: 0.22090685367584229\n",
            "Epoch 101870 | Loss: 0.22275228798389435 | Test loss: 0.22119390964508057\n",
            "Epoch 101880 | Loss: 0.2222585678100586 | Test loss: 0.22148096561431885\n",
            "Epoch 101890 | Loss: 0.22176508605480194 | Test loss: 0.22176598012447357\n",
            "Epoch 101900 | Loss: 0.2212875932455063 | Test loss: 0.22203205525875092\n",
            "Epoch 101910 | Loss: 0.22081008553504944 | Test loss: 0.22229814529418945\n",
            "Epoch 101920 | Loss: 0.2203325778245926 | Test loss: 0.2225642055273056\n",
            "Epoch 101930 | Loss: 0.2198551744222641 | Test loss: 0.22283034026622772\n",
            "Epoch 101940 | Loss: 0.21937762200832367 | Test loss: 0.22309640049934387\n",
            "Epoch 101950 | Loss: 0.21890011429786682 | Test loss: 0.2233625203371048\n",
            "Epoch 101960 | Loss: 0.21842257678508759 | Test loss: 0.22362859547138214\n",
            "Epoch 101970 | Loss: 0.21794508397579193 | Test loss: 0.22389471530914307\n",
            "Epoch 101980 | Loss: 0.21747075021266937 | Test loss: 0.22415585815906525\n",
            "Epoch 101990 | Loss: 0.21700315177440643 | Test loss: 0.2244095355272293\n",
            "Epoch 102000 | Loss: 0.2165355682373047 | Test loss: 0.22466325759887695\n",
            "Epoch 102010 | Loss: 0.21606798470020294 | Test loss: 0.22491693496704102\n",
            "Epoch 102020 | Loss: 0.21560029685497284 | Test loss: 0.22517071664333344\n",
            "Epoch 102030 | Loss: 0.21513286232948303 | Test loss: 0.22542433440685272\n",
            "Epoch 102040 | Loss: 0.2146652787923813 | Test loss: 0.22567808628082275\n",
            "Epoch 102050 | Loss: 0.21419768035411835 | Test loss: 0.22593186795711517\n",
            "Epoch 102060 | Loss: 0.213730126619339 | Test loss: 0.22618556022644043\n",
            "Epoch 102070 | Loss: 0.21326780319213867 | Test loss: 0.22643183171749115\n",
            "Epoch 102080 | Loss: 0.2128099948167801 | Test loss: 0.2266731560230255\n",
            "Epoch 102090 | Loss: 0.21235208213329315 | Test loss: 0.22691448032855988\n",
            "Epoch 102100 | Loss: 0.2118941843509674 | Test loss: 0.22715578973293304\n",
            "Epoch 102110 | Loss: 0.21143631637096405 | Test loss: 0.22739718854427338\n",
            "Epoch 102120 | Loss: 0.2109784632921219 | Test loss: 0.22763848304748535\n",
            "Epoch 102130 | Loss: 0.21052061021327972 | Test loss: 0.2278798669576645\n",
            "Epoch 102140 | Loss: 0.21006274223327637 | Test loss: 0.22812116146087646\n",
            "Epoch 102150 | Loss: 0.20960484445095062 | Test loss: 0.22836251556873322\n",
            "Epoch 102160 | Loss: 0.20915792882442474 | Test loss: 0.2285871058702469\n",
            "Epoch 102170 | Loss: 0.20871515572071075 | Test loss: 0.22880740463733673\n",
            "Epoch 102180 | Loss: 0.20827238261699677 | Test loss: 0.22902777791023254\n",
            "Epoch 102190 | Loss: 0.20782963931560516 | Test loss: 0.22924812138080597\n",
            "Epoch 102200 | Loss: 0.20738685131072998 | Test loss: 0.22946853935718536\n",
            "Epoch 102210 | Loss: 0.20694409310817719 | Test loss: 0.2296888828277588\n",
            "Epoch 102220 | Loss: 0.2065013200044632 | Test loss: 0.22990922629833221\n",
            "Epoch 102230 | Loss: 0.2060585469007492 | Test loss: 0.23012959957122803\n",
            "Epoch 102240 | Loss: 0.20561575889587402 | Test loss: 0.23034997284412384\n",
            "Epoch 102250 | Loss: 0.20517992973327637 | Test loss: 0.23056121170520782\n",
            "Epoch 102260 | Loss: 0.20474682748317719 | Test loss: 0.23077011108398438\n",
            "Epoch 102270 | Loss: 0.2043137103319168 | Test loss: 0.2309790849685669\n",
            "Epoch 102280 | Loss: 0.20388062298297882 | Test loss: 0.23118801414966583\n",
            "Epoch 102290 | Loss: 0.20344746112823486 | Test loss: 0.23139694333076477\n",
            "Epoch 102300 | Loss: 0.20301425457000732 | Test loss: 0.2316058725118637\n",
            "Epoch 102310 | Loss: 0.20258121192455292 | Test loss: 0.23181478679180145\n",
            "Epoch 102320 | Loss: 0.20214810967445374 | Test loss: 0.2320237010717392\n",
            "Epoch 102330 | Loss: 0.20171499252319336 | Test loss: 0.23223264515399933\n",
            "Epoch 102340 | Loss: 0.20128746330738068 | Test loss: 0.23243291676044464\n",
            "Epoch 102350 | Loss: 0.20086336135864258 | Test loss: 0.2326294630765915\n",
            "Epoch 102360 | Loss: 0.2004392147064209 | Test loss: 0.23282597959041595\n",
            "Epoch 102370 | Loss: 0.20001506805419922 | Test loss: 0.23302260041236877\n",
            "Epoch 102380 | Loss: 0.19959092140197754 | Test loss: 0.23321910202503204\n",
            "Epoch 102390 | Loss: 0.19916673004627228 | Test loss: 0.23341567814350128\n",
            "Epoch 102400 | Loss: 0.19874262809753418 | Test loss: 0.23361222445964813\n",
            "Epoch 102410 | Loss: 0.1983184814453125 | Test loss: 0.2338087409734726\n",
            "Epoch 102420 | Loss: 0.19789433479309082 | Test loss: 0.23400536179542542\n",
            "Epoch 102430 | Loss: 0.1974772959947586 | Test loss: 0.23418931663036346\n",
            "Epoch 102440 | Loss: 0.19706715643405914 | Test loss: 0.23436486721038818\n",
            "Epoch 102450 | Loss: 0.19665703177452087 | Test loss: 0.23454049229621887\n",
            "Epoch 102460 | Loss: 0.1962468922138214 | Test loss: 0.2347160428762436\n",
            "Epoch 102470 | Loss: 0.19583675265312195 | Test loss: 0.2348916381597519\n",
            "Epoch 102480 | Loss: 0.19542661309242249 | Test loss: 0.2350672036409378\n",
            "Epoch 102490 | Loss: 0.19501648843288422 | Test loss: 0.2352428287267685\n",
            "Epoch 102500 | Loss: 0.19460634887218475 | Test loss: 0.2354183942079544\n",
            "Epoch 102510 | Loss: 0.19419622421264648 | Test loss: 0.23559395968914032\n",
            "Epoch 102520 | Loss: 0.19378717243671417 | Test loss: 0.23576724529266357\n",
            "Epoch 102530 | Loss: 0.1933860033750534 | Test loss: 0.2359313815832138\n",
            "Epoch 102540 | Loss: 0.1929849237203598 | Test loss: 0.23609559237957\n",
            "Epoch 102550 | Loss: 0.1925835758447647 | Test loss: 0.23625969886779785\n",
            "Epoch 102560 | Loss: 0.19218234717845917 | Test loss: 0.23642385005950928\n",
            "Epoch 102570 | Loss: 0.1917811632156372 | Test loss: 0.2365880012512207\n",
            "Epoch 102580 | Loss: 0.1913800984621048 | Test loss: 0.2367521971464157\n",
            "Epoch 102590 | Loss: 0.1909787505865097 | Test loss: 0.23691630363464355\n",
            "Epoch 102600 | Loss: 0.19057750701904297 | Test loss: 0.23708045482635498\n",
            "Epoch 102610 | Loss: 0.1901763379573822 | Test loss: 0.2372445911169052\n",
            "Epoch 102620 | Loss: 0.18978124856948853 | Test loss: 0.23739968240261078\n",
            "Epoch 102630 | Loss: 0.18938879668712616 | Test loss: 0.23755232989788055\n",
            "Epoch 102640 | Loss: 0.1889963299036026 | Test loss: 0.2377050668001175\n",
            "Epoch 102650 | Loss: 0.18860384821891785 | Test loss: 0.23785780370235443\n",
            "Epoch 102660 | Loss: 0.1882113665342331 | Test loss: 0.23801052570343018\n",
            "Epoch 102670 | Loss: 0.18781886994838715 | Test loss: 0.23816323280334473\n",
            "Epoch 102680 | Loss: 0.1874263882637024 | Test loss: 0.23831593990325928\n",
            "Epoch 102690 | Loss: 0.18703389167785645 | Test loss: 0.23846864700317383\n",
            "Epoch 102700 | Loss: 0.18664143979549408 | Test loss: 0.23862142860889435\n",
            "Epoch 102710 | Loss: 0.18625184893608093 | Test loss: 0.2387681007385254\n",
            "Epoch 102720 | Loss: 0.18587282299995422 | Test loss: 0.2389007806777954\n",
            "Epoch 102730 | Loss: 0.18549378216266632 | Test loss: 0.2390335351228714\n",
            "Epoch 102740 | Loss: 0.18511471152305603 | Test loss: 0.23916618525981903\n",
            "Epoch 102750 | Loss: 0.18473565578460693 | Test loss: 0.23929889500141144\n",
            "Epoch 102760 | Loss: 0.18435658514499664 | Test loss: 0.23943157494068146\n",
            "Epoch 102770 | Loss: 0.18397752940654755 | Test loss: 0.23956428468227386\n",
            "Epoch 102780 | Loss: 0.18359850347042084 | Test loss: 0.23969700932502747\n",
            "Epoch 102790 | Loss: 0.18321946263313293 | Test loss: 0.2398296594619751\n",
            "Epoch 102800 | Loss: 0.18284037709236145 | Test loss: 0.23996232450008392\n",
            "Epoch 102810 | Loss: 0.18246550858020782 | Test loss: 0.24008822441101074\n",
            "Epoch 102820 | Loss: 0.18209467828273773 | Test loss: 0.2402094602584839\n",
            "Epoch 102830 | Loss: 0.18172383308410645 | Test loss: 0.24033072590827942\n",
            "Epoch 102840 | Loss: 0.18135300278663635 | Test loss: 0.24045203626155853\n",
            "Epoch 102850 | Loss: 0.18098215758800507 | Test loss: 0.2405732423067093\n",
            "Epoch 102860 | Loss: 0.18061132729053497 | Test loss: 0.24069452285766602\n",
            "Epoch 102870 | Loss: 0.1802404820919037 | Test loss: 0.24081575870513916\n",
            "Epoch 102880 | Loss: 0.1798696368932724 | Test loss: 0.24093718826770782\n",
            "Epoch 102890 | Loss: 0.1794987916946411 | Test loss: 0.2410583347082138\n",
            "Epoch 102900 | Loss: 0.17912797629833221 | Test loss: 0.24117957055568695\n",
            "Epoch 102910 | Loss: 0.17876291275024414 | Test loss: 0.24129168689250946\n",
            "Epoch 102920 | Loss: 0.17840011417865753 | Test loss: 0.2414015531539917\n",
            "Epoch 102930 | Loss: 0.17803727090358734 | Test loss: 0.24151141941547394\n",
            "Epoch 102940 | Loss: 0.17767448723316193 | Test loss: 0.2416212111711502\n",
            "Epoch 102950 | Loss: 0.17731165885925293 | Test loss: 0.24173109233379364\n",
            "Epoch 102960 | Loss: 0.17694883048534393 | Test loss: 0.2418409138917923\n",
            "Epoch 102970 | Loss: 0.17658601701259613 | Test loss: 0.24195082485675812\n",
            "Epoch 102980 | Loss: 0.17622318863868713 | Test loss: 0.24206054210662842\n",
            "Epoch 102990 | Loss: 0.17586036026477814 | Test loss: 0.24217043817043304\n",
            "Epoch 103000 | Loss: 0.17549756169319153 | Test loss: 0.24228031933307648\n",
            "Epoch 103010 | Loss: 0.17514227330684662 | Test loss: 0.2423795908689499\n",
            "Epoch 103020 | Loss: 0.17478778958320618 | Test loss: 0.24247892200946808\n",
            "Epoch 103030 | Loss: 0.17443327605724335 | Test loss: 0.24257826805114746\n",
            "Epoch 103040 | Loss: 0.1740788221359253 | Test loss: 0.24267764389514923\n",
            "Epoch 103050 | Loss: 0.17372427880764008 | Test loss: 0.24277694523334503\n",
            "Epoch 103060 | Loss: 0.17336979508399963 | Test loss: 0.24287629127502441\n",
            "Epoch 103070 | Loss: 0.1730152666568756 | Test loss: 0.2429756373167038\n",
            "Epoch 103080 | Loss: 0.17266078293323517 | Test loss: 0.24307501316070557\n",
            "Epoch 103090 | Loss: 0.17230626940727234 | Test loss: 0.24317438900470734\n",
            "Epoch 103100 | Loss: 0.1719520539045334 | Test loss: 0.24327166378498077\n",
            "Epoch 103110 | Loss: 0.17160964012145996 | Test loss: 0.2433510571718216\n",
            "Epoch 103120 | Loss: 0.17126719653606415 | Test loss: 0.24343040585517883\n",
            "Epoch 103130 | Loss: 0.17092476785182953 | Test loss: 0.2435096949338913\n",
            "Epoch 103140 | Loss: 0.17058229446411133 | Test loss: 0.24358899891376495\n",
            "Epoch 103150 | Loss: 0.1702398806810379 | Test loss: 0.2436683177947998\n",
            "Epoch 103160 | Loss: 0.1698974221944809 | Test loss: 0.24374771118164062\n",
            "Epoch 103170 | Loss: 0.16955499351024628 | Test loss: 0.2438269704580307\n",
            "Epoch 103180 | Loss: 0.16921257972717285 | Test loss: 0.24390633404254913\n",
            "Epoch 103190 | Loss: 0.16887009143829346 | Test loss: 0.24398568272590637\n",
            "Epoch 103200 | Loss: 0.16852766275405884 | Test loss: 0.24406497180461884\n",
            "Epoch 103210 | Loss: 0.1681918352842331 | Test loss: 0.244132861495018\n",
            "Epoch 103220 | Loss: 0.16785673797130585 | Test loss: 0.24420081079006195\n",
            "Epoch 103230 | Loss: 0.16752159595489502 | Test loss: 0.24426868557929993\n",
            "Epoch 103240 | Loss: 0.16718648374080658 | Test loss: 0.24433660507202148\n",
            "Epoch 103250 | Loss: 0.16685137152671814 | Test loss: 0.24440453946590424\n",
            "Epoch 103260 | Loss: 0.1665162593126297 | Test loss: 0.24447236955165863\n",
            "Epoch 103270 | Loss: 0.16618114709854126 | Test loss: 0.2445402592420578\n",
            "Epoch 103280 | Loss: 0.16584603488445282 | Test loss: 0.24460819363594055\n",
            "Epoch 103290 | Loss: 0.165510892868042 | Test loss: 0.24467603862285614\n",
            "Epoch 103300 | Loss: 0.16517579555511475 | Test loss: 0.24474413692951202\n",
            "Epoch 103310 | Loss: 0.1648460328578949 | Test loss: 0.24480345845222473\n",
            "Epoch 103320 | Loss: 0.1645185500383377 | Test loss: 0.24486087262630463\n",
            "Epoch 103330 | Loss: 0.16419105231761932 | Test loss: 0.24491827189922333\n",
            "Epoch 103340 | Loss: 0.16386356949806213 | Test loss: 0.24497568607330322\n",
            "Epoch 103350 | Loss: 0.16353605687618256 | Test loss: 0.24503310024738312\n",
            "Epoch 103360 | Loss: 0.16320857405662537 | Test loss: 0.245090514421463\n",
            "Epoch 103370 | Loss: 0.16288109123706818 | Test loss: 0.2451479285955429\n",
            "Epoch 103380 | Loss: 0.1625535935163498 | Test loss: 0.2452053278684616\n",
            "Epoch 103390 | Loss: 0.1622261106967926 | Test loss: 0.2452627420425415\n",
            "Epoch 103400 | Loss: 0.16189859807491302 | Test loss: 0.2453201562166214\n",
            "Epoch 103410 | Loss: 0.1615758091211319 | Test loss: 0.24536757171154022\n",
            "Epoch 103420 | Loss: 0.16125932335853577 | Test loss: 0.24540495872497559\n",
            "Epoch 103430 | Loss: 0.16094286739826202 | Test loss: 0.24544230103492737\n",
            "Epoch 103440 | Loss: 0.1606263965368271 | Test loss: 0.24547968804836273\n",
            "Epoch 103450 | Loss: 0.16030989587306976 | Test loss: 0.2455170899629593\n",
            "Epoch 103460 | Loss: 0.15999341011047363 | Test loss: 0.24555444717407227\n",
            "Epoch 103470 | Loss: 0.1596769541501999 | Test loss: 0.24559186398983002\n",
            "Epoch 103480 | Loss: 0.15936045348644257 | Test loss: 0.24562929570674896\n",
            "Epoch 103490 | Loss: 0.15904399752616882 | Test loss: 0.24566666781902313\n",
            "Epoch 103500 | Loss: 0.1587275117635727 | Test loss: 0.2457040548324585\n",
            "Epoch 103510 | Loss: 0.15841104090213776 | Test loss: 0.24574144184589386\n",
            "Epoch 103520 | Loss: 0.15810099244117737 | Test loss: 0.24576838314533234\n",
            "Epoch 103530 | Loss: 0.15779165923595428 | Test loss: 0.24579524993896484\n",
            "Epoch 103540 | Loss: 0.15748231112957 | Test loss: 0.24582219123840332\n",
            "Epoch 103550 | Loss: 0.15717297792434692 | Test loss: 0.24584904313087463\n",
            "Epoch 103560 | Loss: 0.15686361491680145 | Test loss: 0.2458759993314743\n",
            "Epoch 103570 | Loss: 0.15655426681041718 | Test loss: 0.24590285122394562\n",
            "Epoch 103580 | Loss: 0.1562449038028717 | Test loss: 0.24592983722686768\n",
            "Epoch 103590 | Loss: 0.15593555569648743 | Test loss: 0.2459566593170166\n",
            "Epoch 103600 | Loss: 0.15562622249126434 | Test loss: 0.24598360061645508\n",
            "Epoch 103610 | Loss: 0.15531687438488007 | Test loss: 0.24601049721240997\n",
            "Epoch 103620 | Loss: 0.1550096571445465 | Test loss: 0.24603329598903656\n",
            "Epoch 103630 | Loss: 0.15470723807811737 | Test loss: 0.24604974687099457\n",
            "Epoch 103640 | Loss: 0.15440483391284943 | Test loss: 0.2460661679506302\n",
            "Epoch 103650 | Loss: 0.15410242974758148 | Test loss: 0.24608257412910461\n",
            "Epoch 103660 | Loss: 0.15380001068115234 | Test loss: 0.2460990697145462\n",
            "Epoch 103670 | Loss: 0.1534976214170456 | Test loss: 0.24611543118953705\n",
            "Epoch 103680 | Loss: 0.15319523215293884 | Test loss: 0.24613185226917267\n",
            "Epoch 103690 | Loss: 0.1528928130865097 | Test loss: 0.2461482733488083\n",
            "Epoch 103700 | Loss: 0.15259040892124176 | Test loss: 0.24616466462612152\n",
            "Epoch 103710 | Loss: 0.15228800475597382 | Test loss: 0.24618113040924072\n",
            "Epoch 103720 | Loss: 0.15198558568954468 | Test loss: 0.24619753658771515\n",
            "Epoch 103730 | Loss: 0.15168969333171844 | Test loss: 0.24620048701763153\n",
            "Epoch 103740 | Loss: 0.15139774978160858 | Test loss: 0.2461979240179062\n",
            "Epoch 103750 | Loss: 0.1511058360338211 | Test loss: 0.24619527161121368\n",
            "Epoch 103760 | Loss: 0.15081390738487244 | Test loss: 0.24619272351264954\n",
            "Epoch 103770 | Loss: 0.15052197873592377 | Test loss: 0.24618999660015106\n",
            "Epoch 103780 | Loss: 0.1502300500869751 | Test loss: 0.24618731439113617\n",
            "Epoch 103790 | Loss: 0.14993813633918762 | Test loss: 0.24618469178676605\n",
            "Epoch 103800 | Loss: 0.14964620769023895 | Test loss: 0.24618203938007355\n",
            "Epoch 103810 | Loss: 0.1493542641401291 | Test loss: 0.24617937207221985\n",
            "Epoch 103820 | Loss: 0.14906232059001923 | Test loss: 0.2461767941713333\n",
            "Epoch 103830 | Loss: 0.14877042174339294 | Test loss: 0.24617408215999603\n",
            "Epoch 103840 | Loss: 0.1484825313091278 | Test loss: 0.24616408348083496\n",
            "Epoch 103850 | Loss: 0.14819705486297607 | Test loss: 0.24615095555782318\n",
            "Epoch 103860 | Loss: 0.14791157841682434 | Test loss: 0.24613778293132782\n",
            "Epoch 103870 | Loss: 0.1476261466741562 | Test loss: 0.24612465500831604\n",
            "Epoch 103880 | Loss: 0.14734061062335968 | Test loss: 0.24611149728298187\n",
            "Epoch 103890 | Loss: 0.14705514907836914 | Test loss: 0.24609839916229248\n",
            "Epoch 103900 | Loss: 0.14676964282989502 | Test loss: 0.2460852414369583\n",
            "Epoch 103910 | Loss: 0.14648418128490448 | Test loss: 0.2460721731185913\n",
            "Epoch 103920 | Loss: 0.14619866013526917 | Test loss: 0.24605897068977356\n",
            "Epoch 103930 | Loss: 0.14591321349143982 | Test loss: 0.24604582786560059\n",
            "Epoch 103940 | Loss: 0.14562776684761047 | Test loss: 0.24603271484375\n",
            "Epoch 103950 | Loss: 0.1453457623720169 | Test loss: 0.24601328372955322\n",
            "Epoch 103960 | Loss: 0.1450665295124054 | Test loss: 0.24598966538906097\n",
            "Epoch 103970 | Loss: 0.14478731155395508 | Test loss: 0.24596603214740753\n",
            "Epoch 103980 | Loss: 0.14450809359550476 | Test loss: 0.2459423840045929\n",
            "Epoch 103990 | Loss: 0.14422887563705444 | Test loss: 0.24591882526874542\n",
            "Epoch 104000 | Loss: 0.1439497023820877 | Test loss: 0.2458951771259308\n",
            "Epoch 104010 | Loss: 0.14367042481899261 | Test loss: 0.24587154388427734\n",
            "Epoch 104020 | Loss: 0.1433912217617035 | Test loss: 0.2458479255437851\n",
            "Epoch 104030 | Loss: 0.14311204850673676 | Test loss: 0.2458241730928421\n",
            "Epoch 104040 | Loss: 0.14283278584480286 | Test loss: 0.2458006590604782\n",
            "Epoch 104050 | Loss: 0.14255358278751373 | Test loss: 0.24577705562114716\n",
            "Epoch 104060 | Loss: 0.14227910339832306 | Test loss: 0.2457420378923416\n",
            "Epoch 104070 | Loss: 0.14200931787490845 | Test loss: 0.24569933116436005\n",
            "Epoch 104080 | Loss: 0.14173954725265503 | Test loss: 0.24565660953521729\n",
            "Epoch 104090 | Loss: 0.1414697766304016 | Test loss: 0.2456139624118805\n",
            "Epoch 104100 | Loss: 0.1412000060081482 | Test loss: 0.24557125568389893\n",
            "Epoch 104110 | Loss: 0.14093023538589478 | Test loss: 0.24552853405475616\n",
            "Epoch 104120 | Loss: 0.14066043496131897 | Test loss: 0.24548585712909698\n",
            "Epoch 104130 | Loss: 0.14039066433906555 | Test loss: 0.24544315040111542\n",
            "Epoch 104140 | Loss: 0.14012087881565094 | Test loss: 0.24540047347545624\n",
            "Epoch 104150 | Loss: 0.13985112309455872 | Test loss: 0.24535775184631348\n",
            "Epoch 104160 | Loss: 0.1395813524723053 | Test loss: 0.2453150600194931\n",
            "Epoch 104170 | Loss: 0.13931156694889069 | Test loss: 0.24527235329151154\n",
            "Epoch 104180 | Loss: 0.13904769718647003 | Test loss: 0.2452201396226883\n",
            "Epoch 104190 | Loss: 0.1387842893600464 | Test loss: 0.24516792595386505\n",
            "Epoch 104200 | Loss: 0.13852079212665558 | Test loss: 0.24511566758155823\n",
            "Epoch 104210 | Loss: 0.13825730979442596 | Test loss: 0.2450634241104126\n",
            "Epoch 104220 | Loss: 0.13799379765987396 | Test loss: 0.24501124024391174\n",
            "Epoch 104230 | Loss: 0.13773030042648315 | Test loss: 0.24495895206928253\n",
            "Epoch 104240 | Loss: 0.13746681809425354 | Test loss: 0.24490678310394287\n",
            "Epoch 104250 | Loss: 0.13720329105854034 | Test loss: 0.24485452473163605\n",
            "Epoch 104260 | Loss: 0.13693982362747192 | Test loss: 0.24480228126049042\n",
            "Epoch 104270 | Loss: 0.13667632639408112 | Test loss: 0.24475005269050598\n",
            "Epoch 104280 | Loss: 0.1364128142595291 | Test loss: 0.24469788372516632\n",
            "Epoch 104290 | Loss: 0.13615193963050842 | Test loss: 0.24464085698127747\n",
            "Epoch 104300 | Loss: 0.1358945369720459 | Test loss: 0.24457915127277374\n",
            "Epoch 104310 | Loss: 0.1356372833251953 | Test loss: 0.2445172518491745\n",
            "Epoch 104320 | Loss: 0.13537971675395966 | Test loss: 0.244455486536026\n",
            "Epoch 104330 | Loss: 0.13512229919433594 | Test loss: 0.2443937361240387\n",
            "Epoch 104340 | Loss: 0.1348649114370346 | Test loss: 0.24433200061321259\n",
            "Epoch 104350 | Loss: 0.1346074789762497 | Test loss: 0.2442702054977417\n",
            "Epoch 104360 | Loss: 0.13435009121894836 | Test loss: 0.2442084401845932\n",
            "Epoch 104370 | Loss: 0.13409268856048584 | Test loss: 0.2441466599702835\n",
            "Epoch 104380 | Loss: 0.1338353008031845 | Test loss: 0.2440849095582962\n",
            "Epoch 104390 | Loss: 0.1335778832435608 | Test loss: 0.2440231591463089\n",
            "Epoch 104400 | Loss: 0.13332045078277588 | Test loss: 0.243961364030838\n",
            "Epoch 104410 | Loss: 0.13306759297847748 | Test loss: 0.2438901662826538\n",
            "Epoch 104420 | Loss: 0.13281558454036713 | Test loss: 0.24381791055202484\n",
            "Epoch 104430 | Loss: 0.13256359100341797 | Test loss: 0.2437455952167511\n",
            "Epoch 104440 | Loss: 0.13231158256530762 | Test loss: 0.2436734437942505\n",
            "Epoch 104450 | Loss: 0.13205957412719727 | Test loss: 0.24360115826129913\n",
            "Epoch 104460 | Loss: 0.1318075954914093 | Test loss: 0.24352891743183136\n",
            "Epoch 104470 | Loss: 0.13155558705329895 | Test loss: 0.2434566766023636\n",
            "Epoch 104480 | Loss: 0.13130353391170502 | Test loss: 0.24338440597057343\n",
            "Epoch 104490 | Loss: 0.13105155527591705 | Test loss: 0.24331215023994446\n",
            "Epoch 104500 | Loss: 0.13079951703548431 | Test loss: 0.24323990941047668\n",
            "Epoch 104510 | Loss: 0.13054752349853516 | Test loss: 0.24316763877868652\n",
            "Epoch 104520 | Loss: 0.1302972286939621 | Test loss: 0.2430896759033203\n",
            "Epoch 104530 | Loss: 0.1300533413887024 | Test loss: 0.24299834668636322\n",
            "Epoch 104540 | Loss: 0.1298094242811203 | Test loss: 0.24290700256824493\n",
            "Epoch 104550 | Loss: 0.1295655369758606 | Test loss: 0.24281571805477142\n",
            "Epoch 104560 | Loss: 0.1293216198682785 | Test loss: 0.24272434413433075\n",
            "Epoch 104570 | Loss: 0.12907774746418 | Test loss: 0.24263302981853485\n",
            "Epoch 104580 | Loss: 0.1288338452577591 | Test loss: 0.24254174530506134\n",
            "Epoch 104590 | Loss: 0.128589928150177 | Test loss: 0.24245034158229828\n",
            "Epoch 104600 | Loss: 0.12834599614143372 | Test loss: 0.24235907196998596\n",
            "Epoch 104610 | Loss: 0.1281021386384964 | Test loss: 0.24226775765419006\n",
            "Epoch 104620 | Loss: 0.1278582215309143 | Test loss: 0.24217639863491058\n",
            "Epoch 104630 | Loss: 0.1276143491268158 | Test loss: 0.2420850545167923\n",
            "Epoch 104640 | Loss: 0.12737075984477997 | Test loss: 0.24199278652668\n",
            "Epoch 104650 | Loss: 0.12713277339935303 | Test loss: 0.2418929785490036\n",
            "Epoch 104660 | Loss: 0.12689481675624847 | Test loss: 0.24179306626319885\n",
            "Epoch 104670 | Loss: 0.12665681540966034 | Test loss: 0.2416931688785553\n",
            "Epoch 104680 | Loss: 0.1264188587665558 | Test loss: 0.24159324169158936\n",
            "Epoch 104690 | Loss: 0.12618087232112885 | Test loss: 0.2414933443069458\n",
            "Epoch 104700 | Loss: 0.1259429156780243 | Test loss: 0.24139343202114105\n",
            "Epoch 104710 | Loss: 0.12570492923259735 | Test loss: 0.2412935048341751\n",
            "Epoch 104720 | Loss: 0.1254669576883316 | Test loss: 0.24119357764720917\n",
            "Epoch 104730 | Loss: 0.12522898614406586 | Test loss: 0.24109363555908203\n",
            "Epoch 104740 | Loss: 0.12499099969863892 | Test loss: 0.24099373817443848\n",
            "Epoch 104750 | Loss: 0.12475302070379257 | Test loss: 0.2408938705921173\n",
            "Epoch 104760 | Loss: 0.12451505661010742 | Test loss: 0.24079389870166779\n",
            "Epoch 104770 | Loss: 0.1242818832397461 | Test loss: 0.24068443477153778\n",
            "Epoch 104780 | Loss: 0.12404914945363998 | Test loss: 0.24057500064373016\n",
            "Epoch 104790 | Loss: 0.12381627410650253 | Test loss: 0.24046556651592255\n",
            "Epoch 104800 | Loss: 0.12358362972736359 | Test loss: 0.24035611748695374\n",
            "Epoch 104810 | Loss: 0.12335091084241867 | Test loss: 0.24024665355682373\n",
            "Epoch 104820 | Loss: 0.12311817705631256 | Test loss: 0.24013713002204895\n",
            "Epoch 104830 | Loss: 0.12288542091846466 | Test loss: 0.24002771079540253\n",
            "Epoch 104840 | Loss: 0.12265268713235855 | Test loss: 0.2399182766675949\n",
            "Epoch 104850 | Loss: 0.12241995334625244 | Test loss: 0.2398088425397873\n",
            "Epoch 104860 | Loss: 0.12218721956014633 | Test loss: 0.23969939351081848\n",
            "Epoch 104870 | Loss: 0.12195447832345963 | Test loss: 0.23958992958068848\n",
            "Epoch 104880 | Loss: 0.12172167748212814 | Test loss: 0.23948049545288086\n",
            "Epoch 104890 | Loss: 0.12149371951818466 | Test loss: 0.23935675621032715\n",
            "Epoch 104900 | Loss: 0.12126808613538742 | Test loss: 0.2392282485961914\n",
            "Epoch 104910 | Loss: 0.1210423931479454 | Test loss: 0.23909981548786163\n",
            "Epoch 104920 | Loss: 0.12081670761108398 | Test loss: 0.23897115886211395\n",
            "Epoch 104930 | Loss: 0.12059104442596436 | Test loss: 0.23884260654449463\n",
            "Epoch 104940 | Loss: 0.12036539614200592 | Test loss: 0.2387140840291977\n",
            "Epoch 104950 | Loss: 0.1201397180557251 | Test loss: 0.23858562111854553\n",
            "Epoch 104960 | Loss: 0.11991406977176666 | Test loss: 0.23845703899860382\n",
            "Epoch 104970 | Loss: 0.11968841403722763 | Test loss: 0.23832853138446808\n",
            "Epoch 104980 | Loss: 0.1194627434015274 | Test loss: 0.23819999396800995\n",
            "Epoch 104990 | Loss: 0.11923704296350479 | Test loss: 0.238071471452713\n",
            "Epoch 105000 | Loss: 0.11901137977838516 | Test loss: 0.23794296383857727\n",
            "Epoch 105010 | Loss: 0.11878573894500732 | Test loss: 0.23781442642211914\n",
            "Epoch 105020 | Loss: 0.11856435984373093 | Test loss: 0.23767636716365814\n",
            "Epoch 105030 | Loss: 0.11834345012903214 | Test loss: 0.23753826320171356\n",
            "Epoch 105040 | Loss: 0.11812251061201096 | Test loss: 0.23740027844905853\n",
            "Epoch 105050 | Loss: 0.11790156364440918 | Test loss: 0.23726217448711395\n",
            "Epoch 105060 | Loss: 0.117680624127388 | Test loss: 0.23712413012981415\n",
            "Epoch 105070 | Loss: 0.1174597293138504 | Test loss: 0.23698604106903076\n",
            "Epoch 105080 | Loss: 0.11723881214857101 | Test loss: 0.23684799671173096\n",
            "Epoch 105090 | Loss: 0.11701788753271103 | Test loss: 0.23670993745326996\n",
            "Epoch 105100 | Loss: 0.11679697036743164 | Test loss: 0.23657190799713135\n",
            "Epoch 105110 | Loss: 0.11657603085041046 | Test loss: 0.23643381893634796\n",
            "Epoch 105120 | Loss: 0.11635511368513107 | Test loss: 0.2362957000732422\n",
            "Epoch 105130 | Loss: 0.11613418906927109 | Test loss: 0.2361576408147812\n",
            "Epoch 105140 | Loss: 0.11591339111328125 | Test loss: 0.2360188066959381\n",
            "Epoch 105150 | Loss: 0.11569755524396896 | Test loss: 0.23587210476398468\n",
            "Epoch 105160 | Loss: 0.11548169702291489 | Test loss: 0.2357255220413208\n",
            "Epoch 105170 | Loss: 0.11526583880186081 | Test loss: 0.2355787605047226\n",
            "Epoch 105180 | Loss: 0.11504995822906494 | Test loss: 0.23543213307857513\n",
            "Epoch 105190 | Loss: 0.11483409255743027 | Test loss: 0.23528547585010529\n",
            "Epoch 105200 | Loss: 0.11461823433637619 | Test loss: 0.23513884842395782\n",
            "Epoch 105210 | Loss: 0.1144023910164833 | Test loss: 0.23499219119548798\n",
            "Epoch 105220 | Loss: 0.11418652534484863 | Test loss: 0.23484551906585693\n",
            "Epoch 105230 | Loss: 0.11397065967321396 | Test loss: 0.23469892144203186\n",
            "Epoch 105240 | Loss: 0.11375480890274048 | Test loss: 0.234552264213562\n",
            "Epoch 105250 | Loss: 0.1135389432311058 | Test loss: 0.23440559208393097\n",
            "Epoch 105260 | Loss: 0.11332307010889053 | Test loss: 0.2342589646577835\n",
            "Epoch 105270 | Loss: 0.11310797929763794 | Test loss: 0.23410867154598236\n",
            "Epoch 105280 | Loss: 0.11289861053228378 | Test loss: 0.2339438945055008\n",
            "Epoch 105290 | Loss: 0.11268935352563858 | Test loss: 0.2337791472673416\n",
            "Epoch 105300 | Loss: 0.11248002201318741 | Test loss: 0.23361431062221527\n",
            "Epoch 105310 | Loss: 0.11227072775363922 | Test loss: 0.2334495335817337\n",
            "Epoch 105320 | Loss: 0.11206138134002686 | Test loss: 0.23328475654125214\n",
            "Epoch 105330 | Loss: 0.11185210198163986 | Test loss: 0.23311996459960938\n",
            "Epoch 105340 | Loss: 0.11164279282093048 | Test loss: 0.23295529186725616\n",
            "Epoch 105350 | Loss: 0.11143343895673752 | Test loss: 0.2327904999256134\n",
            "Epoch 105360 | Loss: 0.11122412979602814 | Test loss: 0.23262567818164825\n",
            "Epoch 105370 | Loss: 0.11101482063531876 | Test loss: 0.23246093094348907\n",
            "Epoch 105380 | Loss: 0.11080548912286758 | Test loss: 0.23229606449604034\n",
            "Epoch 105390 | Loss: 0.1105962023139 | Test loss: 0.23213136196136475\n",
            "Epoch 105400 | Loss: 0.11038687080144882 | Test loss: 0.23196649551391602\n",
            "Epoch 105410 | Loss: 0.11017966270446777 | Test loss: 0.2317960560321808\n",
            "Epoch 105420 | Loss: 0.10997440665960312 | Test loss: 0.23162177205085754\n",
            "Epoch 105430 | Loss: 0.10976918041706085 | Test loss: 0.23144744336605072\n",
            "Epoch 105440 | Loss: 0.10956387966871262 | Test loss: 0.23127315938472748\n",
            "Epoch 105450 | Loss: 0.10935866832733154 | Test loss: 0.23109884560108185\n",
            "Epoch 105460 | Loss: 0.10915341228246689 | Test loss: 0.23092447221279144\n",
            "Epoch 105470 | Loss: 0.10894818603992462 | Test loss: 0.2307501584291458\n",
            "Epoch 105480 | Loss: 0.10874290764331818 | Test loss: 0.23057584464550018\n",
            "Epoch 105490 | Loss: 0.1085376963019371 | Test loss: 0.23040151596069336\n",
            "Epoch 105500 | Loss: 0.10833244770765305 | Test loss: 0.2302273064851761\n",
            "Epoch 105510 | Loss: 0.1081271544098854 | Test loss: 0.23005297780036926\n",
            "Epoch 105520 | Loss: 0.10792191326618195 | Test loss: 0.22987861931324005\n",
            "Epoch 105530 | Loss: 0.10771667957305908 | Test loss: 0.2297043353319168\n",
            "Epoch 105540 | Loss: 0.10751140117645264 | Test loss: 0.22952993214130402\n",
            "Epoch 105550 | Loss: 0.10730984061956406 | Test loss: 0.22934792935848236\n",
            "Epoch 105560 | Loss: 0.1071089506149292 | Test loss: 0.22916507720947266\n",
            "Epoch 105570 | Loss: 0.10690809786319733 | Test loss: 0.2289821356534958\n",
            "Epoch 105580 | Loss: 0.10670721530914307 | Test loss: 0.22879917919635773\n",
            "Epoch 105590 | Loss: 0.1065063625574112 | Test loss: 0.2286163568496704\n",
            "Epoch 105600 | Loss: 0.10630548000335693 | Test loss: 0.22843341529369354\n",
            "Epoch 105610 | Loss: 0.10610460489988327 | Test loss: 0.22825054824352264\n",
            "Epoch 105620 | Loss: 0.1059037446975708 | Test loss: 0.22806763648986816\n",
            "Epoch 105630 | Loss: 0.10570287704467773 | Test loss: 0.22788472473621368\n",
            "Epoch 105640 | Loss: 0.10550200939178467 | Test loss: 0.2277018278837204\n",
            "Epoch 105650 | Loss: 0.10530116409063339 | Test loss: 0.22751891613006592\n",
            "Epoch 105660 | Loss: 0.10510027408599854 | Test loss: 0.22733603417873383\n",
            "Epoch 105670 | Loss: 0.10489942133426666 | Test loss: 0.22715309262275696\n",
            "Epoch 105680 | Loss: 0.10469876974821091 | Test loss: 0.22696839272975922\n",
            "Epoch 105690 | Loss: 0.10450338572263718 | Test loss: 0.22676734626293182\n",
            "Epoch 105700 | Loss: 0.10430803149938583 | Test loss: 0.2265663594007492\n",
            "Epoch 105710 | Loss: 0.10411267727613449 | Test loss: 0.22636531293392181\n",
            "Epoch 105720 | Loss: 0.10391733795404434 | Test loss: 0.22616426646709442\n",
            "Epoch 105730 | Loss: 0.10372195392847061 | Test loss: 0.2259632647037506\n",
            "Epoch 105740 | Loss: 0.10352661460638046 | Test loss: 0.22576220333576202\n",
            "Epoch 105750 | Loss: 0.10333123058080673 | Test loss: 0.22556118667125702\n",
            "Epoch 105760 | Loss: 0.10313587635755539 | Test loss: 0.2253602296113968\n",
            "Epoch 105770 | Loss: 0.10294049233198166 | Test loss: 0.225159153342247\n",
            "Epoch 105780 | Loss: 0.1027451753616333 | Test loss: 0.22495810687541962\n",
            "Epoch 105790 | Loss: 0.10254979133605957 | Test loss: 0.2247571051120758\n",
            "Epoch 105800 | Loss: 0.10235440731048584 | Test loss: 0.22455604374408722\n",
            "Epoch 105810 | Loss: 0.1021590605378151 | Test loss: 0.22435501217842102\n",
            "Epoch 105820 | Loss: 0.10196372121572495 | Test loss: 0.2241540253162384\n",
            "Epoch 105830 | Loss: 0.10177016258239746 | Test loss: 0.22394876182079315\n",
            "Epoch 105840 | Loss: 0.10157870501279831 | Test loss: 0.2237391322851181\n",
            "Epoch 105850 | Loss: 0.10138722509145737 | Test loss: 0.22352956235408783\n",
            "Epoch 105860 | Loss: 0.10119576752185822 | Test loss: 0.2233198881149292\n",
            "Epoch 105870 | Loss: 0.10100429505109787 | Test loss: 0.22311027348041534\n",
            "Epoch 105880 | Loss: 0.10081279277801514 | Test loss: 0.22290067374706268\n",
            "Epoch 105890 | Loss: 0.10062133520841599 | Test loss: 0.22269104421138763\n",
            "Epoch 105900 | Loss: 0.10042985528707504 | Test loss: 0.22248147428035736\n",
            "Epoch 105910 | Loss: 0.10023839771747589 | Test loss: 0.22227180004119873\n",
            "Epoch 105920 | Loss: 0.10004692524671555 | Test loss: 0.22206218540668488\n",
            "Epoch 105930 | Loss: 0.09985543042421341 | Test loss: 0.22185258567333221\n",
            "Epoch 105940 | Loss: 0.09966396540403366 | Test loss: 0.22164295613765717\n",
            "Epoch 105950 | Loss: 0.09947248548269272 | Test loss: 0.2214333862066269\n",
            "Epoch 105960 | Loss: 0.09928102046251297 | Test loss: 0.22122371196746826\n",
            "Epoch 105970 | Loss: 0.09908954799175262 | Test loss: 0.2210140973329544\n",
            "Epoch 105980 | Loss: 0.09890083968639374 | Test loss: 0.2207975685596466\n",
            "Epoch 105990 | Loss: 0.09871307015419006 | Test loss: 0.22057943046092987\n",
            "Epoch 106000 | Loss: 0.09852524846792221 | Test loss: 0.22036121785640717\n",
            "Epoch 106010 | Loss: 0.09833749383687973 | Test loss: 0.22014300525188446\n",
            "Epoch 106020 | Loss: 0.09814976900815964 | Test loss: 0.21992479264736176\n",
            "Epoch 106030 | Loss: 0.09796196967363358 | Test loss: 0.21970660984516144\n",
            "Epoch 106040 | Loss: 0.0977742001414299 | Test loss: 0.21948836743831635\n",
            "Epoch 106050 | Loss: 0.09758640825748444 | Test loss: 0.21927018463611603\n",
            "Epoch 106060 | Loss: 0.09739862382411957 | Test loss: 0.21905207633972168\n",
            "Epoch 106070 | Loss: 0.09721087664365768 | Test loss: 0.21883375942707062\n",
            "Epoch 106080 | Loss: 0.09702307730913162 | Test loss: 0.2186155617237091\n",
            "Epoch 106090 | Loss: 0.09683528542518616 | Test loss: 0.2183973342180252\n",
            "Epoch 106100 | Loss: 0.09664750844240189 | Test loss: 0.2181791514158249\n",
            "Epoch 106110 | Loss: 0.09645974636077881 | Test loss: 0.2179609090089798\n",
            "Epoch 106120 | Loss: 0.09627198427915573 | Test loss: 0.21774275600910187\n",
            "Epoch 106130 | Loss: 0.09608878940343857 | Test loss: 0.21750736236572266\n",
            "Epoch 106140 | Loss: 0.09590599685907364 | Test loss: 0.21727202832698822\n",
            "Epoch 106150 | Loss: 0.09572308510541916 | Test loss: 0.21703658998012543\n",
            "Epoch 106160 | Loss: 0.09554033726453781 | Test loss: 0.21680119633674622\n",
            "Epoch 106170 | Loss: 0.0953575000166893 | Test loss: 0.2165658324956894\n",
            "Epoch 106180 | Loss: 0.09517472237348557 | Test loss: 0.2163303941488266\n",
            "Epoch 106190 | Loss: 0.09499188512563705 | Test loss: 0.21609504520893097\n",
            "Epoch 106200 | Loss: 0.09480907768011093 | Test loss: 0.21585963666439056\n",
            "Epoch 106210 | Loss: 0.09462624788284302 | Test loss: 0.21562425792217255\n",
            "Epoch 106220 | Loss: 0.09444344788789749 | Test loss: 0.21538880467414856\n",
            "Epoch 106230 | Loss: 0.09426061064004898 | Test loss: 0.2151535302400589\n",
            "Epoch 106240 | Loss: 0.09407778084278107 | Test loss: 0.2149181216955185\n",
            "Epoch 106250 | Loss: 0.09389495849609375 | Test loss: 0.21468274295330048\n",
            "Epoch 106260 | Loss: 0.09371217340230942 | Test loss: 0.21444733440876007\n",
            "Epoch 106270 | Loss: 0.0935293436050415 | Test loss: 0.21421194076538086\n",
            "Epoch 106280 | Loss: 0.09334661811590195 | Test loss: 0.2139757126569748\n",
            "Epoch 106290 | Loss: 0.09316699951887131 | Test loss: 0.2137317955493927\n",
            "Epoch 106300 | Loss: 0.09298738092184067 | Test loss: 0.21348778903484344\n",
            "Epoch 106310 | Loss: 0.09280776232481003 | Test loss: 0.213243767619133\n",
            "Epoch 106320 | Loss: 0.09262813627719879 | Test loss: 0.2129998505115509\n",
            "Epoch 106330 | Loss: 0.09244850277900696 | Test loss: 0.21275587379932404\n",
            "Epoch 106340 | Loss: 0.09226889908313751 | Test loss: 0.21251189708709717\n",
            "Epoch 106350 | Loss: 0.09208928793668747 | Test loss: 0.2122679203748703\n",
            "Epoch 106360 | Loss: 0.09190960973501205 | Test loss: 0.21202395856380463\n",
            "Epoch 106370 | Loss: 0.0917300209403038 | Test loss: 0.21177998185157776\n",
            "Epoch 106380 | Loss: 0.09155040234327316 | Test loss: 0.2115360051393509\n",
            "Epoch 106390 | Loss: 0.09137079119682312 | Test loss: 0.21129210293293\n",
            "Epoch 106400 | Loss: 0.09119117259979248 | Test loss: 0.21104808151721954\n",
            "Epoch 106410 | Loss: 0.09101152420043945 | Test loss: 0.2108040601015091\n",
            "Epoch 106420 | Loss: 0.0908319279551506 | Test loss: 0.21056006848812103\n",
            "Epoch 106430 | Loss: 0.09065231680870056 | Test loss: 0.21031610667705536\n",
            "Epoch 106440 | Loss: 0.09047269076108932 | Test loss: 0.21007215976715088\n",
            "Epoch 106450 | Loss: 0.09029589593410492 | Test loss: 0.20981955528259277\n",
            "Epoch 106460 | Loss: 0.09011931717395782 | Test loss: 0.20956699550151825\n",
            "Epoch 106470 | Loss: 0.08994267880916595 | Test loss: 0.20931442081928253\n",
            "Epoch 106480 | Loss: 0.08976607769727707 | Test loss: 0.2090618908405304\n",
            "Epoch 106490 | Loss: 0.08958936482667923 | Test loss: 0.20880930125713348\n",
            "Epoch 106500 | Loss: 0.08941283077001572 | Test loss: 0.20855675637722015\n",
            "Epoch 106510 | Loss: 0.08923625946044922 | Test loss: 0.20830416679382324\n",
            "Epoch 106520 | Loss: 0.08905977755784988 | Test loss: 0.2080516368150711\n",
            "Epoch 106530 | Loss: 0.08888299018144608 | Test loss: 0.20779910683631897\n",
            "Epoch 106540 | Loss: 0.0887063667178154 | Test loss: 0.20754650235176086\n",
            "Epoch 106550 | Loss: 0.08852975815534592 | Test loss: 0.20729391276836395\n",
            "Epoch 106560 | Loss: 0.08835316449403763 | Test loss: 0.20704138278961182\n",
            "Epoch 106570 | Loss: 0.08817656338214874 | Test loss: 0.2067888230085373\n",
            "Epoch 106580 | Loss: 0.08799990266561508 | Test loss: 0.20653627812862396\n",
            "Epoch 106590 | Loss: 0.08782331645488739 | Test loss: 0.20628368854522705\n",
            "Epoch 106600 | Loss: 0.08764669299125671 | Test loss: 0.20603112876415253\n",
            "Epoch 106610 | Loss: 0.08747246116399765 | Test loss: 0.20577247440814972\n",
            "Epoch 106620 | Loss: 0.0872991755604744 | Test loss: 0.20551221072673798\n",
            "Epoch 106630 | Loss: 0.08712584525346756 | Test loss: 0.205252006649971\n",
            "Epoch 106640 | Loss: 0.08695254474878311 | Test loss: 0.20499181747436523\n",
            "Epoch 106650 | Loss: 0.08677922934293747 | Test loss: 0.20473162829875946\n",
            "Epoch 106660 | Loss: 0.08660590648651123 | Test loss: 0.2044714242219925\n",
            "Epoch 106670 | Loss: 0.08643262833356857 | Test loss: 0.20421123504638672\n",
            "Epoch 106680 | Loss: 0.08625930547714233 | Test loss: 0.20395098626613617\n",
            "Epoch 106690 | Loss: 0.08608599007129669 | Test loss: 0.20369084179401398\n",
            "Epoch 106700 | Loss: 0.08591269701719284 | Test loss: 0.20343057811260223\n",
            "Epoch 106710 | Loss: 0.0857393741607666 | Test loss: 0.20317040383815765\n",
            "Epoch 106720 | Loss: 0.08556608110666275 | Test loss: 0.2029101848602295\n",
            "Epoch 106730 | Loss: 0.08539276570081711 | Test loss: 0.2026500254869461\n",
            "Epoch 106740 | Loss: 0.08521945029497147 | Test loss: 0.20238979160785675\n",
            "Epoch 106750 | Loss: 0.08504615724086761 | Test loss: 0.2021295577287674\n",
            "Epoch 106760 | Loss: 0.08487283438444138 | Test loss: 0.20186935365200043\n",
            "Epoch 106770 | Loss: 0.08470048755407333 | Test loss: 0.20160320401191711\n",
            "Epoch 106780 | Loss: 0.08453080803155899 | Test loss: 0.20132577419281006\n",
            "Epoch 106790 | Loss: 0.08436106890439987 | Test loss: 0.20104829967021942\n",
            "Epoch 106800 | Loss: 0.08419134467840195 | Test loss: 0.2007710039615631\n",
            "Epoch 106810 | Loss: 0.08402162045240402 | Test loss: 0.20049364864826202\n",
            "Epoch 106820 | Loss: 0.0838518887758255 | Test loss: 0.20021624863147736\n",
            "Epoch 106830 | Loss: 0.08368219435214996 | Test loss: 0.19993889331817627\n",
            "Epoch 106840 | Loss: 0.08351244032382965 | Test loss: 0.1996614784002304\n",
            "Epoch 106850 | Loss: 0.08334273099899292 | Test loss: 0.19938407838344574\n",
            "Epoch 106860 | Loss: 0.08317302912473679 | Test loss: 0.19910681247711182\n",
            "Epoch 106870 | Loss: 0.08300328999757767 | Test loss: 0.19882936775684357\n",
            "Epoch 106880 | Loss: 0.08283358067274094 | Test loss: 0.1985519975423813\n",
            "Epoch 106890 | Loss: 0.08266385644674301 | Test loss: 0.19827456772327423\n",
            "Epoch 106900 | Loss: 0.08249413222074509 | Test loss: 0.19799724221229553\n",
            "Epoch 106910 | Loss: 0.08232443034648895 | Test loss: 0.19771981239318848\n",
            "Epoch 106920 | Loss: 0.08215465396642685 | Test loss: 0.1974424421787262\n",
            "Epoch 106930 | Loss: 0.0819849744439125 | Test loss: 0.19716505706310272\n",
            "Epoch 106940 | Loss: 0.08181525021791458 | Test loss: 0.1968877613544464\n",
            "Epoch 106950 | Loss: 0.08164610713720322 | Test loss: 0.19660770893096924\n",
            "Epoch 106960 | Loss: 0.08147867769002914 | Test loss: 0.19632172584533691\n",
            "Epoch 106970 | Loss: 0.08131127804517746 | Test loss: 0.19603577256202698\n",
            "Epoch 106980 | Loss: 0.081143818795681 | Test loss: 0.19574978947639465\n",
            "Epoch 106990 | Loss: 0.08097642660140991 | Test loss: 0.19546382129192352\n",
            "Epoch 107000 | Loss: 0.08080904185771942 | Test loss: 0.19517791271209717\n",
            "Epoch 107010 | Loss: 0.08064161241054535 | Test loss: 0.19489185512065887\n",
            "Epoch 107020 | Loss: 0.08047419041395187 | Test loss: 0.19460590183734894\n",
            "Epoch 107030 | Loss: 0.08030679821968079 | Test loss: 0.1943199634552002\n",
            "Epoch 107040 | Loss: 0.08013936132192612 | Test loss: 0.19403398036956787\n",
            "Epoch 107050 | Loss: 0.07997196167707443 | Test loss: 0.19374799728393555\n",
            "Epoch 107060 | Loss: 0.07980452477931976 | Test loss: 0.19346199929714203\n",
            "Epoch 107070 | Loss: 0.07963711768388748 | Test loss: 0.1931760609149933\n",
            "Epoch 107080 | Loss: 0.0794697254896164 | Test loss: 0.19289007782936096\n",
            "Epoch 107090 | Loss: 0.07930230349302292 | Test loss: 0.19260406494140625\n",
            "Epoch 107100 | Loss: 0.07913487404584885 | Test loss: 0.1923181265592575\n",
            "Epoch 107110 | Loss: 0.07896747440099716 | Test loss: 0.19203217327594757\n",
            "Epoch 107120 | Loss: 0.07880006730556488 | Test loss: 0.19174621999263763\n",
            "Epoch 107130 | Loss: 0.0786326676607132 | Test loss: 0.19145946204662323\n",
            "Epoch 107140 | Loss: 0.07846786826848984 | Test loss: 0.19116584956645966\n",
            "Epoch 107150 | Loss: 0.07830303907394409 | Test loss: 0.19087226688861847\n",
            "Epoch 107160 | Loss: 0.07813820987939835 | Test loss: 0.1905786544084549\n",
            "Epoch 107170 | Loss: 0.07797340303659439 | Test loss: 0.19028501212596893\n",
            "Epoch 107180 | Loss: 0.07780860364437103 | Test loss: 0.18999139964580536\n",
            "Epoch 107190 | Loss: 0.07764377444982529 | Test loss: 0.18969781696796417\n",
            "Epoch 107200 | Loss: 0.07747894525527954 | Test loss: 0.1894042044878006\n",
            "Epoch 107210 | Loss: 0.07731413841247559 | Test loss: 0.18911056220531464\n",
            "Epoch 107220 | Loss: 0.07714933902025223 | Test loss: 0.18881694972515106\n",
            "Epoch 107230 | Loss: 0.07698451727628708 | Test loss: 0.18852336704730988\n",
            "Epoch 107240 | Loss: 0.07681968063116074 | Test loss: 0.1882297545671463\n",
            "Epoch 107250 | Loss: 0.07665487378835678 | Test loss: 0.18793611228466034\n",
            "Epoch 107260 | Loss: 0.07649007439613342 | Test loss: 0.18764249980449677\n",
            "Epoch 107270 | Loss: 0.07632525265216827 | Test loss: 0.18734891712665558\n",
            "Epoch 107280 | Loss: 0.07616042345762253 | Test loss: 0.187055304646492\n",
            "Epoch 107290 | Loss: 0.07599561661481857 | Test loss: 0.18676166236400604\n",
            "Epoch 107300 | Loss: 0.07583080977201462 | Test loss: 0.18646804988384247\n",
            "Epoch 107310 | Loss: 0.07566598802804947 | Test loss: 0.18617446720600128\n",
            "Epoch 107320 | Loss: 0.07550220191478729 | Test loss: 0.18587222695350647\n",
            "Epoch 107330 | Loss: 0.0753399059176445 | Test loss: 0.18556146323680878\n",
            "Epoch 107340 | Loss: 0.07517757266759872 | Test loss: 0.1852506846189499\n",
            "Epoch 107350 | Loss: 0.07501531392335892 | Test loss: 0.184939906001091\n",
            "Epoch 107360 | Loss: 0.07485302537679672 | Test loss: 0.18462912738323212\n",
            "Epoch 107370 | Loss: 0.07469085603952408 | Test loss: 0.18431828916072845\n",
            "Epoch 107380 | Loss: 0.07452841103076935 | Test loss: 0.18400751054286957\n",
            "Epoch 107390 | Loss: 0.07436610758304596 | Test loss: 0.18369679152965546\n",
            "Epoch 107400 | Loss: 0.07420382648706436 | Test loss: 0.1833859235048294\n",
            "Epoch 107410 | Loss: 0.07404151558876038 | Test loss: 0.18307511508464813\n",
            "Epoch 107420 | Loss: 0.07387922704219818 | Test loss: 0.18276432156562805\n",
            "Epoch 107430 | Loss: 0.07371693849563599 | Test loss: 0.18245355784893036\n",
            "Epoch 107440 | Loss: 0.073554627597332 | Test loss: 0.18214304745197296\n",
            "Epoch 107450 | Loss: 0.073392353951931 | Test loss: 0.181831955909729\n",
            "Epoch 107460 | Loss: 0.07323002815246582 | Test loss: 0.18152116239070892\n",
            "Epoch 107470 | Loss: 0.07306775450706482 | Test loss: 0.18121038377285004\n",
            "Epoch 107480 | Loss: 0.07290545850992203 | Test loss: 0.18089960515499115\n",
            "Epoch 107490 | Loss: 0.07274314016103745 | Test loss: 0.1805887669324875\n",
            "Epoch 107500 | Loss: 0.07258086651563644 | Test loss: 0.1802779883146286\n",
            "Epoch 107510 | Loss: 0.07241855561733246 | Test loss: 0.1799672693014145\n",
            "Epoch 107520 | Loss: 0.07225625962018967 | Test loss: 0.17965643107891083\n",
            "Epoch 107530 | Loss: 0.0720948800444603 | Test loss: 0.17934191226959229\n",
            "Epoch 107540 | Loss: 0.07193463295698166 | Test loss: 0.1790233850479126\n",
            "Epoch 107550 | Loss: 0.07177428901195526 | Test loss: 0.17870496213436127\n",
            "Epoch 107560 | Loss: 0.07161420583724976 | Test loss: 0.17838649451732635\n",
            "Epoch 107570 | Loss: 0.0714540183544159 | Test loss: 0.17806808650493622\n",
            "Epoch 107580 | Loss: 0.07129379361867905 | Test loss: 0.17774975299835205\n",
            "Epoch 107590 | Loss: 0.07113354653120041 | Test loss: 0.17743122577667236\n",
            "Epoch 107600 | Loss: 0.07097320258617401 | Test loss: 0.17711280286312103\n",
            "Epoch 107610 | Loss: 0.0708131194114685 | Test loss: 0.17679433524608612\n",
            "Epoch 107620 | Loss: 0.07065293192863464 | Test loss: 0.17647592723369598\n",
            "Epoch 107630 | Loss: 0.0704927071928978 | Test loss: 0.17615759372711182\n",
            "Epoch 107640 | Loss: 0.07033246010541916 | Test loss: 0.17583906650543213\n",
            "Epoch 107650 | Loss: 0.07017211616039276 | Test loss: 0.1755206435918808\n",
            "Epoch 107660 | Loss: 0.07001203298568726 | Test loss: 0.1752021759748459\n",
            "Epoch 107670 | Loss: 0.0698518455028534 | Test loss: 0.17488376796245575\n",
            "Epoch 107680 | Loss: 0.06969162076711655 | Test loss: 0.17456543445587158\n",
            "Epoch 107690 | Loss: 0.06953137367963791 | Test loss: 0.1742469072341919\n",
            "Epoch 107700 | Loss: 0.06937102973461151 | Test loss: 0.17392848432064056\n",
            "Epoch 107710 | Loss: 0.069210946559906 | Test loss: 0.17361001670360565\n",
            "Epoch 107720 | Loss: 0.06905075907707214 | Test loss: 0.17329160869121552\n",
            "Epoch 107730 | Loss: 0.0688905343413353 | Test loss: 0.17297327518463135\n",
            "Epoch 107740 | Loss: 0.06873031705617905 | Test loss: 0.17265407741069794\n",
            "Epoch 107750 | Loss: 0.06857243925333023 | Test loss: 0.17232894897460938\n",
            "Epoch 107760 | Loss: 0.0684145987033844 | Test loss: 0.1720038205385208\n",
            "Epoch 107770 | Loss: 0.06825671344995499 | Test loss: 0.17167870700359344\n",
            "Epoch 107780 | Loss: 0.06809886544942856 | Test loss: 0.17135357856750488\n",
            "Epoch 107790 | Loss: 0.06794100254774094 | Test loss: 0.17102845013141632\n",
            "Epoch 107800 | Loss: 0.06778314709663391 | Test loss: 0.17070333659648895\n",
            "Epoch 107810 | Loss: 0.06762527674436569 | Test loss: 0.17037823796272278\n",
            "Epoch 107820 | Loss: 0.06746742874383926 | Test loss: 0.1700531244277954\n",
            "Epoch 107830 | Loss: 0.06730955094099045 | Test loss: 0.16972792148590088\n",
            "Epoch 107840 | Loss: 0.06715167313814163 | Test loss: 0.1694028377532959\n",
            "Epoch 107850 | Loss: 0.0669938251376152 | Test loss: 0.16907770931720734\n",
            "Epoch 107860 | Loss: 0.06683594733476639 | Test loss: 0.16875262558460236\n",
            "Epoch 107870 | Loss: 0.06667808443307877 | Test loss: 0.16842742264270782\n",
            "Epoch 107880 | Loss: 0.06652020663022995 | Test loss: 0.16810236871242523\n",
            "Epoch 107890 | Loss: 0.06636237353086472 | Test loss: 0.16777725517749786\n",
            "Epoch 107900 | Loss: 0.06620451807975769 | Test loss: 0.16745208203792572\n",
            "Epoch 107910 | Loss: 0.06604664027690887 | Test loss: 0.16712692379951477\n",
            "Epoch 107920 | Loss: 0.06588878482580185 | Test loss: 0.16680185496807098\n",
            "Epoch 107930 | Loss: 0.06573092192411423 | Test loss: 0.16647672653198242\n",
            "Epoch 107940 | Loss: 0.06557305157184601 | Test loss: 0.16615159809589386\n",
            "Epoch 107950 | Loss: 0.06541518121957779 | Test loss: 0.1658264398574829\n",
            "Epoch 107960 | Loss: 0.06525730341672897 | Test loss: 0.16550134122371674\n",
            "Epoch 107970 | Loss: 0.0651007816195488 | Test loss: 0.16515903174877167\n",
            "Epoch 107980 | Loss: 0.06494436413049698 | Test loss: 0.1648167222738266\n",
            "Epoch 107990 | Loss: 0.06478793174028397 | Test loss: 0.16447441279888153\n",
            "Epoch 108000 | Loss: 0.06463148444890976 | Test loss: 0.16413211822509766\n",
            "Epoch 108010 | Loss: 0.06447505205869675 | Test loss: 0.16378982365131378\n",
            "Epoch 108020 | Loss: 0.06431862711906433 | Test loss: 0.1634475290775299\n",
            "Epoch 108030 | Loss: 0.0641622468829155 | Test loss: 0.16310523450374603\n",
            "Epoch 108040 | Loss: 0.0640057846903801 | Test loss: 0.16276292502880096\n",
            "Epoch 108050 | Loss: 0.06384935975074768 | Test loss: 0.1624205857515335\n",
            "Epoch 108060 | Loss: 0.06369295716285706 | Test loss: 0.16207833588123322\n",
            "Epoch 108070 | Loss: 0.06353650987148285 | Test loss: 0.16173601150512695\n",
            "Epoch 108080 | Loss: 0.06338007748126984 | Test loss: 0.1613936871290207\n",
            "Epoch 108090 | Loss: 0.06322365999221802 | Test loss: 0.16105137765407562\n",
            "Epoch 108100 | Loss: 0.0630672350525856 | Test loss: 0.16070909798145294\n",
            "Epoch 108110 | Loss: 0.06291081011295319 | Test loss: 0.16036677360534668\n",
            "Epoch 108120 | Loss: 0.06275437027215958 | Test loss: 0.1600244790315628\n",
            "Epoch 108130 | Loss: 0.06259796023368835 | Test loss: 0.15968215465545654\n",
            "Epoch 108140 | Loss: 0.062441516667604446 | Test loss: 0.15933986008167267\n",
            "Epoch 108150 | Loss: 0.06228507310152054 | Test loss: 0.1589975655078888\n",
            "Epoch 108160 | Loss: 0.06212867051362991 | Test loss: 0.15865524113178253\n",
            "Epoch 108170 | Loss: 0.061972230672836304 | Test loss: 0.15831291675567627\n",
            "Epoch 108180 | Loss: 0.06181580573320389 | Test loss: 0.15797065198421478\n",
            "Epoch 108190 | Loss: 0.06165939196944237 | Test loss: 0.1576283723115921\n",
            "Epoch 108200 | Loss: 0.06150294095277786 | Test loss: 0.15728604793548584\n",
            "Epoch 108210 | Loss: 0.06134652718901634 | Test loss: 0.15694378316402435\n",
            "Epoch 108220 | Loss: 0.06119009479880333 | Test loss: 0.1566014587879181\n",
            "Epoch 108230 | Loss: 0.06103460118174553 | Test loss: 0.1562538892030716\n",
            "Epoch 108240 | Loss: 0.06087953597307205 | Test loss: 0.1559039205312729\n",
            "Epoch 108250 | Loss: 0.06072445586323738 | Test loss: 0.15555398166179657\n",
            "Epoch 108260 | Loss: 0.06056937202811241 | Test loss: 0.15520405769348145\n",
            "Epoch 108270 | Loss: 0.06041431054472923 | Test loss: 0.15485408902168274\n",
            "Epoch 108280 | Loss: 0.06025925278663635 | Test loss: 0.15450413525104523\n",
            "Epoch 108290 | Loss: 0.06010415032505989 | Test loss: 0.1541542112827301\n",
            "Epoch 108300 | Loss: 0.05994909629225731 | Test loss: 0.1538042277097702\n",
            "Epoch 108310 | Loss: 0.059794045984745026 | Test loss: 0.15345428884029388\n",
            "Epoch 108320 | Loss: 0.05963895469903946 | Test loss: 0.15310437977313995\n",
            "Epoch 108330 | Loss: 0.05948387458920479 | Test loss: 0.15275445580482483\n",
            "Epoch 108340 | Loss: 0.05932879447937012 | Test loss: 0.15240447223186493\n",
            "Epoch 108350 | Loss: 0.05917372182011604 | Test loss: 0.15205450356006622\n",
            "Epoch 108360 | Loss: 0.059018637984991074 | Test loss: 0.1517045795917511\n",
            "Epoch 108370 | Loss: 0.058863576501607895 | Test loss: 0.1513546258211136\n",
            "Epoch 108380 | Loss: 0.05870848894119263 | Test loss: 0.15100470185279846\n",
            "Epoch 108390 | Loss: 0.05855340510606766 | Test loss: 0.15065474808216095\n",
            "Epoch 108400 | Loss: 0.05839838460087776 | Test loss: 0.15030479431152344\n",
            "Epoch 108410 | Loss: 0.058243270963430405 | Test loss: 0.1499549150466919\n",
            "Epoch 108420 | Loss: 0.05808818340301514 | Test loss: 0.1496049165725708\n",
            "Epoch 108430 | Loss: 0.057933103293180466 | Test loss: 0.14925499260425568\n",
            "Epoch 108440 | Loss: 0.05777806043624878 | Test loss: 0.14890503883361816\n",
            "Epoch 108450 | Loss: 0.05762296915054321 | Test loss: 0.14855511486530304\n",
            "Epoch 108460 | Loss: 0.05746788904070854 | Test loss: 0.14820511639118195\n",
            "Epoch 108470 | Loss: 0.05731284245848656 | Test loss: 0.14785520732402802\n",
            "Epoch 108480 | Loss: 0.057157766073942184 | Test loss: 0.1475052684545517\n",
            "Epoch 108490 | Loss: 0.05700249597430229 | Test loss: 0.14715532958507538\n",
            "Epoch 108500 | Loss: 0.05684828385710716 | Test loss: 0.14680206775665283\n",
            "Epoch 108510 | Loss: 0.05669480562210083 | Test loss: 0.1464453786611557\n",
            "Epoch 108520 | Loss: 0.056541360914707184 | Test loss: 0.14608879387378693\n",
            "Epoch 108530 | Loss: 0.056387897580862045 | Test loss: 0.14573214948177338\n",
            "Epoch 108540 | Loss: 0.05623443052172661 | Test loss: 0.14537549018859863\n",
            "Epoch 108550 | Loss: 0.056080978363752365 | Test loss: 0.1450188159942627\n",
            "Epoch 108560 | Loss: 0.055927522480487823 | Test loss: 0.14466221630573273\n",
            "Epoch 108570 | Loss: 0.055774059146642685 | Test loss: 0.1443055421113968\n",
            "Epoch 108580 | Loss: 0.05562061071395874 | Test loss: 0.14394891262054443\n",
            "Epoch 108590 | Loss: 0.055467139929533005 | Test loss: 0.14359228312969208\n",
            "Epoch 108600 | Loss: 0.05531367287039757 | Test loss: 0.14323566854000092\n",
            "Epoch 108610 | Loss: 0.05516021326184273 | Test loss: 0.1428789645433426\n",
            "Epoch 108620 | Loss: 0.05500667914748192 | Test loss: 0.14252236485481262\n",
            "Epoch 108630 | Loss: 0.054853279143571854 | Test loss: 0.14216573536396027\n",
            "Epoch 108640 | Loss: 0.054699819535017014 | Test loss: 0.1418091058731079\n",
            "Epoch 108650 | Loss: 0.054546356201171875 | Test loss: 0.14145243167877197\n",
            "Epoch 108660 | Loss: 0.05439291521906853 | Test loss: 0.141095831990242\n",
            "Epoch 108670 | Loss: 0.05423944815993309 | Test loss: 0.14073918759822845\n",
            "Epoch 108680 | Loss: 0.05408599600195885 | Test loss: 0.1403825283050537\n",
            "Epoch 108690 | Loss: 0.05393252521753311 | Test loss: 0.14002589881420135\n",
            "Epoch 108700 | Loss: 0.05377906188368797 | Test loss: 0.1396692544221878\n",
            "Epoch 108710 | Loss: 0.05362563207745552 | Test loss: 0.13931261003017426\n",
            "Epoch 108720 | Loss: 0.0534721314907074 | Test loss: 0.1389559954404831\n",
            "Epoch 108730 | Loss: 0.05331868678331375 | Test loss: 0.13859938085079193\n",
            "Epoch 108740 | Loss: 0.05316522344946861 | Test loss: 0.138242706656456\n",
            "Epoch 108750 | Loss: 0.05301177129149437 | Test loss: 0.13788604736328125\n",
            "Epoch 108760 | Loss: 0.05285831168293953 | Test loss: 0.13752947747707367\n",
            "Epoch 108770 | Loss: 0.0527048297226429 | Test loss: 0.13717277348041534\n",
            "Epoch 108780 | Loss: 0.05255138874053955 | Test loss: 0.136816143989563\n",
            "Epoch 108790 | Loss: 0.05239849165081978 | Test loss: 0.1364465057849884\n",
            "Epoch 108800 | Loss: 0.0522458553314209 | Test loss: 0.13607366383075714\n",
            "Epoch 108810 | Loss: 0.052093178033828735 | Test loss: 0.13570080697536469\n",
            "Epoch 108820 | Loss: 0.05194053053855896 | Test loss: 0.13532789051532745\n",
            "Epoch 108830 | Loss: 0.051787879317998886 | Test loss: 0.1349550038576126\n",
            "Epoch 108840 | Loss: 0.05163523554801941 | Test loss: 0.13458219170570374\n",
            "Epoch 108850 | Loss: 0.051482558250427246 | Test loss: 0.13420931994915009\n",
            "Epoch 108860 | Loss: 0.05132989212870598 | Test loss: 0.13383643329143524\n",
            "Epoch 108870 | Loss: 0.0511772446334362 | Test loss: 0.1334635317325592\n",
            "Epoch 108880 | Loss: 0.05102461203932762 | Test loss: 0.13309071958065033\n",
            "Epoch 108890 | Loss: 0.050871919840574265 | Test loss: 0.1327178031206131\n",
            "Epoch 108900 | Loss: 0.05071928724646568 | Test loss: 0.13234499096870422\n",
            "Epoch 108910 | Loss: 0.050566624850034714 | Test loss: 0.13197211921215057\n",
            "Epoch 108920 | Loss: 0.05041399225592613 | Test loss: 0.13159923255443573\n",
            "Epoch 108930 | Loss: 0.050261326134204865 | Test loss: 0.1312263458967209\n",
            "Epoch 108940 | Loss: 0.0501086600124836 | Test loss: 0.13085345923900604\n",
            "Epoch 108950 | Loss: 0.049956005066633224 | Test loss: 0.13048063218593597\n",
            "Epoch 108960 | Loss: 0.049803365021944046 | Test loss: 0.1301077902317047\n",
            "Epoch 108970 | Loss: 0.049650680273771286 | Test loss: 0.12973490357398987\n",
            "Epoch 108980 | Loss: 0.04949804022908211 | Test loss: 0.1293620616197586\n",
            "Epoch 108990 | Loss: 0.049345340579748154 | Test loss: 0.12898914515972137\n",
            "Epoch 109000 | Loss: 0.04919274523854256 | Test loss: 0.1286163181066513\n",
            "Epoch 109010 | Loss: 0.04904007539153099 | Test loss: 0.1282433718442917\n",
            "Epoch 109020 | Loss: 0.04888742044568062 | Test loss: 0.12787044048309326\n",
            "Epoch 109030 | Loss: 0.04873476177453995 | Test loss: 0.12749770283699036\n",
            "Epoch 109040 | Loss: 0.04858211427927017 | Test loss: 0.12712478637695312\n",
            "Epoch 109050 | Loss: 0.048429425805807114 | Test loss: 0.12675197422504425\n",
            "Epoch 109060 | Loss: 0.04827678203582764 | Test loss: 0.1263791173696518\n",
            "Epoch 109070 | Loss: 0.048124123364686966 | Test loss: 0.12600620090961456\n",
            "Epoch 109080 | Loss: 0.047971487045288086 | Test loss: 0.12563331425189972\n",
            "Epoch 109090 | Loss: 0.047818828374147415 | Test loss: 0.12526045739650726\n",
            "Epoch 109100 | Loss: 0.047666169703006744 | Test loss: 0.1248876079916954\n",
            "Epoch 109110 | Loss: 0.04751352593302727 | Test loss: 0.12451472133398056\n",
            "Epoch 109120 | Loss: 0.04736088588833809 | Test loss: 0.12414181232452393\n",
            "Epoch 109130 | Loss: 0.04720817878842354 | Test loss: 0.12376900017261505\n",
            "Epoch 109140 | Loss: 0.04705556109547615 | Test loss: 0.12339615821838379\n",
            "Epoch 109150 | Loss: 0.046902891248464584 | Test loss: 0.12302296608686447\n",
            "Epoch 109160 | Loss: 0.046750258654356 | Test loss: 0.12265045940876007\n",
            "Epoch 109170 | Loss: 0.04659757763147354 | Test loss: 0.12227749824523926\n",
            "Epoch 109180 | Loss: 0.04644523933529854 | Test loss: 0.1219000369310379\n",
            "Epoch 109190 | Loss: 0.04629320651292801 | Test loss: 0.12151956558227539\n",
            "Epoch 109200 | Loss: 0.046141136437654495 | Test loss: 0.12113900482654572\n",
            "Epoch 109210 | Loss: 0.045989133417606354 | Test loss: 0.12075848877429962\n",
            "Epoch 109220 | Loss: 0.04583708196878433 | Test loss: 0.12037797272205353\n",
            "Epoch 109230 | Loss: 0.0456850528717041 | Test loss: 0.1199975237250328\n",
            "Epoch 109240 | Loss: 0.04553299769759178 | Test loss: 0.11961700767278671\n",
            "Epoch 109250 | Loss: 0.045380957424640656 | Test loss: 0.11923646926879883\n",
            "Epoch 109260 | Loss: 0.04522889479994774 | Test loss: 0.11885600537061691\n",
            "Epoch 109270 | Loss: 0.04507685825228691 | Test loss: 0.11847545951604843\n",
            "Epoch 109280 | Loss: 0.04492480307817459 | Test loss: 0.11809497326612473\n",
            "Epoch 109290 | Loss: 0.04477279260754585 | Test loss: 0.11771447956562042\n",
            "Epoch 109300 | Loss: 0.044620733708143234 | Test loss: 0.11733386665582657\n",
            "Epoch 109310 | Loss: 0.04446868970990181 | Test loss: 0.11695344746112823\n",
            "Epoch 109320 | Loss: 0.04431665688753128 | Test loss: 0.11657290905714035\n",
            "Epoch 109330 | Loss: 0.04416462406516075 | Test loss: 0.11619241535663605\n",
            "Epoch 109340 | Loss: 0.044012557715177536 | Test loss: 0.11581192165613174\n",
            "Epoch 109350 | Loss: 0.043860532343387604 | Test loss: 0.11543140560388565\n",
            "Epoch 109360 | Loss: 0.04370846971869469 | Test loss: 0.11505088955163956\n",
            "Epoch 109370 | Loss: 0.04355644807219505 | Test loss: 0.11467039585113525\n",
            "Epoch 109380 | Loss: 0.043404411524534225 | Test loss: 0.11428983509540558\n",
            "Epoch 109390 | Loss: 0.043252356350421906 | Test loss: 0.11390938609838486\n",
            "Epoch 109400 | Loss: 0.04310030862689018 | Test loss: 0.11352882534265518\n",
            "Epoch 109410 | Loss: 0.042948316782712936 | Test loss: 0.11314830929040909\n",
            "Epoch 109420 | Loss: 0.04279618337750435 | Test loss: 0.1127677708864212\n",
            "Epoch 109430 | Loss: 0.042644329369068146 | Test loss: 0.11238729953765869\n",
            "Epoch 109440 | Loss: 0.042492061853408813 | Test loss: 0.1120067834854126\n",
            "Epoch 109450 | Loss: 0.04234011843800545 | Test loss: 0.11162631958723068\n",
            "Epoch 109460 | Loss: 0.04218805581331253 | Test loss: 0.11124580353498459\n",
            "Epoch 109470 | Loss: 0.042036015540361404 | Test loss: 0.1108652874827385\n",
            "Epoch 109480 | Loss: 0.04188397526741028 | Test loss: 0.11048479378223419\n",
            "Epoch 109490 | Loss: 0.04173194244503975 | Test loss: 0.11010420322418213\n",
            "Epoch 109500 | Loss: 0.04157988354563713 | Test loss: 0.10972373932600021\n",
            "Epoch 109510 | Loss: 0.0414278507232666 | Test loss: 0.10934334248304367\n",
            "Epoch 109520 | Loss: 0.041275788098573685 | Test loss: 0.10896270722150803\n",
            "Epoch 109530 | Loss: 0.04112374782562256 | Test loss: 0.10858223587274551\n",
            "Epoch 109540 | Loss: 0.040971722453832626 | Test loss: 0.10820169746875763\n",
            "Epoch 109550 | Loss: 0.0408196859061718 | Test loss: 0.10782120376825333\n",
            "Epoch 109560 | Loss: 0.040667641907930374 | Test loss: 0.10744066536426544\n",
            "Epoch 109570 | Loss: 0.04051560163497925 | Test loss: 0.10706017166376114\n",
            "Epoch 109580 | Loss: 0.04036355018615723 | Test loss: 0.10667963325977325\n",
            "Epoch 109590 | Loss: 0.04021149501204491 | Test loss: 0.10629913955926895\n",
            "Epoch 109600 | Loss: 0.040059443563222885 | Test loss: 0.10591860115528107\n",
            "Epoch 109610 | Loss: 0.039907973259687424 | Test loss: 0.10553266853094101\n",
            "Epoch 109620 | Loss: 0.039756812155246735 | Test loss: 0.10514545440673828\n",
            "Epoch 109630 | Loss: 0.039605624973773956 | Test loss: 0.10475826263427734\n",
            "Epoch 109640 | Loss: 0.03945441171526909 | Test loss: 0.10437112301588058\n",
            "Epoch 109650 | Loss: 0.0393032543361187 | Test loss: 0.10398373752832413\n",
            "Epoch 109660 | Loss: 0.03915207460522652 | Test loss: 0.10359666496515274\n",
            "Epoch 109670 | Loss: 0.03900087624788284 | Test loss: 0.10320942848920822\n",
            "Epoch 109680 | Loss: 0.03884971886873245 | Test loss: 0.10282223671674728\n",
            "Epoch 109690 | Loss: 0.03869865462183952 | Test loss: 0.10243506729602814\n",
            "Epoch 109700 | Loss: 0.03854737430810928 | Test loss: 0.10204780101776123\n",
            "Epoch 109710 | Loss: 0.03839614987373352 | Test loss: 0.10166066139936447\n",
            "Epoch 109720 | Loss: 0.03824497014284134 | Test loss: 0.10127341747283936\n",
            "Epoch 109730 | Loss: 0.03809378296136856 | Test loss: 0.10088622570037842\n",
            "Epoch 109740 | Loss: 0.037942614406347275 | Test loss: 0.10049905627965927\n",
            "Epoch 109750 | Loss: 0.037791408598423004 | Test loss: 0.10011179745197296\n",
            "Epoch 109760 | Loss: 0.03764024004340172 | Test loss: 0.09972460567951202\n",
            "Epoch 109770 | Loss: 0.03748904541134834 | Test loss: 0.09933741390705109\n",
            "Epoch 109780 | Loss: 0.03733786940574646 | Test loss: 0.09895019978284836\n",
            "Epoch 109790 | Loss: 0.03718669340014458 | Test loss: 0.09856303036212921\n",
            "Epoch 109800 | Loss: 0.037035513669252396 | Test loss: 0.09817581623792648\n",
            "Epoch 109810 | Loss: 0.03688431531190872 | Test loss: 0.09778857231140137\n",
            "Epoch 109820 | Loss: 0.03673313185572624 | Test loss: 0.09740138053894043\n",
            "Epoch 109830 | Loss: 0.036581944674253464 | Test loss: 0.09701418876647949\n",
            "Epoch 109840 | Loss: 0.03643077611923218 | Test loss: 0.09662699699401855\n",
            "Epoch 109850 | Loss: 0.03627956658601761 | Test loss: 0.09623987972736359\n",
            "Epoch 109860 | Loss: 0.03612840920686722 | Test loss: 0.0958525687456131\n",
            "Epoch 109870 | Loss: 0.03597721457481384 | Test loss: 0.09546539932489395\n",
            "Epoch 109880 | Loss: 0.03582604601979256 | Test loss: 0.09507818520069122\n",
            "Epoch 109890 | Loss: 0.03567482903599739 | Test loss: 0.09469099342823029\n",
            "Epoch 109900 | Loss: 0.0355236791074276 | Test loss: 0.09430377930402756\n",
            "Epoch 109910 | Loss: 0.03537246584892273 | Test loss: 0.09391655772924423\n",
            "Epoch 109920 | Loss: 0.03522130846977234 | Test loss: 0.0935293436050415\n",
            "Epoch 109930 | Loss: 0.03507009893655777 | Test loss: 0.09314215183258057\n",
            "Epoch 109940 | Loss: 0.03491892293095589 | Test loss: 0.09275498241186142\n",
            "Epoch 109950 | Loss: 0.034767743200063705 | Test loss: 0.09236772358417511\n",
            "Epoch 109960 | Loss: 0.03461656719446182 | Test loss: 0.09198050945997238\n",
            "Epoch 109970 | Loss: 0.03446537256240845 | Test loss: 0.09159334003925323\n",
            "Epoch 109980 | Loss: 0.03431420400738716 | Test loss: 0.0912061482667923\n",
            "Epoch 109990 | Loss: 0.03416299819946289 | Test loss: 0.09081895649433136\n",
            "Epoch 110000 | Loss: 0.0340118370950222 | Test loss: 0.09043176472187042\n",
            "Epoch 110010 | Loss: 0.033860623836517334 | Test loss: 0.0900445207953453\n",
            "Epoch 110020 | Loss: 0.03370946645736694 | Test loss: 0.08965730667114258\n",
            "Epoch 110030 | Loss: 0.033558279275894165 | Test loss: 0.08927011489868164\n",
            "Epoch 110040 | Loss: 0.033407069742679596 | Test loss: 0.08888297528028488\n",
            "Epoch 110050 | Loss: 0.033255912363529205 | Test loss: 0.08849558979272842\n",
            "Epoch 110060 | Loss: 0.033104728907346725 | Test loss: 0.08810851722955704\n",
            "Epoch 110070 | Loss: 0.03295353055000305 | Test loss: 0.08772128075361252\n",
            "Epoch 110080 | Loss: 0.03280237317085266 | Test loss: 0.08733408898115158\n",
            "Epoch 110090 | Loss: 0.03265131264925003 | Test loss: 0.08694691956043243\n",
            "Epoch 110100 | Loss: 0.03250047191977501 | Test loss: 0.08655434101819992\n",
            "Epoch 110110 | Loss: 0.03234991431236267 | Test loss: 0.08616042137145996\n",
            "Epoch 110120 | Loss: 0.03219937905669212 | Test loss: 0.08576653152704239\n",
            "Epoch 110130 | Loss: 0.03204883262515068 | Test loss: 0.08537264168262482\n",
            "Epoch 110140 | Loss: 0.03189830854535103 | Test loss: 0.08497877418994904\n",
            "Epoch 110150 | Loss: 0.03174775838851929 | Test loss: 0.08458485454320908\n",
            "Epoch 110160 | Loss: 0.03159722685813904 | Test loss: 0.08419103920459747\n",
            "Epoch 110170 | Loss: 0.0314466618001461 | Test loss: 0.08379707485437393\n",
            "Epoch 110180 | Loss: 0.03129614144563675 | Test loss: 0.08340316265821457\n",
            "Epoch 110190 | Loss: 0.031145591288805008 | Test loss: 0.0830092653632164\n",
            "Epoch 110200 | Loss: 0.030995065346360207 | Test loss: 0.082615427672863\n",
            "Epoch 110210 | Loss: 0.030844515189528465 | Test loss: 0.08222146332263947\n",
            "Epoch 110220 | Loss: 0.03069397248327732 | Test loss: 0.08182759582996368\n",
            "Epoch 110230 | Loss: 0.030543435364961624 | Test loss: 0.08143365383148193\n",
            "Epoch 110240 | Loss: 0.030392898246645927 | Test loss: 0.08103978633880615\n",
            "Epoch 110250 | Loss: 0.030242299661040306 | Test loss: 0.08064589649438858\n",
            "Epoch 110260 | Loss: 0.030091822147369385 | Test loss: 0.08025198429822922\n",
            "Epoch 110270 | Loss: 0.029941266402602196 | Test loss: 0.07985806465148926\n",
            "Epoch 110280 | Loss: 0.0297907292842865 | Test loss: 0.07946417480707169\n",
            "Epoch 110290 | Loss: 0.029640186578035355 | Test loss: 0.07907028496265411\n",
            "Epoch 110300 | Loss: 0.029489660635590553 | Test loss: 0.07867641746997833\n",
            "Epoch 110310 | Loss: 0.029339110478758812 | Test loss: 0.07828249782323837\n",
            "Epoch 110320 | Loss: 0.029188578948378563 | Test loss: 0.07788868248462677\n",
            "Epoch 110330 | Loss: 0.02903801202774048 | Test loss: 0.07749471813440323\n",
            "Epoch 110340 | Loss: 0.028887493535876274 | Test loss: 0.07710080593824387\n",
            "Epoch 110350 | Loss: 0.028736943379044533 | Test loss: 0.0767069086432457\n",
            "Epoch 110360 | Loss: 0.02858641743659973 | Test loss: 0.0763130709528923\n",
            "Epoch 110370 | Loss: 0.02843586914241314 | Test loss: 0.07591910660266876\n",
            "Epoch 110380 | Loss: 0.028285324573516846 | Test loss: 0.07552523910999298\n",
            "Epoch 110390 | Loss: 0.028134793043136597 | Test loss: 0.07513129711151123\n",
            "Epoch 110400 | Loss: 0.027984250336885452 | Test loss: 0.07473742961883545\n",
            "Epoch 110410 | Loss: 0.02783365361392498 | Test loss: 0.07434353977441788\n",
            "Epoch 110420 | Loss: 0.02768317423760891 | Test loss: 0.07394962757825851\n",
            "Epoch 110430 | Loss: 0.02753262035548687 | Test loss: 0.07355570793151855\n",
            "Epoch 110440 | Loss: 0.027382081374526024 | Test loss: 0.07316181808710098\n",
            "Epoch 110450 | Loss: 0.02723153866827488 | Test loss: 0.07276792824268341\n",
            "Epoch 110460 | Loss: 0.027081012725830078 | Test loss: 0.07237406075000763\n",
            "Epoch 110470 | Loss: 0.026930464431643486 | Test loss: 0.07198014110326767\n",
            "Epoch 110480 | Loss: 0.026779932901263237 | Test loss: 0.07158632576465607\n",
            "Epoch 110490 | Loss: 0.026629364117980003 | Test loss: 0.07119236141443253\n",
            "Epoch 110500 | Loss: 0.0264788456261158 | Test loss: 0.07079844921827316\n",
            "Epoch 110510 | Loss: 0.026328295469284058 | Test loss: 0.070404551923275\n",
            "Epoch 110520 | Loss: 0.026177769526839256 | Test loss: 0.0700107142329216\n",
            "Epoch 110530 | Loss: 0.026027221232652664 | Test loss: 0.06961674988269806\n",
            "Epoch 110540 | Loss: 0.02587667666375637 | Test loss: 0.06922288239002228\n",
            "Epoch 110550 | Loss: 0.025726139545440674 | Test loss: 0.06882894039154053\n",
            "Epoch 110560 | Loss: 0.025575602427124977 | Test loss: 0.06843507289886475\n",
            "Epoch 110570 | Loss: 0.025425005704164505 | Test loss: 0.06804118305444717\n",
            "Epoch 110580 | Loss: 0.025274528190493584 | Test loss: 0.06764727085828781\n",
            "Epoch 110590 | Loss: 0.025123972445726395 | Test loss: 0.06725335121154785\n",
            "Epoch 110600 | Loss: 0.024973435327410698 | Test loss: 0.06685946136713028\n",
            "Epoch 110610 | Loss: 0.024822890758514404 | Test loss: 0.06646557152271271\n",
            "Epoch 110620 | Loss: 0.024672364816069603 | Test loss: 0.06607170403003693\n",
            "Epoch 110630 | Loss: 0.02452181652188301 | Test loss: 0.06567778438329697\n",
            "Epoch 110640 | Loss: 0.024371284991502762 | Test loss: 0.06528396904468536\n",
            "Epoch 110650 | Loss: 0.024220718070864677 | Test loss: 0.06489000469446182\n",
            "Epoch 110660 | Loss: 0.024070197716355324 | Test loss: 0.06449609249830246\n",
            "Epoch 110670 | Loss: 0.023919647559523582 | Test loss: 0.06410219520330429\n",
            "Epoch 110680 | Loss: 0.02376912347972393 | Test loss: 0.0637083575129509\n",
            "Epoch 110690 | Loss: 0.02361856773495674 | Test loss: 0.06331205368041992\n",
            "Epoch 110700 | Loss: 0.023467762395739555 | Test loss: 0.06290788948535919\n",
            "Epoch 110710 | Loss: 0.023317014798521996 | Test loss: 0.06250374764204025\n",
            "Epoch 110720 | Loss: 0.023166293278336525 | Test loss: 0.06209881231188774\n",
            "Epoch 110730 | Loss: 0.02301545813679695 | Test loss: 0.06169474124908447\n",
            "Epoch 110740 | Loss: 0.02286471240222454 | Test loss: 0.06129055097699165\n",
            "Epoch 110750 | Loss: 0.022713959217071533 | Test loss: 0.06088559702038765\n",
            "Epoch 110760 | Loss: 0.022563165053725243 | Test loss: 0.06048138067126274\n",
            "Epoch 110770 | Loss: 0.02241242490708828 | Test loss: 0.06007738038897514\n",
            "Epoch 110780 | Loss: 0.022261662408709526 | Test loss: 0.05967242643237114\n",
            "Epoch 110790 | Loss: 0.022110868245363235 | Test loss: 0.0592682845890522\n",
            "Epoch 110800 | Loss: 0.021960098296403885 | Test loss: 0.05886414274573326\n",
            "Epoch 110810 | Loss: 0.02180936373770237 | Test loss: 0.05845918878912926\n",
            "Epoch 110820 | Loss: 0.021658558398485184 | Test loss: 0.0580550916492939\n",
            "Epoch 110830 | Loss: 0.021507835015654564 | Test loss: 0.05765097215771675\n",
            "Epoch 110840 | Loss: 0.021357053890824318 | Test loss: 0.057245995849370956\n",
            "Epoch 110850 | Loss: 0.02120622992515564 | Test loss: 0.05684187635779381\n",
            "Epoch 110860 | Loss: 0.021055495366454124 | Test loss: 0.05643777921795845\n",
            "Epoch 110870 | Loss: 0.02090475521981716 | Test loss: 0.05603296682238579\n",
            "Epoch 110880 | Loss: 0.020753949880599976 | Test loss: 0.05562868341803551\n",
            "Epoch 110890 | Loss: 0.020603209733963013 | Test loss: 0.05522456392645836\n",
            "Epoch 110900 | Loss: 0.020452458411455154 | Test loss: 0.054819609969854355\n",
            "Epoch 110910 | Loss: 0.02030165307223797 | Test loss: 0.05441546440124512\n",
            "Epoch 110920 | Loss: 0.02015090547502041 | Test loss: 0.05401046201586723\n",
            "Epoch 110930 | Loss: 0.020000075921416283 | Test loss: 0.053606368601322174\n",
            "Epoch 110940 | Loss: 0.01984935998916626 | Test loss: 0.053202271461486816\n",
            "Epoch 110950 | Loss: 0.019698619842529297 | Test loss: 0.05279724672436714\n",
            "Epoch 110960 | Loss: 0.019547784700989723 | Test loss: 0.052393149584531784\n",
            "Epoch 110970 | Loss: 0.019397050142288208 | Test loss: 0.051989078521728516\n",
            "Epoch 110980 | Loss: 0.019246285781264305 | Test loss: 0.05158412456512451\n",
            "Epoch 110990 | Loss: 0.019095486029982567 | Test loss: 0.05117995664477348\n",
            "Epoch 111000 | Loss: 0.01894473470747471 | Test loss: 0.050775837153196335\n",
            "Epoch 111010 | Loss: 0.018794000148773193 | Test loss: 0.05037086084485054\n",
            "Epoch 111020 | Loss: 0.018643170595169067 | Test loss: 0.04996678978204727\n",
            "Epoch 111030 | Loss: 0.01849244348704815 | Test loss: 0.04956267029047012\n",
            "Epoch 111040 | Loss: 0.01834169030189514 | Test loss: 0.04915766790509224\n",
            "Epoch 111050 | Loss: 0.01819087378680706 | Test loss: 0.04875354841351509\n",
            "Epoch 111060 | Loss: 0.0180401261895895 | Test loss: 0.04834942892193794\n",
            "Epoch 111070 | Loss: 0.017889399081468582 | Test loss: 0.04794445261359215\n",
            "Epoch 111080 | Loss: 0.017738569527864456 | Test loss: 0.04754035547375679\n",
            "Epoch 111090 | Loss: 0.01758783496916294 | Test loss: 0.04713613912463188\n",
            "Epoch 111100 | Loss: 0.017437070608139038 | Test loss: 0.04673121124505997\n",
            "Epoch 111110 | Loss: 0.017286289483308792 | Test loss: 0.0463271401822567\n",
            "Epoch 111120 | Loss: 0.017135513946413994 | Test loss: 0.04592304304242134\n",
            "Epoch 111130 | Loss: 0.016984790563583374 | Test loss: 0.04551804065704346\n",
            "Epoch 111140 | Loss: 0.016833966597914696 | Test loss: 0.04511392116546631\n",
            "Epoch 111150 | Loss: 0.016683239489793777 | Test loss: 0.04470985010266304\n",
            "Epoch 111160 | Loss: 0.016532475128769875 | Test loss: 0.044304873794317245\n",
            "Epoch 111170 | Loss: 0.016381680965423584 | Test loss: 0.04390077665448189\n",
            "Epoch 111180 | Loss: 0.016230935230851173 | Test loss: 0.04349663481116295\n",
            "Epoch 111190 | Loss: 0.016080176457762718 | Test loss: 0.043091606348752975\n",
            "Epoch 111200 | Loss: 0.015929371118545532 | Test loss: 0.04268753528594971\n",
            "Epoch 111210 | Loss: 0.01577860116958618 | Test loss: 0.04228339344263077\n",
            "Epoch 111220 | Loss: 0.01562786102294922 | Test loss: 0.041878413408994675\n",
            "Epoch 111230 | Loss: 0.015477055683732033 | Test loss: 0.041474342346191406\n",
            "Epoch 111240 | Loss: 0.015326321125030518 | Test loss: 0.04107022285461426\n",
            "Epoch 111250 | Loss: 0.01517555769532919 | Test loss: 0.040665220469236374\n",
            "Epoch 111260 | Loss: 0.015024781227111816 | Test loss: 0.040261127054691315\n",
            "Epoch 111270 | Loss: 0.014874023385345936 | Test loss: 0.039856936782598495\n",
            "Epoch 111280 | Loss: 0.014723265543580055 | Test loss: 0.03945200517773628\n",
            "Epoch 111290 | Loss: 0.014572459273040295 | Test loss: 0.039047934114933014\n",
            "Epoch 111300 | Loss: 0.014421713538467884 | Test loss: 0.038643766194581985\n",
            "Epoch 111310 | Loss: 0.014270978979766369 | Test loss: 0.03823883458971977\n",
            "Epoch 111320 | Loss: 0.014120155945420265 | Test loss: 0.037834715098142624\n",
            "Epoch 111330 | Loss: 0.013969403691589832 | Test loss: 0.037430621683597565\n",
            "Epoch 111340 | Loss: 0.013818609528243542 | Test loss: 0.0370255708694458\n",
            "Epoch 111350 | Loss: 0.013667869381606579 | Test loss: 0.03662147745490074\n",
            "Epoch 111360 | Loss: 0.01351711805909872 | Test loss: 0.03621652349829674\n",
            "Epoch 111370 | Loss: 0.01336629968136549 | Test loss: 0.03581240400671959\n",
            "Epoch 111380 | Loss: 0.01321555394679308 | Test loss: 0.03540832921862602\n",
            "Epoch 111390 | Loss: 0.013064819388091564 | Test loss: 0.03500335291028023\n",
            "Epoch 111400 | Loss: 0.012914001941680908 | Test loss: 0.03459916263818741\n",
            "Epoch 111410 | Loss: 0.012763268314301968 | Test loss: 0.03419511392712593\n",
            "Epoch 111420 | Loss: 0.012612509541213512 | Test loss: 0.03379018232226372\n",
            "Epoch 111430 | Loss: 0.012461692094802856 | Test loss: 0.0333859920501709\n",
            "Epoch 111440 | Loss: 0.012310934253036976 | Test loss: 0.03298189863562584\n",
            "Epoch 111450 | Loss: 0.01216019969433546 | Test loss: 0.032576944679021835\n",
            "Epoch 111460 | Loss: 0.012009394355118275 | Test loss: 0.03217277675867081\n",
            "Epoch 111470 | Loss: 0.011858642101287842 | Test loss: 0.03176863119006157\n",
            "Epoch 111480 | Loss: 0.011707854457199574 | Test loss: 0.031363677233457565\n",
            "Epoch 111490 | Loss: 0.011557084508240223 | Test loss: 0.030959581956267357\n",
            "Epoch 111500 | Loss: 0.011406349949538708 | Test loss: 0.03055551089346409\n",
            "Epoch 111510 | Loss: 0.011255592107772827 | Test loss: 0.030150556936860085\n",
            "Epoch 111520 | Loss: 0.011104786768555641 | Test loss: 0.029746366664767265\n",
            "Epoch 111530 | Loss: 0.01095402892678976 | Test loss: 0.029342247173190117\n",
            "Epoch 111540 | Loss: 0.010803288780152798 | Test loss: 0.028937293216586113\n",
            "Epoch 111550 | Loss: 0.01065248902887106 | Test loss: 0.028533149510622025\n",
            "Epoch 111560 | Loss: 0.010501742362976074 | Test loss: 0.028129100799560547\n",
            "Epoch 111570 | Loss: 0.010351002216339111 | Test loss: 0.027724100276827812\n",
            "Epoch 111580 | Loss: 0.010200190357863903 | Test loss: 0.027319980785250664\n",
            "Epoch 111590 | Loss: 0.010049426928162575 | Test loss: 0.026915861293673515\n",
            "Epoch 111600 | Loss: 0.00989869236946106 | Test loss: 0.02651085890829563\n",
            "Epoch 111610 | Loss: 0.009747880510985851 | Test loss: 0.026106787845492363\n",
            "Epoch 111620 | Loss: 0.009597117081284523 | Test loss: 0.025702644139528275\n",
            "Epoch 111630 | Loss: 0.00944638904184103 | Test loss: 0.02529776096343994\n",
            "Epoch 111640 | Loss: 0.009295577183365822 | Test loss: 0.024893546476960182\n",
            "Epoch 111650 | Loss: 0.009144830517470837 | Test loss: 0.024489475414156914\n",
            "Epoch 111660 | Loss: 0.008994096890091896 | Test loss: 0.02408447302877903\n",
            "Epoch 111670 | Loss: 0.008843285031616688 | Test loss: 0.02368037775158882\n",
            "Epoch 111680 | Loss: 0.008692527189850807 | Test loss: 0.023276282474398613\n",
            "Epoch 111690 | Loss: 0.008541780523955822 | Test loss: 0.02287123166024685\n",
            "Epoch 111700 | Loss: 0.008391064591705799 | Test loss: 0.02246713638305664\n",
            "Epoch 111710 | Loss: 0.008240235038101673 | Test loss: 0.022063065320253372\n",
            "Epoch 111720 | Loss: 0.00808948278427124 | Test loss: 0.02165803872048855\n",
            "Epoch 111730 | Loss: 0.007938665337860584 | Test loss: 0.02125394344329834\n",
            "Epoch 111740 | Loss: 0.007787925191223621 | Test loss: 0.02084982395172119\n",
            "Epoch 111750 | Loss: 0.007637131493538618 | Test loss: 0.020444845780730247\n",
            "Epoch 111760 | Loss: 0.00748637318611145 | Test loss: 0.02004075050354004\n",
            "Epoch 111770 | Loss: 0.007335633039474487 | Test loss: 0.019635749980807304\n",
            "Epoch 111780 | Loss: 0.007184809539467096 | Test loss: 0.019231677055358887\n",
            "Epoch 111790 | Loss: 0.0070340693928301334 | Test loss: 0.0188275333493948\n",
            "Epoch 111800 | Loss: 0.0068833231925964355 | Test loss: 0.018422579392790794\n",
            "Epoch 111810 | Loss: 0.0067324223928153515 | Test loss: 0.018018437549471855\n",
            "Epoch 111820 | Loss: 0.006581747438758612 | Test loss: 0.017614340409636497\n",
            "Epoch 111830 | Loss: 0.006431025452911854 | Test loss: 0.017209364101290703\n",
            "Epoch 111840 | Loss: 0.0062802135944366455 | Test loss: 0.016805266961455345\n",
            "Epoch 111850 | Loss: 0.006129467394202948 | Test loss: 0.016401125118136406\n",
            "Epoch 111860 | Loss: 0.005978721659630537 | Test loss: 0.015996171161532402\n",
            "Epoch 111870 | Loss: 0.005827903747558594 | Test loss: 0.015592050738632679\n",
            "Epoch 111880 | Loss: 0.005677145905792713 | Test loss: 0.01518790703266859\n",
            "Epoch 111890 | Loss: 0.005526417400687933 | Test loss: 0.014782977290451527\n",
            "Epoch 111900 | Loss: 0.005375606007874012 | Test loss: 0.014378833584487438\n",
            "Epoch 111910 | Loss: 0.005224859807640314 | Test loss: 0.013974666595458984\n",
            "Epoch 111920 | Loss: 0.005074125714600086 | Test loss: 0.01356971263885498\n",
            "Epoch 111930 | Loss: 0.00492328405380249 | Test loss: 0.013165640644729137\n",
            "Epoch 111940 | Loss: 0.004772543907165527 | Test loss: 0.012761497870087624\n",
            "Epoch 111950 | Loss: 0.0046218219213187695 | Test loss: 0.01235651969909668\n",
            "Epoch 111960 | Loss: 0.004471004009246826 | Test loss: 0.011952233500778675\n",
            "Epoch 111970 | Loss: 0.004320240113884211 | Test loss: 0.011548304930329323\n",
            "Epoch 111980 | Loss: 0.004169512074440718 | Test loss: 0.011143326759338379\n",
            "Epoch 111990 | Loss: 0.004018723964691162 | Test loss: 0.01073920726776123\n",
            "Epoch 112000 | Loss: 0.00386793608777225 | Test loss: 0.010335040278732777\n",
            "Epoch 112010 | Loss: 0.0037172199226915836 | Test loss: 0.009930133819580078\n",
            "Epoch 112020 | Loss: 0.0035663903690874577 | Test loss: 0.009525991044938564\n",
            "Epoch 112030 | Loss: 0.0034156502224504948 | Test loss: 0.009121894836425781\n",
            "Epoch 112040 | Loss: 0.0032648982014507055 | Test loss: 0.008716965094208717\n",
            "Epoch 112050 | Loss: 0.0031140863429754972 | Test loss: 0.008312845602631569\n",
            "Epoch 112060 | Loss: 0.0029633522499352694 | Test loss: 0.00790870189666748\n",
            "Epoch 112070 | Loss: 0.0028126060497015715 | Test loss: 0.007503819651901722\n",
            "Epoch 112080 | Loss: 0.0026617886032909155 | Test loss: 0.007099628448486328\n",
            "Epoch 112090 | Loss: 0.0025110424030572176 | Test loss: 0.00669550895690918\n",
            "Epoch 112100 | Loss: 0.002360320184379816 | Test loss: 0.006290483754128218\n",
            "Epoch 112110 | Loss: 0.0022095085587352514 | Test loss: 0.005886435508728027\n",
            "Epoch 112120 | Loss: 0.002058732556179166 | Test loss: 0.0054822685196995735\n",
            "Epoch 112130 | Loss: 0.0019079804187640548 | Test loss: 0.005077290814369917\n",
            "Epoch 112140 | Loss: 0.001757204532623291 | Test loss: 0.004673195071518421\n",
            "Epoch 112150 | Loss: 0.0016064525116235018 | Test loss: 0.00426905183121562\n",
            "Epoch 112160 | Loss: 0.0014556884998455644 | Test loss: 0.0038640976417809725\n",
            "Epoch 112170 | Loss: 0.0013048648834228516 | Test loss: 0.0034600018989294767\n",
            "Epoch 112180 | Loss: 0.00115414266474545 | Test loss: 0.0030551671516150236\n",
            "Epoch 112190 | Loss: 0.0010033607250079513 | Test loss: 0.0026509284507483244\n",
            "Epoch 112200 | Loss: 0.0008525789016857743 | Test loss: 0.0022467852104455233\n",
            "Epoch 112210 | Loss: 0.0007018506876192987 | Test loss: 0.0018418073887005448\n",
            "Epoch 112220 | Loss: 0.0005510211340151727 | Test loss: 0.0014377356274053454\n",
            "Epoch 112230 | Loss: 0.00040028095827437937 | Test loss: 0.001033616135828197\n",
            "Epoch 112240 | Loss: 0.0002495110093150288 | Test loss: 0.0006286382558755577\n",
            "Epoch 112250 | Loss: 9.871125075733289e-05 | Test loss: 0.00022451877885032445\n",
            "Epoch 112260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 112990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 113990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 114990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 115990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 116990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 117990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 118990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 119990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 120990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 121990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 122990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 123990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 124990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 125990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 126990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 127990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 128990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129000 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129010 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129020 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129030 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129040 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129050 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129060 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129070 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129080 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129090 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129100 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129110 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129120 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129130 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129140 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129150 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129160 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129170 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129180 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129190 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129200 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129210 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129220 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129230 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129240 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129250 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129260 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129270 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129280 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129290 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129300 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129310 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129320 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129330 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129340 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129350 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129360 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129370 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129380 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129390 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129400 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129410 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129420 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129430 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129440 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129450 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129460 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129470 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129480 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129490 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129500 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129510 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129520 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129530 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129540 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129550 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129560 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129570 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129580 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129590 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129600 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129610 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129620 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129630 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129640 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129650 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129660 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129670 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129680 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129690 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129700 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129710 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129720 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129730 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129740 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129750 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129760 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129770 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129780 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129790 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129800 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129810 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129820 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129830 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129840 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129850 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129860 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129870 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129880 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129890 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129900 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129910 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129920 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129930 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129940 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129950 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129960 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129970 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129980 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n",
            "Epoch 129990 | Loss: 3.917217327398248e-05 | Test loss: 5.44071190233808e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQThMi9CXNK8",
        "outputId": "cf8588fa-6053-4363-afac-016ad9b5a141"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[-2.0000]], device='cuda:0')),\n",
              "             ('linear_layer.bias', tensor([10.0000], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Making and evaluating predictions"
      ],
      "metadata": {
        "id": "CLRBVjmzagOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  Y_preds = model_1(X_test)\n",
        "Y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlrUwN3lX4y3",
        "outputId": "45d4b8b8-5e4d-4e6e-daf7-f52cea69ff86"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8000],\n",
              "        [8.7800],\n",
              "        [8.7600],\n",
              "        [8.7400],\n",
              "        [8.7200],\n",
              "        [8.7000],\n",
              "        [8.6800],\n",
              "        [8.6600],\n",
              "        [8.6400],\n",
              "        [8.6200],\n",
              "        [8.6000],\n",
              "        [8.5800],\n",
              "        [8.5600],\n",
              "        [8.5400],\n",
              "        [8.5200],\n",
              "        [8.5000],\n",
              "        [8.4800],\n",
              "        [8.4600],\n",
              "        [8.4400],\n",
              "        [8.4200],\n",
              "        [8.4000],\n",
              "        [8.3800],\n",
              "        [8.3600],\n",
              "        [8.3400],\n",
              "        [8.3200],\n",
              "        [8.3000],\n",
              "        [8.2800],\n",
              "        [8.2600],\n",
              "        [8.2400],\n",
              "        [8.2200],\n",
              "        [8.2000],\n",
              "        [8.1800],\n",
              "        [8.1600],\n",
              "        [8.1400],\n",
              "        [8.1200],\n",
              "        [8.1000],\n",
              "        [8.0800],\n",
              "        [8.0600],\n",
              "        [8.0400],\n",
              "        [8.0200]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put data(training and test ) on cpu device\n",
        "device = \"cpu\"\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "Y_train = Y_train.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "Y_preds = Y_preds.to(device)\n",
        "plot_predictions(train_data=X_train,\n",
        "                 train_label=Y_train.cpu(),\n",
        "                 test_data=X_test,\n",
        "                 test_label=Y_test,\n",
        "                 predictions=Y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "ffIBWNzVxyYq",
        "outputId": "38be6ac2-3c5b-4c25-f9e8-58763216c567"
      },
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5 Save and Loading a trained model"
      ],
      "metadata": {
        "id": "-tws3tYFcbiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model_1\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_1_PATH = Path(\"Models\")\n",
        "MODEL_1_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_1_NAME = \"pytorch_workflow_1.pt\"\n",
        "MODEL_1_SAVE_PATH = MODEL_1_PATH / MODEL_1_NAME\n",
        "\n",
        "print(f\"Saving {MODEL_1_SAVE_PATH}\")\n",
        "torch.save(obj=model_1.state_dict(), f=MODEL_1_SAVE_PATH)"
      ],
      "metadata": {
        "id": "zwiDe5Y1cn-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56c9acc-b547-44d1-fe92-1625f8273807"
      },
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Models/pytorch_workflow_1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model_1\n",
        "loaded_model_1 = LinearRegressionModelV2().to(device)\n",
        "loaded_model_1.load_state_dict(torch.load(f=MODEL_1_SAVE_PATH))\n",
        "loaded_model_1.state_dict()"
      ],
      "metadata": {
        "id": "5NI2rhtyfaFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158cdfa8-f39d-416d-db80-641e032d4c7e"
      },
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[-2.0000]])),\n",
              "             ('linear_layer.bias', tensor([10.0000]))])"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ]
    }
  ]
}